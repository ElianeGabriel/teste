{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b1eb2e34d76b499f87299d156c5a4dbe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<center><br> <img src=\"https://www.iscte-iul.pt/assets/images/logo_iscte_detailed.svg\" style=\"width: 300px;\"></center><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db9cd1552ab8498e88b44eb0bbc583bc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/archive/c/cc/20180621135549%21Vodafone_2017_logo.svg\" style=\"width: 200px;margin-top:90px;margin-bottom:100px;\" align=\"left\">\n",
    "   <div style= \"font-size: 28px;font-weight:bold;margin-top:20px;margin-bottom:20px;margin-left:360px;margin-right:150px; line-height: 1.1;color:#F91701;\"><center>An√°lise de Sentimentos & T√≥picos nas Redes Sociais | Vodafone</center></div>\n",
    "   <div style= \"font-size: 17px;font-weight:bold;\"><center>Projeto Final Aplicado em Ci√™ncia de Dados</center></div>\n",
    "   <div><center><b>Orientador:</b> Nuno Santos (ISCTE-IUL) </center></div>\n",
    "   <div><center><b>Cliente:</b> Carlos Santos (Vodafone) </center></div>\n",
    " <br>\n",
    "    <div><center>Andr√© Silvestre N¬∫104532 </center></div>\n",
    "    <div><center>Eliane Gabriel N¬∫103303 <b>|</b> Maria Jo√£o Louren√ßo N¬∫104716 </center></div>\n",
    "    <div><center>Margarida Pereira N¬∫105877 <b>|</b> Umeima Mahomed N¬∫99239</center></div>\n",
    "    <br>\n",
    "    <div><center><b>CDC1</b> & <b>CDC2</b></center></div>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9f7b4f35253e417d916d43ca60d0ad84",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: .7px; color: white; border-radius: 300px; text-align: center;\">\n",
    "</div>\n",
    "\n",
    "# √çndice\n",
    "\n",
    "\n",
    "<ol style=\"list-style-type: upper-roman;font-weight: bold;\">\n",
    "  <li><span style = \"font-weight: normal;\"><a href=\"#1\">Business Understanding</a></span></li>\n",
    "  <li>\n",
    "      <span style = \"font-weight: normal;\">\n",
    "          <a href=\"#1\">Data Understanding</a>\n",
    "      </span>\n",
    "      <ol style=\"list-style-type:decimal;\">\n",
    "          <li><span style=\"font-weight: normal;\"><a href=\"#1.2\">Pr√©-Processamento dos Dados</a></span></li>\n",
    "          <li><span style=\"font-weight: normal;\"><a href=\"#1.3\">AED | An√°lise Explorat√≥ria de Dados</a></span></li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>\n",
    "    <span style=\"font-weight: normal;\">\n",
    "      <a href=\"#2\">Data Preparation</a>\n",
    "    </span> \n",
    "    <ol style=\"list-style-type:decimal;\">\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#4.2\">Users | Local de Resid√™ncia + G√©nero</a></span></li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <!--\n",
    "  <li>\n",
    "    <span style = \"font-weight: normal;\">\n",
    "        <a href=\"#3\">Modeling</a>\n",
    "    </span>\n",
    "    \n",
    "    <ol style=\"list-style-type:decimal;\">\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#M_1Obje\">AAAAA</a></span></li>\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#M_2Obje\">AAAAA</a></span></li>\n",
    "    </ol>\n",
    "  </li>-->\n",
    "    \n",
    "  <!--<li><span style = \"font-weight: normal;\"><a href=\"#4\">Evaluation</a></span></li>\n",
    "  <li><span style = \"font-weight: normal;\"><a href=\"#4\">Deployment</a></span></li>\n",
    "    -->\n",
    "</ol>\n",
    "\n",
    "<br>\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: .7px; color: white; border-radius: 300px; text-align: center;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T18:30:55.718041Z",
     "start_time": "2024-05-13T18:30:55.711882Z"
    },
    "cell_id": "4c1009bc8ff04db2bb664cd89ad62120",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 34,
    "execution_start": 1710005438477,
    "source_hash": null
   },
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>ol > li::marker {font-weight: bold;font-size: 15px;}\n",
    "    .output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T18:30:55.724095Z",
     "start_time": "2024-05-13T18:30:55.720050Z"
    }
   },
   "source": [
    "# Fonte: https://python.plainenglish.io/displaying-multiple-dataframes-side-by-side-in-jupyter-lab-notebook-9a4649a4940\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def display_side_by_side(*args, super_title, titles=cycle([''])):\n",
    "    html_str = ''\n",
    "    html_str += f'<h1 style=\"text-align: left; margin-bottom: -15px;\">{super_title}</h1><br>'\n",
    "    html_str += '<div style=\"display: flex;\">'\n",
    "    for df, title in zip(args, chain(titles, cycle(['</br>']))):\n",
    "        html_str += f'<div style=\"margin-right: 20px;\"><h3 style=\"text-align: center;color:#555555;\">{title}</h3>'\n",
    "        html_str += df.to_html().replace('table', 'table style=\"display:inline; margin-right: 20px;\"')\n",
    "        html_str += '</div>'\n",
    "    html_str += '</div>'\n",
    "    display_html(html_str, raw=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5d00703c76594fb98260e8c5af0db20d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## üìö Import das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T18:30:56.090362Z",
     "start_time": "2024-05-13T18:30:55.725101Z"
    },
    "cell_id": "b5c35872252f49f7b15a2a1c7262a128",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1060,
    "execution_start": 1710005440203,
    "source_hash": null
   },
   "source": [
    "# Bibliotecas que vamos usar no Projeto\n",
    "import os                    # Operating System\n",
    "from tqdm import tqdm        # Barra de Progresso\n",
    "\n",
    "# Para dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "pd.set_option('display.max_colwidth', None)  # Visualizar a informa√ß√£o toda\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.reset_option('display.max_colwidth') \n",
    "\n",
    "# Para gr√°ficos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "# import folium\n",
    "\n",
    "# --------------------- Scraping ---------------------\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm            # Barra de Progresso\n",
    "\n",
    "# Estilo dos Gr√°ficos    \n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Semente para reprodutibilidade\n",
    "import random\n",
    "random.seed(123)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-13T18:30:57.745533Z",
     "start_time": "2024-05-13T18:30:57.742372Z"
    }
   },
   "source": [
    "# !pip install unidecode, seaborn, selenium, bs4"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "c2468ae5305b473f9b5d72bfbf86b199",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# <a class='anchor' id='1'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 140px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Avenir Next LT Pro', sans-serif;\">\n",
    "        <b>1 & 2 | Business & Data Understanding</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db7bfe07502d47afaaae65e9e9afa992",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "## ‚õìÔ∏è Webscrape dos Dados relativos aos Coment√°rios e Avalia√ß√µes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "34fcb8d0c6954b6e944cb832759f2bd9",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "### Facebook üîµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fonte: https://github.com/harmindersinghnijjar/facebook-post-and-comments-scraper/blob/main/data_directory_facebook.py\n",
    "# Define Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')            # Sem interface gr√°fica vis√≠vel\n",
    "options.add_argument('--incognito')\n",
    "options.add_argument('--no-sandbox')            # Permitir a execu√ß√£o em ambientes controlados \n",
    "options.add_argument('--disable-dev-shm-usage') # Desativando o uso do /dev/shm no navegador para evitar problemas relacionados √† aloca√ß√£o de mem√≥ria compartilhada\n",
    "options.add_argument(\"--disable-notifications\")\n",
    "options.add_argument(\"--disable-extensions\")\n",
    "options.add_argument('--disable-application-cache')\n",
    "driver = webdriver.Chrome('./chromedriver',options=options)\n",
    "\n",
    "# ======= OU - Para as vers√µes mais recentes do Selenium\n",
    "# service = Service(executable_path='./chromedriver.exe')\n",
    "# driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Method to open Facebook in Chrome.\n",
    "driver.get(\"https://www.facebook.com\")\n",
    "time.sleep(5)\n",
    "\n",
    "# Login Facebook\n",
    "email_elem = driver.find_element(By.ID, \"email\")    # Locate by ID\n",
    "password_elem = driver.find_element(By.ID, \"pass\")  # Locate by ID\n",
    "\n",
    "email_elem.send_keys('pfacd20@gmail.com')                # ---- BLOQUEADO [?]\n",
    "password_elem.send_keys('Pfacd20!*****') \n",
    "\n",
    "# email_elem.send_keys('antonio.alfredo1920@gmail.com')    # ---- BLOQUEADO [?]\n",
    "# password_elem.send_keys('antonioalfredo2024')\n",
    "\n",
    "# email_elem.send_keys('mariajota847@gmail.com')           # ---- ESPERA DE VERIFICA√á√ÉO-----\n",
    "# password_elem.send_keys('Mariajota.847!#')\n",
    "\n",
    "# email_elem.send_keys('josefinaluisao82@gmail.com')       # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('Josefina_luisao82!')\n",
    "\n",
    "# email_elem.send_keys('marioalbertocd202020@gmail.com')   # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('marioalberto.2020')\n",
    "\n",
    "# email_elem.send_keys('marioalberto.2020@outlook.pt')     # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('marioalberto202020')\n",
    "\n",
    "# email_elem.send_keys('filipefernandes20009@gmail.com')   # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('filipe.fernandes09')\n",
    "\n",
    "# email_elem.send_keys('joaompfa1998@gmail.com')           # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('joaompfa.1998#')\n",
    "\n",
    "# email_elem.send_keys('anacarvalho2024v@gmail.com')       # ---- BLOQUEADO [?] ----------- MM\n",
    "# password_elem.send_keys('Projeto.final2024')\n",
    "\n",
    "# email_elem.send_keys('brunosilva2024v@gmail.com')        # ---- UMI\n",
    "# password_elem.send_keys('Projeto.final2024')\n",
    "\n",
    "# email_elem.send_keys('alicecorreia590@gmail.com')        # ---- BLOQUEADO [?]\n",
    "# password_elem.send_keys('Projeto.final2024')\n",
    "\n",
    "# email_elem.send_keys('g96847449@gmail.com')              # ---- BLOQUEADO [?] -----\n",
    "# password_elem.send_keys('Projeto.final2024')\n",
    "\n",
    "# email_elem.send_keys('maria.romao52000@gmail.com')       # ---- MORREU MESMO :(\n",
    "# password_elem.send_keys('Projeto.final2024')\n",
    "\n",
    "# email_elem.send_keys('ana.antuness.987@outlook.pt')       # ---- ESPERA DE VERIFICA√á√ÉO-----\n",
    "# password_elem.send_keys('ana_1antunes!') \n",
    "\n",
    "# email_elem.send_keys('koalalo345@gmail.com')        # ---- [NOVA]\n",
    "# password_elem.send_keys('Koala123!')\n",
    "\n",
    "# email_elem.send_keys('hipo123@outlook.com')        # ---- [NOVA]\n",
    "# password_elem.send_keys('Hipo123!')\n",
    "\n",
    "# Submit the login form just sending the Enter/Return Key\n",
    "password_elem.send_keys(Keys.RETURN)\n",
    "time.sleep(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## üìå Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üìå Posts üü•üü¶‚¨õ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "%%time\n",
    "# Open Vodafone/MEO/NOS Page. ----------------------- Descomentar o link atribuido\n",
    "\n",
    "# driver.get(\"https://www.facebook.com/vodafonePT?locale=pt_PT\")   # Facebook da Vodafone - Margarida\n",
    "# driver.get(\"https://www.facebook.com/meo?locale=pt_PT\")          # Facebook da MEO      - Maria Jo√£o\n",
    "# driver.get(\"https://www.facebook.com/nosportugal?locale=pt_PT\")  # Facebook da NOS      - Umeima\n",
    "\n",
    "# =============================== Scraper dos Posts dos Operadores [Vodafone|MEO|NOS] ========================================\n",
    "try:\n",
    "    # Lista para armazenar os dicion√°rios de posts e um conjunto para adicionar os IDs dos posts\n",
    "    posts_data = []\n",
    "    existing_post_ids = set()\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # Scroll for 10 seconds\n",
    "    start_time = time.time()\n",
    "    end_time = start_time + 100 # 00\n",
    "    \n",
    "    while time.time() < end_time:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Extract posts and comments after the page has loaded\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        posts = soup.find_all(\"div\", class_=\"x78zum5 x1n2onr6 xh8yej3\")\n",
    "\n",
    "        for post in posts:\n",
    "\n",
    "            # Link do Post\n",
    "            href_link = post.find(\"a\", {\"class\": \"x1i10hfl x1qjc9v5 xjbqb8w xjqpnuy xa49m3k xqeqjp1 x2hbi6w x13fuv20 xu3j5b3 x1q0q8m5 x26u7qi x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xdl72j9 x2lah0s xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r x2lwn1j xeuugli xexx8yu x4uap5 x18d9i69 xkhd6sd x1n2onr6 x16tdsg8 x1hl2dhg xggy1nq x1ja2u2z x1t137rt x1o1ewxj x3x9cwd x1e5q0jg x13rtm0m x1q0g3np x87ps6o x1lku1pv x1a2a7pz x1lliihq x1pdlv7q\"}, href=True)\n",
    "            if href_link:\n",
    "                href_tags = href_link[\"href\"]\n",
    "                link_post= re.findall(r'(https://www\\.facebook\\.com/photo/\\?fbid=\\d+&set=[^\\s&]+)', href_tags)        \n",
    "                id_post = re.findall(r'https://www\\.facebook\\.com/photo/\\?fbid=(\\d+)&set=[^\\s&]+', href_tags)\n",
    "                \n",
    "                if id_post:\n",
    "                    # Texto do Post\n",
    "                    post_text = post.find(\"div\", class_=\"x1iorvi4 x1pi30zi x1l90r2v x1swvt13\")\n",
    "\n",
    "                    # Data do Post\n",
    "                    post_date = post.find(\"span\", class_=\"x4k7w5x x1h91t0o x1h9r5lt x1jfb8zj xv2umb2 x1beo9mf xaigb6o x12ejxvf x3igimt xarpa2k xedcshv x1lytzrv x1t2pt76 x7ja8zs x1qrby5j\")\n",
    "\n",
    "                    # Estat√≠sicas do Post\n",
    "                    n_estatisticas = post.find(\"div\", class_=\"x6s0dn4 xi81zsa x78zum5 x6prxxf x13a6bvl xvq8zen xdj266r xktsk01 xat24cr x1d52u69 x889kno x4uap5 x1a8lsjc xkhd6sd xdppsyt\")    \n",
    "\n",
    "                    n_reacts = post.find(\"span\", class_=\"xt0b8zv x1e558r4\")\n",
    "                    n_comments = post.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa\")\n",
    "                    n_shares = post.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa\")\n",
    "                    if n_shares:\n",
    "                        n_shares = n_shares.find_next(\"span\")\n",
    "\n",
    "                    # Lista de Comments - N√£o faz sentido para n√≥s pq s√≥ retira alguns\n",
    "                    # comments = post.find_all(\"div\", class_=\"x1y1aw1k xn6708d xwib8y2 x1ye3gou\")\n",
    "\n",
    "                    # Verificar se o post j√° existe na lista de IDs de postagens existentes\n",
    "                    if id_post[0] not in existing_post_ids:    \n",
    "                        # Criar dicion√°rio para o post\n",
    "                        post_dict = {\n",
    "                            \"id\": id_post[0],\n",
    "                            \"text\": post_text.text.strip() if post_text else \"\",\n",
    "                            \"date\": post_date.text.strip() if post_date else \"\",\n",
    "                            \"statistic\": n_estatisticas.text.strip() if n_estatisticas else \"\",\n",
    "                            \"reactions\": n_reacts.text.strip() if n_reacts else \"\",\n",
    "                            \"comments\": n_comments.text.strip() if n_comments else \"\",\n",
    "                            \"shares\": n_shares.text.strip() if n_shares else \"\",\n",
    "                            \"link\": link_post[0] if link_post else \"\"\n",
    "                        }\n",
    "\n",
    "                        # Adicionar o ID do post √† lista de IDs de postagens existentes\n",
    "                        existing_post_ids.add(id_post[0])\n",
    "\n",
    "                        # Adicionar o dicion√°rio √† lista de posts\n",
    "                        posts_data.append(post_dict)    \n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Error scraping posts: {e}\")\n",
    "    \n",
    "    \n",
    "# Escrever a lista de dicion√°rios em um arquivo JSON \n",
    "# ----------------------- ALTERAR NOME PARA A OPERADORA QUE TIVER A SER OBTIDA\n",
    "with open(f'Facebook_Vodafone_posts_{datetime.date.today()}.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(posts_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tempo a por no 'while' \n",
    "print(\"8 horas em '':\", 60*60*8)\n",
    "print(\"25 000'' em Horas:\", round(25000/60/60,4))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üìå Posts de Not√≠cias üü®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "%%time\n",
    "\n",
    "links_noticias = [\n",
    "    \"https://www.facebook.com/profile/100064861681188/search/?q=DIGI%20operador&locale=pt_PT\",    # Pplware\n",
    "    \"https://www.facebook.com/profile/100055885456176/search/?q=DIGI%20operador&locale=pt_PT\",    # 4gnews\n",
    "    \"https://www.facebook.com/profile/100064388515461/search/?q=DIGI%20operador&locale=pt_PT\",    # P√∫blico\n",
    "    \"https://www.facebook.com/profile/100066638765808/search/?q=DIGI%20operador&locale=pt_PT\",    # Observador\n",
    "    \"https://www.facebook.com/profile/100064621156693/search/?q=DIGI%20operador&locale=pt_PT\",    # Jornal de Not√≠cias\n",
    "    \"https://www.facebook.com/profile/100066432781929/search/?q=DIGI%20operador&locale=pt_PT\",    # CNN\n",
    "    # De fora ficaram o SIC Noticias, TVI, RTP Not√≠cias, CMTV, SAPO, Expresso, \n",
    "    #                   Vis√£o, Exame, Exame Inform√°tica e Jornal de Neg√≥cios por n√£o conterem posts e coment√°rios relevantes     \n",
    "]\n",
    "\n",
    "# =============================== Scraper dos Posts da [DIGI] ========================================\n",
    "\n",
    "# Lista para armazenar os dicion√°rios de posts e um conjunto para adicionar os IDs dos posts\n",
    "posts_data = []\n",
    "existing_post_ids = set()\n",
    "\n",
    "for url in links_noticias:\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        last_result = None       \n",
    "        while last_result is None:\n",
    "            \n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Extract posts and comments after the page has loaded\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            posts = soup.find_all(\"div\", class_=\"x78zum5 x1n2onr6 xh8yej3\")\n",
    "\n",
    "            for post in posts:\n",
    "\n",
    "                # Link do Post\n",
    "                href_link = post.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv xo1l8bm\", href=True)\n",
    "                if href_link:\n",
    "                    href_tags = href_link[\"href\"]\n",
    "                    id_post= re.findall(r'/posts/([^/?]+)', href_tags)    \n",
    "                    link_post = re.findall(r'https://www\\.facebook\\.com[^?]+', href_tags)\n",
    "                    \n",
    "                    if id_post:\n",
    "                        # Page\n",
    "                        page = post.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xt0b8zv xzsf02u x1s688f\")\n",
    "                        \n",
    "                        # Texto do Post\n",
    "                        post_text = post.find(\"div\", class_=\"x1iorvi4 x1pi30zi x1l90r2v x1swvt13\")\n",
    "                        news_text = post.find(\"span\", class_=\"x1lliihq x6ikm8r x10wlt62 x1n2onr6 x1j85h84\")\n",
    "                        \n",
    "                        # Data do Post\n",
    "                        post_date = post.find(\"span\", class_=\"x4k7w5x x1h91t0o x1h9r5lt x1jfb8zj xv2umb2 x1beo9mf xaigb6o x12ejxvf x3igimt xarpa2k xedcshv x1lytzrv x1t2pt76 x7ja8zs x1qrby5j\")\n",
    "\n",
    "                        # Estat√≠sicas do Post\n",
    "                        n_estatisticas = post.find(\"div\", class_=\"x6s0dn4 xi81zsa x78zum5 x6prxxf x13a6bvl xvq8zen xdj266r xktsk01 xat24cr x1d52u69 x889kno x4uap5 x1a8lsjc xkhd6sd xdppsyt\")    \n",
    "\n",
    "                        n_reacts = post.find(\"span\", class_=\"xt0b8zv x1e558r4\")\n",
    "                        n_comments = post.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa\")\n",
    "                        n_shares = post.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xi81zsa\")\n",
    "                        if n_shares:\n",
    "                            n_shares = n_shares.find_next(\"span\")\n",
    "\n",
    "                        # Lista de Comments - N√£o faz sentido para n√≥s pq s√≥ retira alguns\n",
    "                        # comments = post.find_all(\"div\", class_=\"x1y1aw1k xn6708d xwib8y2 x1ye3gou\")\n",
    "\n",
    "                        # Verificar se o post j√° existe na lista de IDs de postagens existentes\n",
    "                        if id_post[0] not in existing_post_ids:    \n",
    "                            # Criar dicion√°rio para o post\n",
    "                            post_dict = {\n",
    "                                \"page\": page.text.strip(),\n",
    "                                \"id\": id_post[0],\n",
    "                                \"post_text\": post_text.text.strip() if post_text else \"\",\n",
    "                                \"news_text\": news_text.text.strip() if post_text else \"\",\n",
    "                                \"date\": post_date.text.strip() if post_date else \"\",\n",
    "                                \"statistic\": n_estatisticas.text.strip() if n_estatisticas else \"\",\n",
    "                                \"reactions\": n_reacts.text.strip() if n_reacts else \"\",\n",
    "                                \"comments\": n_comments.text.strip() if n_comments else \"\",\n",
    "                                \"shares\": n_shares.text.strip() if n_shares else \"\",\n",
    "                                \"link\": link_post[0] if link_post else \"\"\n",
    "                            }\n",
    "\n",
    "                            # Adicionar o ID do post √† lista de IDs de postagens existentes\n",
    "                            existing_post_ids.add(id_post[0])\n",
    "\n",
    "                            # Adicionar o dicion√°rio √† lista de posts\n",
    "                            posts_data.append(post_dict)  \n",
    "                \n",
    "                last_result = soup.find(\"div\", class_= \"x9f619 x1n2onr6 x1ja2u2z x78zum5 xdt5ytf x2lah0s x193iq5w x1gslohp x12nagc xzboxd6 x14l7nz5\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping posts: {e}\")  \n",
    "        \n",
    "# Escrever a lista de dicion√°rios em um arquivo JSON\n",
    "with open(f'Facebook_DIGI_news_posts_{datetime.date.today()}.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(posts_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## üí≠ Coment√°rios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "71bcc16b331d419eb971580e906b788e",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "### üßÆ Importar as Bases de Dados dos Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "71194ce10a5c4900bcbd221eb2727539",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# Importar os post.json\n",
    "Vodafone_Posts = pd.read_json('Datasets/Facebook_Vodafone_posts_2024-03-17.json')\n",
    "MEO_Posts = pd.read_json('Datasets/Facebook_MEO_posts_2024-03-16.json')\n",
    "NOS_Posts = pd.read_json('Datasets/Facebook_NOS_posts_2024-03-16.json')\n",
    "DIGI_News_Posts = pd.read_json('Datasets/Facebook_DIGI_news_posts_2024-03-15.json')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Infos das Bases de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "71194ce10a5c4900bcbd221eb2727539",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# Observa√ß√µes Aleat√≥rias da base de dados de modo a ver a sua estrutura\n",
    "print(\"\\033[1mMEO\\033[0m\\nN√∫mero de colunas:\", len(MEO_Posts.columns), \"\\nN√∫mero de linhas:\", len(MEO_Posts), \"\\n\\nObserva√ß√µes Aleat√≥rias:\\n\")\n",
    "# MEO_Posts.sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mVodafone\\033[0m\\nN√∫mero de colunas:\", len(Vodafone_Posts.columns), \"\\nN√∫mero de linhas:\", len(Vodafone_Posts), \"\\n\\nObserva√ß√µes Aleat√≥rias:\\n\")\n",
    "# Vodafone_Posts.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mNOS\\033[0m\\nN√∫mero de colunas:\", len(NOS_Posts.columns), \"\\nN√∫mero de linhas:\", len(NOS_Posts), \"\\n\\nObserva√ß√µes Aleat√≥rias:\\n\")\n",
    "# NOS_Posts.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mDIGI [News]\\033[0m\\nN√∫mero de colunas:\", len(DIGI_News_Posts.columns), \"\\nN√∫mero de linhas:\", len(DIGI_News_Posts), \"\\n\\nObserva√ß√µes Aleat√≥rias:\\n\")\n",
    "# DIGI_News_Posts.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "71194ce10a5c4900bcbd221eb2727539",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# Verificar Posts duplicados\n",
    "print(\"\\033[1mPosts Duplicados da Vodafone:\\033[0m\", Vodafone_Posts.duplicated(subset='id').sum())\n",
    "print(\"\\033[1mPosts Duplicados da MEO:\\033[0m\", MEO_Posts.duplicated(subset='id').sum())\n",
    "print(\"\\033[1mPosts Duplicados da NOS:\\033[0m\", NOS_Posts.duplicated(subset='id').sum())\n",
    "print(\"\\033[1mPosts Duplicados da DIGI_News_Posts:\\033[0m\", DIGI_News_Posts.duplicated(subset='id').sum())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  üí≠ Coment√°rios  üü•üü¶‚¨õ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "```Python\n",
    "%%time\n",
    "# ========================================= Scraper dos Comments ==========================================================\n",
    "# Fonte: https://github.com/sesamechee/python-facebook-scraper/blob/master/scraper.py [Adaptado]\n",
    "\n",
    "# Lista para armazenar os dicion√°rios de coment√°rios e um conjunto para adicionar os IDs dos coment√°rios\n",
    "comments_data = []\n",
    "existing_comments_ids = set()\n",
    "\n",
    "# for post_link in tqdm(Vodafone_Posts['link']):\n",
    "# for post_link in tqdm(MEO_Posts['link'].iloc[X:]):\n",
    "# for post_link in tqdm(NOS_Posts['link'].iloc[(721 + 203 + 1):1000]):\n",
    "# for post_link in tqdm(NOS_Posts['link'].iloc[1254:]):\n",
    "for post_link in tqdm(Operadora_Posts['link']):\n",
    "\n",
    "    # Open post link\n",
    "    driver.get(post_link)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # ID do Post\n",
    "    post_id = Operadora_Posts[Operadora_Posts['link'] == post_link]['id'].values[0]           ########################## ------- MUDAR DF\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    next_page_comments = not None\n",
    "    comment_set_number = 0\n",
    "\n",
    "    while next_page_comments is not None:\n",
    "        \n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            # Scroll down to bottom \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Extract comments\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            comments = soup.find_all(\"div\", class_=\"x78zum5 xdt5ytf\")\n",
    "\n",
    "            for comment in comments:\n",
    "                try:\n",
    "                    # Link e Nome do User\n",
    "                    user = comment.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv\")\n",
    "                    if user:\n",
    "                        user_name = user.text.strip()\n",
    "                        user_link = user[\"href\"]\n",
    "\n",
    "                        # Texto do coment√°rio\n",
    "                        comment_text = comment.find(\"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\")\n",
    "\n",
    "                        # Data do coment√°rio\n",
    "                        comment_date = comment.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xt0b8zv xi81zsa xo1l8bm\")\n",
    "\n",
    "                        # ID do coment√°rio\n",
    "                        comment_link = comment_date[\"href\"]\n",
    "                        comment_id = re.findall(r'comment_id=(\\d+)', comment_link)\n",
    "\n",
    "                        # N√∫mero de rea√ß√µes\n",
    "                        comment_reactions = comment.find(\"div\", class_=\"x6s0dn4 x1a2cdl4 xnhgr82 x1qt0ttw xgk8upj x78zum5 x1ncwhqj xlup9mm x1nn3v0j xg83lxy x1120s5i x1h0ha7o x9bbmet xqnafso\")\n",
    "\n",
    "                        # N√∫mero de respostas\n",
    "                        comment_responses = comment.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen x1s688f xi81zsa\")\n",
    "\n",
    "                        if comment_text:\n",
    "                            # Verificar se o post j√° existe na lista de IDs de postagens existentes\n",
    "                            if comment_id[0] not in existing_comments_ids: \n",
    "                                # Criar dicion√°rio para o coment√°rio\n",
    "                                comment_dict = {\n",
    "                                    \"post_id\": str(post_id),\n",
    "                                    \"comment_id\": comment_id[0],\n",
    "                                    \"user_name\": user_name if user_name else \"\",\n",
    "                                    \"user_link\": user_link if user_link else \"\",\n",
    "                                    \"comment_text\": comment_text.text.strip() if comment_text else \"\",\n",
    "                                    \"comment_date\": comment_date.text.strip() if comment_date else \"\",\n",
    "                                    \"comment_reactions\": comment_reactions.text.strip() if comment_reactions else \"\",\n",
    "                                    \"comment_responses\": comment_responses.text.strip() if comment_responses else \"\",\n",
    "                                    \"comment_link\": comment_link if comment_link else \"\",\n",
    "                                }\n",
    "\n",
    "                                # Adicionar o dicion√°rio √† lista de coment√°rios\n",
    "                                comments_data.append(comment_dict)\n",
    "\n",
    "                                # Adicionar o ID do coment√°rio √† lista de IDs de coment√°rios existentes\n",
    "                                existing_comments_ids.add(comment_id[0])\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Erro ao processar coment√°rio: {e}\")    \n",
    "\n",
    "            # Pr√≥xima p√°gina de coment√°rios\n",
    "            next_page_comments = soup.find_all(\"span\", class_= \"x78zum5 x1w0mnb xeuugli\")\n",
    "\n",
    "            if next_page_comments:\n",
    "                if next_page_comments[-1].text == \"Ver mais coment√°rios\" and comment_set_number <= 50:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    element = driver.find_element(By.XPATH, f'/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div/div[2]/div/div/div/div[1]/div[4]/div[{comment_set_number * 50+8}]/div[1]/div/div[2]')\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView();\", element)\n",
    "                    element.click()\n",
    "\n",
    "                    time.sleep(1)\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    comment_set_number += 1\n",
    "\n",
    "                # M√ÅX = 2500 [comment_set_number <= 50] coment√°rios renderizados numa p√°gina porque apenas \n",
    "                # 5 dos posts t√™m mais de 2000 comet√°rios e a tem√°tica nada tem a ver com as operadoras\n",
    "                else:\n",
    "                    next_page_comments = None\n",
    "            else:\n",
    "                next_page_comments = None\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erro: {e}\")\n",
    "            # Continua para a pr√≥xima itera√ß√£o se n√£o conseguir algum link\n",
    "            continue\n",
    "            \n",
    "            \n",
    "# Escrever os dados dos coment√°rios em um arquivo JSON                          ########################## ------- MUDAR NOME\n",
    "with open(f'Facebook_Operadora_comments_{datetime.date.today()}.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(comments_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Fechar o navegador Selenium\n",
    "    driver.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Encontrar o 'id_post' especifico\n",
    "# MEO_Posts.index[MEO_Posts['id'] == 10157023959859570][0]\n",
    "# MEO_Posts[MEO_Posts['id'] == 10157023959859570]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Vodafone_Posts.index[Vodafone_Posts['id'] == 5566394966712254][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Vodafone_Posts.index[Vodafone_Posts['id'] == 1289433231075137][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# NOS_Posts.index[NOS_Posts['id'] == 3586234618056991][0] # - 00\n",
    "# NOS_Posts.index[NOS_Posts['id'] == 2693202527360209][0] # - 5\n",
    "# NOS_Posts.index[NOS_Posts['id'] == 1933085940038542][0] # - 9"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "###  üí≠ Coment√°rios  üü®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "%%time\n",
    "\n",
    "# ========================================= Scraper dos Comments ==========================================================\n",
    "# Fonte: https://github.com/sesamechee/python-facebook-scraper/blob/master/scraper.py [Adaptado]\n",
    "\n",
    "# link_teste = [\"https://www.facebook.com/jornalnoticias/posts/pfbid02WPYdh34GuAHAh3rUzRx7GmLrt8r61u8tMtAb1FhRoPa5SEeNYTTaGZKJyrJBYoBol\"]\n",
    "\n",
    "# Lista para armazenar os dicion√°rios de coment√°rios\n",
    "comments_data = []\n",
    "\n",
    "for post_link in tqdm(DIGI_News_Posts['link']):\n",
    "# for post_link in link_teste:\n",
    "\n",
    "    # Open post link\n",
    "    driver.get(post_link)\n",
    "    time.sleep(3)\n",
    "\n",
    "    # ID do Post\n",
    "    # post_id = DIGI_News_Posts[DIGI_News_Posts['link'] == post_link]['id'].values[0]\n",
    "    post_id = post_link\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    next_page_comments = not None\n",
    "    comment_set_number = 1\n",
    "\n",
    "    while next_page_comments is not None:\n",
    "\n",
    "        # Scroll down to bottom \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Extract comments\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        comments = soup.find_all(\"div\", class_=\"x1n2onr6 x1swvt13 x1iorvi4 x78zum5 x1q0g3np x1a2a7pz\")\n",
    "\n",
    "        for comment in comments:\n",
    "            try:\n",
    "                # Link e Nome do User\n",
    "                user = comment.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g xt0b8zv\")\n",
    "                if user:\n",
    "                    user_name = user.text.strip()\n",
    "                    user_link = user[\"href\"]\n",
    "\n",
    "                    # Texto do coment√°rio\n",
    "                    comment_text = comment.find(\"div\", class_=\"xdj266r x11i5rnm xat24cr x1mh8g0r x1vvkbs\")\n",
    "\n",
    "                    # Data do coment√°rio\n",
    "                    comment_date = comment.find(\"a\", class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz xt0b8zv xi81zsa xo1l8bm\")\n",
    "\n",
    "                    # ID do coment√°rio\n",
    "                    comment_link = comment_date[\"href\"]\n",
    "                    comment_id = re.findall(r'comment_id=(\\d+)', comment_link)\n",
    "\n",
    "                    # N√∫mero de rea√ß√µes\n",
    "                    comment_reactions = comment.find(\"div\", class_=\"x6s0dn4 x1a2cdl4 xnhgr82 x1qt0ttw xgk8upj x78zum5 x1ncwhqj xlup9mm x1nn3v0j xg83lxy x1120s5i x1h0ha7o x9bbmet xqnafso\")\n",
    "\n",
    "                    # N√∫mero de respostas\n",
    "                    comment_responses = comment.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen x1s688f xi81zsa\")\n",
    "\n",
    "                    if comment_text:\n",
    "                        # Criar dicion√°rio para o coment√°rio\n",
    "                        comment_dict = {\n",
    "                            \"post_id\": post_id,\n",
    "                            \"comment_id\":comment_id[0] if comment_id else \"\",\n",
    "                            \"user_name\": user_name if user_name else \"\",\n",
    "                            \"user_link\": user_link if user_link else \"\",\n",
    "                            \"comment_text\": comment_text.text.strip() if comment_text else \"\",\n",
    "                            \"comment_date\": comment_date.text.strip() if comment_date else \"\",\n",
    "                            \"comment_reactions\": comment_reactions.text.strip() if comment_reactions else \"\",\n",
    "                            \"comment_responses\": comment_responses.text.strip() if comment_responses else \"\",\n",
    "                            \"comment_link\": comment_link if comment_link else \"\",\n",
    "                        }\n",
    "\n",
    "                        # Adicionar o dicion√°rio √† lista de coment√°rios\n",
    "                        comments_data.append(comment_dict)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Erro ao processar coment√°rio: {e}\")    \n",
    "\n",
    "        # Pr√≥xima p√°gina de coment√°rios\n",
    "        next_page_comments = soup.find_all(\"span\", class_= \"x78zum5 x1w0mnb xeuugli\")\n",
    "        \n",
    "        if next_page_comments:\n",
    "            if next_page_comments[-1].text == \"Ver mais coment√°rios\":\n",
    "                driver.find_element(By.XPATH, f'/html/body/div[1]/div/div[1]/div/div[3]/div/div/div[1]/div[1]/div/div/div/div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[4]/div/div/div[2]/div[3]/div[{comment_set_number * 50+2}]/div[1]/div/div[2]').click()\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                comment_set_number += 1\n",
    "\n",
    "            else:\n",
    "                next_page_comments = None\n",
    "        else:\n",
    "            next_page_comments = None\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        \n",
    "        \n",
    "# Escrever os dados dos coment√°rios em um arquivo JSON\n",
    "with open(f'Facebook_DIGI_new_comments_{datetime.date.today()}.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(comments_data, json_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Fechar o navegador Selenium\n",
    "    driver.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## üôé‚Äç‚ôÇÔ∏èüôé‚Äç‚ôÄÔ∏è Utilizadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üôé‚Äç‚ôÇÔ∏èüôé‚Äç‚ôÄÔ∏è Utilizadores üü•üü¶‚¨õüü®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### üßÆ Importar as Bases de Dados dos Coment√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Importar base de dados Facebook_Comments.txt\n",
    "Facebook_Posts = pd.read_csv('Datasets_Vodafone/Facebook_Posts.txt', sep='\\t', encoding='utf-8')\n",
    "Facebook_Comments = pd.read_csv('Datasets_Vodafone/Facebook_Comments.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Garantir que as colunas 'post_id' e 'page' s√£o do tipo str\n",
    "Facebook_Comments['post_id'] = Facebook_Comments['post_id'].astype(str)\n",
    "Facebook_Comments['page'] = Facebook_Comments['page'].astype(str)\n",
    "\n",
    "Facebook_Posts['post_id'] = Facebook_Posts['post_id'].astype(str)\n",
    "Facebook_Posts['page'] = Facebook_Posts['page'].astype(str)\n",
    "\n",
    "# Juntar as tabelas\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Comments, Facebook_Posts, how='outer', on=['post_id', 'page'])\n",
    "Facebook_Posts_Comments.reset_index(drop=True, inplace=True)\n",
    "# Facebook_Posts_Comments.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# --------------------------------------------------------------------------------------------\n",
    "# Filtrar os users com 'post_year' maior ou igual a 2019\n",
    "Users = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]\n",
    "print(f'\\033[1mN¬∫ de Users [Com Duplicados] a obter dos coment√°rios entre 2019-2024:\\033[0m {len(Users)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Filtrar os users com 'user_link' √∫nico\n",
    "Users = Users[~Users.duplicated(subset=['user_link'])][['user_name','user_link']]\n",
    "Users.reset_index(drop=True, inplace=True)\n",
    "print(f'\\033[1mN¬∫ de Users [√önicos] a obter dos coment√°rios entre 2019-2024:\\033[0m {len(Users)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019][['user_name','user_link']].drop_duplicates()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Importar as bases de dados dos Users j√° obtidos\n",
    "DIGI_News_Users = pd.read_json('Datasets/Users/Facebook_DIGI_News_users_2024-03-21.json')\n",
    "\n",
    "# Fazer drop da coluna 'genero'\n",
    "DIGI_News_Users.drop(columns=['genero'], inplace=True)\n",
    "\n",
    "# Renomear as colunas 'cidade_atual' e 'naturalidade' para 'user_current_city' e 'user_hometown'\n",
    "DIGI_News_Users.rename(columns={'cidade_atual': 'user_current_city', 'naturalidade': 'user_hometown'}, inplace=True)\n",
    "\n",
    "Users_1_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_1.json', encoding='utf-8')\n",
    "Users_2_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_2.json', encoding='utf-8')\n",
    "Users_3_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_3.json', encoding='utf-8')\n",
    "Users_4_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_4.json', encoding='utf-8')\n",
    "Users_5_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_5.json', encoding='utf-8')\n",
    "Users_6_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_6.json', encoding='utf-8')\n",
    "Users_7_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_7.json', encoding='utf-8')\n",
    "\n",
    "Users_8_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_8.json', encoding='utf-8')\n",
    "Users_9_AS = pd.read_json('Datasets/Users/Facebook_Users_AS_9.json', encoding='utf-8')\n",
    "\n",
    "Users_1_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_1.json', encoding='utf-8')\n",
    "Users_2_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_2.json', encoding='utf-8')\n",
    "Users_3_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_3.json', encoding='utf-8')\n",
    "Users_4_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_4.json', encoding='utf-8')\n",
    "\n",
    "Users_5_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_5.json', encoding='utf-8')\n",
    "Users_6_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_6.json', encoding='utf-8')\n",
    "Users_7_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_7.json', encoding='utf-8')\n",
    "Users_8_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_8.json', encoding='utf-8')\n",
    "Users_9_MJ = pd.read_json('Datasets/Users/Facebook_Users_MJ_9.json', encoding='utf-8')\n",
    "\n",
    "Users_1_UM = pd.read_json('Datasets/Users/Facebook_Users_U_1.json', encoding='utf-8')\n",
    "Users_2_UM = pd.read_json('Datasets/Users/Facebook_Users_U_2.json', encoding='utf-8')\n",
    "Users_3_UM = pd.read_json('Datasets/Users/Facebook_Users_U_3.json', encoding='utf-8')\n",
    "Users_4_UM = pd.read_json('Datasets/Users/Facebook_Users_U_4.json', encoding='utf-8')\n",
    "Users_5_UM = pd.read_json('Datasets/Users/Facebook_Users_U_5.json', encoding='utf-8')\n",
    "\n",
    "Users_6_UM = pd.read_json('Datasets/Users/Facebook_Users_U_6.json', encoding='utf-8')\n",
    "Users_7_UM = pd.read_json('Datasets/Users/Facebook_Users_U_7.json', encoding='utf-8')\n",
    "Users_8_UM = pd.read_json('Datasets/Users/Facebook_Users_U_8.json', encoding='utf-8')\n",
    "Users_9_UM = pd.read_json('Datasets/Users/Facebook_Users_U_9.json', encoding='utf-8')\n",
    "Users_10_UM = pd.read_json('Datasets/Users/Facebook_Users_U_10.json', encoding='utf-8')\n",
    "Users_11_UM = pd.read_json('Datasets/Users/Facebook_Users_U_11.json', encoding='utf-8')\n",
    "\n",
    "Users_1_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_1.json', encoding='utf-8')\n",
    "Users_2_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_2.json', encoding='utf-8')\n",
    "\n",
    "Users_3_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_3.json', encoding='utf-8')\n",
    "Users_4_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_4.json', encoding='utf-8')\n",
    "Users_5_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_5.json', encoding='utf-8')\n",
    "Users_6_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_6.json', encoding='utf-8')\n",
    "Users_7_MM = pd.read_json('Datasets/Users/Facebook_Users_MM_7.json', encoding='utf-8')\n",
    "\n",
    "# Concatenar os Users obtidos\n",
    "Users_Obtidos = pd.concat(\n",
    "    [DIGI_News_Users,\n",
    "     Users_1_AS, Users_2_AS, Users_3_AS, Users_4_AS, Users_5_AS, Users_6_AS, Users_7_AS, Users_8_AS, Users_9_AS,\n",
    "     Users_1_MJ, Users_2_MJ, Users_3_MJ, Users_4_MJ, Users_5_MJ, Users_6_MJ, Users_7_MJ, Users_8_MJ, Users_9_MJ,\n",
    "     Users_1_UM, Users_2_UM, Users_3_UM, Users_4_UM, Users_5_UM, Users_6_UM, Users_7_UM, Users_8_UM, Users_9_UM, Users_10_UM, Users_11_UM,\n",
    "     Users_1_MM, Users_2_MM, Users_3_MM, Users_4_MM, Users_5_MM, Users_6_MM, Users_7_MM\n",
    "    ], ignore_index=True)\n",
    "\n",
    "# Remover duplicados\n",
    "Users_Obtidos.drop_duplicates(subset=['user_link'], inplace=True, keep='first')\n",
    "Users_Obtidos.reset_index(drop=True, inplace=True)\n",
    "print(\"\\033[1mTotal de Users N√£o Duplicados Obtidos\\033[0m\", len(Users_Obtidos))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Merge dos dataframes Users e Users_Obtidos\n",
    "Users_Por_Obter = Users.merge(Users_Obtidos[['user_link']], how='left', on='user_link', indicator=True)\n",
    "\n",
    "# Selecionar apenas as linhas que est√£o apenas no dataframe Users\n",
    "Users_Por_Obter = Users_Por_Obter[Users_Por_Obter['_merge'] == 'left_only']\n",
    "\n",
    "# Remover a coluna de indica√ß√£o de merge\n",
    "Users_Por_Obter.drop(columns=['_merge'], inplace=True)\n",
    "Users_Por_Obter.reset_index(drop=True, inplace=True)\n",
    "print(\"\\033[1mTotal de Users Por Recolher\\033[0m\", len(Users_Por_Obter.dropna()), '\\n')\n",
    "Users_Por_Obter.dropna(inplace=True)\n",
    "Users_Por_Obter"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Conclu√≠do o *scraping* dos *users* a **10/04/2024**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "################## LISTA [4] ##################\n",
    "# Andr√©    - 0      : 10 000  -----------------\n",
    "# MM       - 10 000 : 24 000  -----------------\n",
    "# MJ       - 24 000 : 38 000  -----------------\n",
    "# Umeima   - 38 000 : 51 865  -----------------\n",
    "\n",
    "%%time\n",
    "# ========================================= Scraper dos Comments ==========================================================\n",
    "# Fonte: https://github.com/sesamechee/python-facebook-scraper/blob/master/scraper.py [Adaptado]\n",
    "# user_link_fb = ['about_places', 'about_contact_and_basic_info'] \n",
    "\n",
    "# Lista para armazenar os dicion√°rios de users\n",
    "users_data = []\n",
    "links_por_fazer = []\n",
    "\n",
    "for user_link in tqdm(Users_Por_Obter['user_link']):  ##.iloc[8815:10000]):        ######## MUDAR N√öMERO\n",
    "\n",
    "    # Link e Nome do User\n",
    "    user_name = Users[Users['user_link'] == user_link]['user_name'].values[0]\n",
    "    \n",
    "    try:\n",
    "        # for link in user_link_fb: \n",
    "        # [Opt√°mos por s√≥ obter a informa√ß√£o crucial (Local de Resid√™ncia do User) devido ao elevado n√∫mero de utilizadores]\n",
    "        cidade_atual = ''\n",
    "        naturalidade = ''\n",
    "            \n",
    "        if 'profile.php?' in user_link:\n",
    "            # Open about-user links\n",
    "            driver.get(f'{user_link}&sk=about_places') \n",
    "            # Para se adaptar aos outros urls para obter mais informa√ß√£o seria\n",
    "            # driver.get(f'{user_link}&sk={link}') \n",
    "        else:\n",
    "            # Open about-user links\n",
    "            driver.get(f'{user_link}/about_places')\n",
    "            # driver.get(f'{user_link}/{link}')\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Extract users\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 'Local de Resid√™ncia' do User\n",
    "        # if link == \"about_places\":\n",
    "        locais = soup.find_all(\"div\", class_=\"x9f619 x1n2onr6 x1ja2u2z x78zum5 xdt5ytf x193iq5w xeuugli x1r8uery x1iyjqo2 xs83m0k xamitd3 xsyo7zv x16hj40l x10b6aqq x1yrsyyn\")\n",
    "\n",
    "        # Informa√ß√µes dos 'Locais'\n",
    "        if locais:\n",
    "            for local in locais:\n",
    "                if local:\n",
    "                    if \"Cidade atual\" in local.text:\n",
    "                        cidade_atual = local.text  # re.sub(r'Cidade atual.*', '',\" -> Para a Limpeza\n",
    "\n",
    "                    elif \"Naturalidade\" in local.text:\n",
    "                        naturalidade = local.text # re.sub(r'Naturalidade.*', '',\n",
    "                        \n",
    "                    else:\n",
    "                        continue\n",
    "                        \n",
    "        else:\n",
    "            cidade_atual = ''\n",
    "            naturalidade = ''\n",
    "\n",
    "        # # 'G√©nero' do User [Caso fizessemos pelo webscraping]\n",
    "        # if link == \"about_contact_and_basic_info\":\n",
    "        #     time.sleep(2)\n",
    "        #     genero_div = soup.find(\"span\", class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x3x7a5m x6prxxf xvq8zen xo1l8bm xzsf02u x1yc453h\")\n",
    "        #     if genero_div:\n",
    "        #        genero = genero_div.text\n",
    "        #    else:\n",
    "        #        genero = ''\n",
    "        \n",
    "        # Caso a conta seja bloqueada\n",
    "        bloq = soup.find(\"div\", class_=\"x6s0dn4 x78zum5 xdt5ytf xl56j7k x1p5oq8j xxbr6pl xwxc41k xbbxn1n\")\n",
    "        if bloq:\n",
    "            if \"Est√°s temporariamente bloquead\" in bloq.text:\n",
    "                # Faz print do 'user_link' e 'index' e guarda numa lista\n",
    "                print(f\"Conta bloqueada temporariamente no: {user_link} | Index: {Users_Por_Obter.index[Users_Por_Obter['user_link'] == user_link].tolist()[0]}\")\n",
    "                links_por_fazer.append(user_link)\n",
    "                \n",
    "                tqdm._instances.clear()\n",
    "                \n",
    "                # Continua para a pr√≥xima itera√ß√£o sem guardar dados\n",
    "                continue            \n",
    "\n",
    "        # Criar dicion√°rio para o utilizador\n",
    "        user_dict = {\n",
    "            \"user_link\": user_link,\n",
    "            \"user_name\": user_name,\n",
    "            \"user_current_city\": cidade_atual,\n",
    "            \"user_hometown\": naturalidade\n",
    "            # \"user_gender\": genero.strip()\n",
    "        }\n",
    "\n",
    "        # Adicionar o dicion√°rio √† lista de utilizadores\n",
    "        users_data.append(user_dict)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "        # Continua para a pr√≥xima itera√ß√£o se n√£o conseguir algum link\n",
    "        continue\n",
    "\n",
    "\n",
    "# df_links_por_fazer = pd.DataFrame(links_por_fazer, columns=[\"user_link\"])\n",
    "# df_links_por_fazer.to_csv('Links_por_fazer_AS_5.csv', index=False) ## ALTERAR SIGLA DO VOSSO NOME E ALTERAR NUMERO \n",
    "\n",
    "# Escrever os dados dos utilizadores num arquivo JSON\n",
    "with open(f'Facebook_Users_AS_9.json', 'w', encoding='utf-8') as json_file:  \n",
    "    json.dump(users_data, json_file, ensure_ascii=False, indent=4) ## ALTERAR SIGLA DO VOSSO NOME E ALTERAR NUMERO \n",
    "    \n",
    "    # Fechar o navegador Selenium\n",
    "    driver.quit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "2b71365634154f5eb4965f554f2e65ea",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar os duplicados da concatena√ß√£o\n",
    "print(\"\\033[1mUsers Duplicados:\\033[0m\", len(Users_Obtidos) - \n",
    "      len(Users_Obtidos.drop_duplicates(subset=['user_link'], keep='first')))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Total de Users Extraidos [Total = 129 096] - 114 198\n",
    "print('\\033[1mTotal\\033[0m:', len(Users_Obtidos))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Guardar os dados RAW dos Users em .JSON compilado\n",
    "# with open('Datasets_Vodafone/Facebook_Users.json', 'w', encoding='utf-8') as file:\n",
    "#     Users_Obtidos.to_json(file, orient='records', force_ascii=False, indent=4)\n",
    "\n",
    "with open('Datasets/Facebook_Users.json', 'w', encoding='utf-8') as file:\n",
    "     Users_Obtidos.to_json(file, orient='records', force_ascii=False, indent=4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <a class='anchor' id='1.2'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Pr√©-Processamento de Dados</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üßÆ Importar as Bases de Dados dos Posts, Coment√°rios e Users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Importar os post.json\n",
    "Vodafone_Posts = pd.read_json('Datasets/Facebook_Vodafone_posts_2024-03-17.json')\n",
    "MEO_Posts = pd.read_json('Datasets/Facebook_MEO_posts_2024-03-16.json')\n",
    "NOS_Posts = pd.read_json('Datasets/Facebook_NOS_posts_2024-03-16.json')\n",
    "DIGI_News_Posts = pd.read_json('Datasets/Facebook_DIGI_news_posts_2024-03-15.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Verificar duplicados\n",
    "print(\"\\033[1mPosts duplicados\\033[0m\")\n",
    "print(\"Vodafone:\", len(Vodafone_Posts) - len(Vodafone_Posts.drop_duplicates(subset=['id'], keep='first')))\n",
    "print(\"MEO:\", len(MEO_Posts) - len(MEO_Posts.drop_duplicates(subset=['id'], keep='first')))\n",
    "print(\"NOS:\", len(NOS_Posts) - len(NOS_Posts.drop_duplicates(subset=['id'], keep='first')))\n",
    "print(\"DIGI News:\", len(DIGI_News_Posts) - len(DIGI_News_Posts.drop_duplicates(subset=['id'], keep='first')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Remover duplicados dos posts, se existirem e verificar o N¬∫ Total de Posts por operadora\n",
    "Vodafone_Posts.drop_duplicates(subset=['id'], inplace=True, keep='first')\n",
    "MEO_Posts.drop_duplicates(subset=['id'], inplace=True, keep='first')\n",
    "NOS_Posts.drop_duplicates(subset=['id'], inplace=True, keep='first')\n",
    "DIGI_News_Posts.drop_duplicates(subset=['id'], inplace=True, keep='first')\n",
    "\n",
    "print(\"\\033[1mTotal de Posts N√£o Duplicados\\033[0m\")\n",
    "print(\"Vodafone:\", len(Vodafone_Posts))\n",
    "print(\"MEO:\", len(MEO_Posts))\n",
    "print(\"NOS:\", len(NOS_Posts))\n",
    "print(\"DIGI News:\", len(DIGI_News_Posts))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### üîóConcatenar as Bases de Dados dos Coment√°rios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Importar os comments.json\n",
    "Vodafone_Comments_000 = pd.read_json('Datasets/Facebook_Vodafone_comments_000_2024-03-23.json')\n",
    "Vodafone_Comments_00 = pd.read_json('Datasets/Facebook_Vodafone_comments_00_2024-03-23.json')\n",
    "Vodafone_Comments_0 = pd.read_json('Datasets/Facebook_Vodafone_comments_0_2024-03-20.json')\n",
    "Vodafone_Comments_1 = pd.read_json('Datasets/Facebook_Vodafone_comments_1_2024-03-21.json')\n",
    "Vodafone_Comments_2 = pd.read_json('Datasets/Facebook_Vodafone_comments_2_2024-03-22.json')\n",
    "\n",
    "MEO_Comments_0 = pd.read_json('Datasets/Facebook_MEO_comments_0_2024-03-21.json')\n",
    "MEO_Comments_1 = pd.read_json('Datasets/Facebook_MEO_comments_1_2024-03-21.json')\n",
    "MEO_Comments_2 = pd.read_json('Datasets/Facebook_MEO_comments_2_2024-03-22.json')\n",
    "MEO_Comments_3 = pd.read_json('Datasets/Facebook_MEO_comments_3_2024-03-23.json')\n",
    "\n",
    "NOS_Comments_00 = pd.read_json('Datasets/Facebook_NOS_comments_00_2024-03-24.json')\n",
    "NOS_Comments_0 = pd.read_json('Datasets/Facebook_NOS_comments_0_2024-03-20.json')\n",
    "NOS_Comments_1 = pd.read_json('Datasets/Facebook_NOS_comments_1_2024-03-21.json')\n",
    "NOS_Comments_2 = pd.read_json('Datasets/Facebook_NOS_comments_2_2024-03-22.json')\n",
    "NOS_Comments_3 = pd.read_json('Datasets/Facebook_NOS_comments_3_2024-03-22.json')\n",
    "NOS_Comments_4 = pd.read_json('Datasets/Facebook_NOS_comments_4_2024-03-23.json')\n",
    "NOS_Comments_5 = pd.read_json('Datasets/Facebook_NOS_comments_5_2024-03-24.json')\n",
    "NOS_Comments_6 = pd.read_json('Datasets/Facebook_NOS_comments_6_2024-03-24.json')\n",
    "NOS_Comments_7 = pd.read_json('Datasets/Facebook_NOS_comments_7_2024-03-24.json')\n",
    "NOS_Comments_8 = pd.read_json('Datasets/Facebook_NOS_comments_8_2024-03-24.json')\n",
    "NOS_Comments_9 = pd.read_json('Datasets/Facebook_NOS_comments_9_2024-03-24.json')\n",
    "NOS_Comments_10 = pd.read_json('Datasets/Facebook_NOS_comments_10_2024-03-25.json')\n",
    "\n",
    "DIGI_News_Comments = pd.read_json('Datasets/Facebook_DIGI_new_comments_2024-03-17.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ==================== Limpeza extra para o DIGI_News_Comments ===========================\n",
    "# A base de dados da DIGI teve estas limpezas extra dado ter sido a primeira a ser obtida\n",
    "# tendo-se feito ajustes no c√≥digo posteriores para as restantes n√£o terem de ser limpas com estas informa√ß√µes \n",
    "\n",
    "# Aplique a express√£o regular em cada observa√ß√£o da coluna 'post_id' para ficar apenas o ID\n",
    "DIGI_News_Comments['post_id'] = DIGI_News_Comments['post_id'].apply(lambda x: re.findall(r'/posts/([^/?]+)', x)[0] if re.findall(r'/posts/([^/?]+)', x) else None)\n",
    "\n",
    "# Converter a coluna 'comment_id' para inteiro\n",
    "DIGI_News_Comments['comment_id'] = DIGI_News_Comments['comment_id'].apply(lambda x: int(x[0]))\n",
    "\n",
    "# Simplificar a coluna 'user_link'\n",
    "DIGI_News_Comments['user_link'] = DIGI_News_Comments['user_link'].apply(lambda x: re.sub(r'\\?comment_id=.*$', '', x))\n",
    "DIGI_News_Comments['user_link'] = DIGI_News_Comments['user_link'].apply(lambda x: re.sub(r'&comment_id=.*$', '', x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "print(\"\\n\\033[1mComent√°rios duplicados\\033[0m\")\n",
    "\n",
    "print(\"Vodafone 000:\", len(Vodafone_Comments_000) - len(Vodafone_Comments_000.drop_duplicates(keep='first')))\n",
    "print(\"Vodafone 00:\", len(Vodafone_Comments_00) - len(Vodafone_Comments_00.drop_duplicates(keep='first')))\n",
    "print(\"Vodafone 0:\", len(Vodafone_Comments_0) - len(Vodafone_Comments_0.drop_duplicates(keep='first')))\n",
    "print(\"Vodafone 1:\", len(Vodafone_Comments_1) - len(Vodafone_Comments_1.drop_duplicates(keep='first')))\n",
    "print(\"Vodafone 2:\", len(Vodafone_Comments_2) - len(Vodafone_Comments_2.drop_duplicates(keep='first')), '\\n')\n",
    "\n",
    "print(\"MEO 0:\", len(MEO_Comments_0) - len(MEO_Comments_0.drop_duplicates(keep='first')))\n",
    "print(\"MEO 1:\", len(MEO_Comments_1) - len(MEO_Comments_1.drop_duplicates(keep='first')))\n",
    "print(\"MEO 2:\", len(MEO_Comments_2) - len(MEO_Comments_2.drop_duplicates(keep='first')))\n",
    "print(\"MEO 3:\", len(MEO_Comments_3) - len(MEO_Comments_3.drop_duplicates(keep='first')), '\\n')\n",
    "\n",
    "print(\"NOS 00:\", len(NOS_Comments_00) - len(NOS_Comments_00.drop_duplicates(keep='first')))\n",
    "print(\"NOS 0:\", len(NOS_Comments_0) - len(NOS_Comments_0.drop_duplicates(keep='first')))\n",
    "print(\"NOS 1:\", len(NOS_Comments_1) - len(NOS_Comments_1.drop_duplicates(keep='first')))\n",
    "print(\"NOS 2:\", len(NOS_Comments_2) - len(NOS_Comments_2.drop_duplicates(keep='first')))\n",
    "print(\"NOS 3:\", len(NOS_Comments_3) - len(NOS_Comments_3.drop_duplicates(keep='first')))\n",
    "print(\"NOS 4:\", len(NOS_Comments_4) - len(NOS_Comments_4.drop_duplicates(keep='first')))\n",
    "print(\"NOS 5:\", len(NOS_Comments_5) - len(NOS_Comments_5.drop_duplicates(keep='first')))\n",
    "print(\"NOS 6:\", len(NOS_Comments_6) - len(NOS_Comments_6.drop_duplicates(keep='first')))\n",
    "print(\"NOS 7:\", len(NOS_Comments_7) - len(NOS_Comments_7.drop_duplicates(keep='first')))\n",
    "print(\"NOS 8:\", len(NOS_Comments_8) - len(NOS_Comments_8.drop_duplicates(keep='first')))\n",
    "print(\"NOS 9:\", len(NOS_Comments_9) - len(NOS_Comments_9.drop_duplicates(keep='first')))\n",
    "print(\"NOS 10:\", len(NOS_Comments_10) - len(NOS_Comments_10.drop_duplicates(keep='first')), '\\n')\n",
    "\n",
    "print(\"DIGI News:\", len(DIGI_News_Comments) - len(DIGI_News_Comments.drop_duplicates(keep='first')))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Remover duplicados dos comments\n",
    "Vodafone_Comments_000.drop_duplicates(inplace=True, keep='first')\n",
    "Vodafone_Comments_00.drop_duplicates(inplace=True, keep='first')\n",
    "Vodafone_Comments_0.drop_duplicates(inplace=True, keep='first')\n",
    "Vodafone_Comments_1.drop_duplicates(inplace=True, keep='first')\n",
    "Vodafone_Comments_2.drop_duplicates(inplace=True, keep='first')\n",
    "\n",
    "MEO_Comments_0.drop_duplicates(inplace=True, keep='first')\n",
    "MEO_Comments_1.drop_duplicates(inplace=True, keep='first')\n",
    "MEO_Comments_2.drop_duplicates(inplace=True, keep='first')\n",
    "MEO_Comments_3.drop_duplicates(inplace=True, keep='first')\n",
    "\n",
    "NOS_Comments_00.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_0.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_1.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_2.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_3.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_4.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_5.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_6.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_7.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_8.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_9.drop_duplicates(inplace=True, keep='first')\n",
    "NOS_Comments_10.drop_duplicates(inplace=True, keep='first')\n",
    "\n",
    "DIGI_News_Comments.drop_duplicates(inplace=True)\n",
    "\n",
    "# Valores dos Dataframes Finais (s/ duplicados)\n",
    "print(len(Vodafone_Comments_000),len(Vodafone_Comments_00),len(Vodafone_Comments_0),\n",
    "      len(Vodafone_Comments_1), len(Vodafone_Comments_2))\n",
    "\n",
    "print(len(MEO_Comments_0), len(MEO_Comments_1), len(MEO_Comments_2), len(MEO_Comments_3))\n",
    "\n",
    "print(len(NOS_Comments_00), len(NOS_Comments_0), len(NOS_Comments_1), len(NOS_Comments_2), len(NOS_Comments_3), \n",
    "      len(NOS_Comments_4), len(NOS_Comments_5), len(NOS_Comments_6), len(NOS_Comments_7), len(NOS_Comments_8), \n",
    "      len(NOS_Comments_9), len(NOS_Comments_10))\n",
    "\n",
    "print(len(DIGI_News_Comments))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Concatenar os DataFrames de cada operadora\n",
    "Vodafone_Comments = pd.concat([Vodafone_Comments_000,Vodafone_Comments_00,Vodafone_Comments_0, \n",
    "                                      Vodafone_Comments_1, Vodafone_Comments_2], ignore_index=True)\n",
    "\n",
    "MEO_Comments = pd.concat([MEO_Comments_0, MEO_Comments_1, MEO_Comments_2, MEO_Comments_3], ignore_index=True)\n",
    "\n",
    "NOS_Comments = pd.concat([NOS_Comments_00, NOS_Comments_0, NOS_Comments_1, NOS_Comments_2,\n",
    "                          NOS_Comments_3, NOS_Comments_4, NOS_Comments_5, NOS_Comments_6, \n",
    "                          NOS_Comments_7, NOS_Comments_8, NOS_Comments_9, NOS_Comments_10], ignore_index=True)\n",
    "\n",
    "# Verificar os duplicados da concatena√ß√£o\n",
    "print(\"\\033[1mVodafone Duplicados:\\033[0m\", len(Vodafone_Comments) - len(Vodafone_Comments.drop_duplicates(subset=['comment_id'], keep='first')))\n",
    "print(\"\\033[1mMEO Duplicados:\\033[0m\", len(MEO_Comments) - len(MEO_Comments.drop_duplicates(subset=['comment_id'], keep='first')))\n",
    "print(\"\\033[1mNOS Duplicados:\\033[0m\", len(NOS_Comments) - len(NOS_Comments.drop_duplicates(subset=['comment_id'], keep='first')))\n",
    "\n",
    "# Remover duplicatas dos DataFrames concatenados\n",
    "Vodafone_Comments.drop_duplicates(subset=['comment_id'], inplace=True, keep='first')\n",
    "MEO_Comments.drop_duplicates(subset=['comment_id'], inplace=True, keep='first')\n",
    "NOS_Comments.drop_duplicates(subset=['comment_id'], inplace=True, keep='first')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Contar o n√∫mero de observa√ß√µes com 'comment_id' =\n",
    "print(\"\\n\\033[1mVodafone Coment√°rios duplicados\\033[0m:\", len(Vodafone_Comments[Vodafone_Comments.duplicated(subset='comment_id', keep=False)]))\n",
    "print(\"\\033[1mMEO Coment√°rios duplicados\\033[0m:\", len(MEO_Comments[MEO_Comments.duplicated(subset='comment_id', keep=False)]))\n",
    "print(\"\\033[1mNOS Coment√°rios duplicados\\033[0m:\", len(NOS_Comments[NOS_Comments.duplicated(subset='comment_id', keep=False)]))\n",
    "print(\"\\033[1mDIGI News Coment√°rios duplicados\\033[0m:\", len(DIGI_News_Comments[DIGI_News_Comments.duplicated(subset='comment_id', keep=False)]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# N¬∫ de Observa√ß√µes finais sem duplicados\n",
    "print(\"\\n\\033[1mVodafone Coment√°rios\\033[0m:\", len(Vodafone_Comments))\n",
    "print(\"\\033[1mMEO Coment√°rios\\033[0m:\", len(MEO_Comments))\n",
    "print(\"\\033[1mNOS Coment√°rios\\033[0m:\", len(NOS_Comments))\n",
    "print(\"\\033[1mDIGI News Coment√°rios\\033[0m:\", len(DIGI_News_Comments))\n",
    "\n",
    "# Total de Coment√°rios Extraidos [Total = 435 074]\n",
    "print('\\033[1mTotal\\033[0m:', len(Vodafone_Comments) + len(MEO_Comments) + len(NOS_Comments) + len(DIGI_News_Comments))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Guardar as bases de dados finais compiladas\n",
    "with open('Datasets/Facebook_Vodafone_comments.json', 'w', encoding='utf-8') as file:\n",
    "    Vodafone_Comments.to_json(file, orient='records', force_ascii=False, indent=4)\n",
    "    \n",
    "with open('Datasets/Facebook_MEO_comments.json', 'w', encoding='utf-8') as file:\n",
    "    MEO_Comments.to_json(file, orient='records', force_ascii=False, indent=4)\n",
    "\n",
    "with open('Datasets/Facebook_NOS_comments.json', 'w', encoding='utf-8') as file:\n",
    "    NOS_Comments.to_json(file, orient='records', force_ascii=False, indent=4)\n",
    "\n",
    "with open('Datasets/Facebook_DIGI_News_comments.json', 'w', encoding='utf-8') as file:\n",
    "    DIGI_News_Comments.to_json(file, orient='records', force_ascii=False, indent=4)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "33404c7792ac48ddb7d3190658d0a0c0",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "## üßπ Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Importar as Bases de Dados Completas e s/duplicados\n",
    "# Importar os post.json\n",
    "Vodafone_Posts = pd.read_json('Datasets_Vodafone/Facebook_Vodafone_posts_2024-03-17.json')\n",
    "MEO_Posts = pd.read_json('Datasets_Vodafone/Facebook_MEO_posts_2024-03-16.json')\n",
    "NOS_Posts = pd.read_json('Datasets_Vodafone/Facebook_NOS_posts_2024-03-16.json')\n",
    "DIGI_News_Posts = pd.read_json('Datasets_Vodafone/Facebook_DIGI_news_posts_2024-03-15.json')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Importar os comments.json\n",
    "Vodafone_Comments = pd.read_json('Datasets_Vodafone/Facebook_Vodafone_comments.json')\n",
    "MEO_Comments = pd.read_json('Datasets_Vodafone/Facebook_MEO_comments.json')\n",
    "NOS_Comments = pd.read_json('Datasets_Vodafone/Facebook_NOS_comments.json')\n",
    "DIGI_News_Comments = pd.read_json('Datasets_Vodafone/Facebook_DIGI_News_comments.json')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vodafone_Posts[:2]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vodafone_Comments.sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Users.sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print('\\033[1mTotal de Posts:\\033[0m', len(Vodafone_Posts) + len(MEO_Posts) + len(NOS_Posts) + len(DIGI_News_Posts))\n",
    "print('\\033[1mTotal de Comments:\\033[0m', \n",
    "      len(Vodafone_Comments) + len(MEO_Comments) + len(NOS_Comments) + len(DIGI_News_Comments))\n",
    "# print('\\033[1mTotal de Users:\\033[0m', len(Users))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üìå Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Valores poss√≠veis da vari√°vel 'post_date'\n",
    "print('\\033[1mVodafone\\033[0m\\n', Vodafone_Posts['date'].unique())\n",
    "print('\\033[1mMEO\\033[0m\\n', MEO_Posts['date'].unique())\n",
    "print('\\033[1mNOS\\033[0m\\n', NOS_Posts['date'].unique())\n",
    "print('\\033[1mDIGI News\\033[0m\\n', DIGI_News_Posts['date'].unique()[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Limpar as datas que t√™m \"X d\"\n",
    "# Data de refer√™ncia (2024-03-16) --------------- COLOCAR A DATA DO DOCUMENTO\n",
    "# Dicion√°rio com os 7 dias antes do dia de refer√™ncia\n",
    "\n",
    "# Substituir a data apenas nos casos que correspondem ao padr√£o 'X h'\n",
    "MEO_Posts.loc[MEO_Posts['date'].str.contains(r'\\d+ h'), 'date'] = '15 de mar√ßo de 2024'\n",
    "\n",
    "mapeamento_datas = {\n",
    "    '1 d': '15 de mar√ßo de 2024', '1d': '15 de mar√ßo de 2024',\n",
    "    '2 d': '14 de mar√ßo de 2024', '2d': '14 de mar√ßo de 2024',\n",
    "    '3 d': '13 de mar√ßo de 2024', '3d': '13 de mar√ßo de 2024',\n",
    "    '4 d': '12 de mar√ßo de 2024', '4d': '12 de mar√ßo de 2024',\n",
    "    '5 d': '11 de mar√ßo de 2024', '5d': '11 de mar√ßo de 2024',\n",
    "    '6 d': '10 de mar√ßo de 2024', '6d': '10 de mar√ßo de 2024',\n",
    "    '7 d': '9 de mar√ßo de 2024', '7d': '9 de mar√ßo de 2024'\n",
    "}\n",
    "\n",
    "meses_portugues_para_numero = {\n",
    "    'janeiro': 1,\n",
    "    'fevereiro': 2,\n",
    "    'mar√ßo': 3,\n",
    "    'abril': 4,\n",
    "    'maio': 5,\n",
    "    'junho': 6,\n",
    "    'julho': 7,\n",
    "    'agosto': 8,\n",
    "    'setembro': 9,\n",
    "    'outubro': 10,\n",
    "    'novembro': 11,\n",
    "    'dezembro': 12\n",
    "}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Substituir as datas na coluna \"date\" com base no dicion√°rio mapeamento_datas, mantendo os valores sem correspond√™ncia\n",
    "Vodafone_Posts['date'] = Vodafone_Posts['date'].map(lambda x: mapeamento_datas.get(x, x), na_action='ignore').str.strip()\n",
    "MEO_Posts['date'] = MEO_Posts['date'].map(lambda x: mapeamento_datas.get(x, x), na_action='ignore').str.strip()\n",
    "NOS_Posts['date'] = NOS_Posts['date'].map(lambda x: mapeamento_datas.get(x, x), na_action='ignore').str.strip()\n",
    "DIGI_News_Posts['date'] = DIGI_News_Posts['date'].map(lambda x: mapeamento_datas.get(x, x), na_action='ignore').str.strip()\n",
    "\n",
    "# Limpeza da coluna \"date\"\n",
    "Vodafone_Posts[['day', 'month', 'year', 'hour']]= Vodafone_Posts['date'].str.extract(r'(?:(\\d+) de )?(\\w+)?(?: de (\\d+))?(?: √†s (\\d+:\\d+))?')\n",
    "MEO_Posts[['day', 'month', 'year', 'hour']]= MEO_Posts['date'].str.extract(r'(?:(\\d+) de )?(\\w+)?(?: de (\\d+))?(?: √†s (\\d+:\\d+))?')\n",
    "NOS_Posts[['day', 'month', 'year', 'hour']]= NOS_Posts['date'].str.extract(r'(?:(\\d+) de )?(\\w+)?(?: de (\\d+))?(?: √†s (\\d+:\\d+))?')\n",
    "DIGI_News_Posts[['day', 'month', 'year', 'hour']]= DIGI_News_Posts['date'].str.extract(r'(?:(\\d+) de )?(\\w+)?(?: de (\\d+))?(?: √†s (\\d+:\\d+))?')\n",
    "\n",
    "Vodafone_Posts['month'] = Vodafone_Posts['month'].map(lambda x: meses_portugues_para_numero.get(x, x), na_action='ignore')\n",
    "MEO_Posts['month'] = MEO_Posts['month'].map(lambda x: meses_portugues_para_numero.get(x, x), na_action='ignore')\n",
    "NOS_Posts['month'] = NOS_Posts['month'].map(lambda x: meses_portugues_para_numero.get(x, x), na_action='ignore')\n",
    "DIGI_News_Posts['month'] = DIGI_News_Posts['month'].map(lambda x: meses_portugues_para_numero.get(x, x), na_action='ignore')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar os posts sem data\n",
    "print('\\033[1mMeses Vodafone:\\033[0m', Vodafone_Posts['month'].unique())\n",
    "print('\\033[1mMeses MEO:\\033[0m', MEO_Posts['month'].unique())\n",
    "print('\\033[1mMeses NOS:\\033[0m', NOS_Posts['month'].unique())\n",
    "print('\\033[1mMeses DIGI [News]:\\033[0m', DIGI_News_Posts['month'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mVodafone | N¬∫ de Posts sem dia:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['day'].isna()]))\n",
    "print(\"\\033[1mVodafone | N¬∫ de Posts sem m√™s:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['month'].isna()]))\n",
    "print(\"\\033[1mVodafone | N¬∫ de Posts sem ano:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['year'].isna()]))\n",
    "print(\"\\033[1mVodafone | N¬∫ de Posts sem hora:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['hour'].isna()]))\n",
    "print(\"\\033[1mVodafone | N¬∫ de Posts sem data:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['date'] == \"\"]))\n",
    "\n",
    "# Averiguar a raz√£o \n",
    "# Vodafone_Posts[Vodafone_Posts['mes'].isna()].sample(1) # Sem raz√£o aparente\n",
    "# Os posts sem ano s√£o os deste ano"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mMEO | N¬∫ de Posts sem dia:\\033[0m\", len(MEO_Posts[MEO_Posts['day'].isna()]))\n",
    "print(\"\\033[1mMEO | N¬∫ de Posts sem m√™s:\\033[0m\", len(MEO_Posts[MEO_Posts['month'].isna()]))\n",
    "print(\"\\033[1mMEO | N¬∫ de Posts sem ano:\\033[0m\", len(MEO_Posts[MEO_Posts['year'].isna()]))\n",
    "print(\"\\033[1mMEO | N¬∫ de Posts sem hora:\\033[0m\", len(MEO_Posts[MEO_Posts['hour'].isna()]))\n",
    "print(\"\\033[1mMEO | N¬∫ de Posts sem data:\\033[0m\", len(MEO_Posts[MEO_Posts['date'] == \"\"]))\n",
    "\n",
    "# Averiguar a raz√£o \n",
    "# MEO_Posts[MEO_Posts['mes'].isna()].sample(1) # Sem raz√£o aparente\n",
    "# Os posts sem ano s√£o os deste ano"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mNOS | N¬∫ de Posts sem dia:\\033[0m\", len(NOS_Posts[NOS_Posts['day'].isna()]))\n",
    "print(\"\\033[1mNOS | N¬∫ de Posts sem m√™s:\\033[0m\", len(NOS_Posts[NOS_Posts['month'].isna()]))\n",
    "print(\"\\033[1mNOS | N¬∫ de Posts sem ano:\\033[0m\", len(NOS_Posts[NOS_Posts['year'].isna()]))\n",
    "print(\"\\033[1mNOS | N¬∫ de Posts sem hora:\\033[0m\", len(NOS_Posts[NOS_Posts['hour'].isna()]))\n",
    "print(\"\\033[1mNOS | N¬∫ de Posts sem data:\\033[0m\", len(NOS_Posts[NOS_Posts['date'] == \"\"]))\n",
    "\n",
    "# Averiguar a raz√£o \n",
    "# NOS_Posts[NOS_Posts['mes'].isna()].sample(1) # Sem raz√£o aparente\n",
    "# Os posts sem ano s√£o os deste ano"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mDIGI[News] | N¬∫ de Posts sem dia:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['day'].isna()]))\n",
    "print(\"\\033[1mDIGI[News] | N¬∫ de Posts sem m√™s:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['month'].isna()]))\n",
    "print(\"\\033[1mDIGI[News] | N¬∫ de Posts sem ano:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['year'].isna()]))\n",
    "print(\"\\033[1mDIGI[News] | N¬∫ de Posts sem hora:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['hour'].isna()]))\n",
    "print(\"\\033[1mDIGI[News] | N¬∫ de Posts sem data:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['date'] == \"\"]))\n",
    "\n",
    "# Averiguar a raz√£o \n",
    "# DIGI_News_Posts[DIGI_News_Posts['mes'].isna()].sample(1) # Sem raz√£o aparente\n",
    "# Os posts sem ano s√£o os deste ano"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Substitui valores em branco por NaN na coluna 'date'\n",
    "Vodafone_Posts['date'] = Vodafone_Posts['date'].replace('', np.nan)\n",
    "MEO_Posts['date'] = MEO_Posts['date'].replace('', np.nan)\n",
    "NOS_Posts['date'] = NOS_Posts['date'].replace('', np.nan)\n",
    "DIGI_News_Posts['date'] = DIGI_News_Posts['date'].replace('', np.nan)\n",
    "\n",
    "# N¬∫ de Posts Eliminados por n√£o terem data\n",
    "print(\"\\033[1mN¬∫ de Posts sem data e a respetiva %:\\033[0m\")\n",
    "print(\"\\033[1mVodafone:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['date'].isna()]), \n",
    "      f\"({(len(Vodafone_Posts[Vodafone_Posts['date'].isna()]) / len(Vodafone_Posts)) * 100:.2f}%)\")\n",
    "print(\"\\033[1mMEO:\\033[0m\", len(MEO_Posts[MEO_Posts['date'].isna()]), \n",
    "      f\"({(len(MEO_Posts[MEO_Posts['date'].isna()]) / len(MEO_Posts)) * 100:.2f}%)\")\n",
    "print(\"\\033[1mNOS:\\033[0m\", len(NOS_Posts[NOS_Posts['date'].isna()]), \n",
    "      f\"({(len(NOS_Posts[NOS_Posts['date'].isna()]) / len(NOS_Posts)) * 100:.2f}%)\")\n",
    "print(\"\\033[1mDIGI [News]:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['date'].isna()]), \n",
    "      f\"({(len(DIGI_News_Posts[DIGI_News_Posts['date'].isna()]) / len(DIGI_News_Posts)) * 100:.2f}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Dado a data ser relevante na an√°lise dos ***posts***, e serem poucos os que n√£o t√™m data associada, introduzir-se-√† manualmente as que faltam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verficar os posts sem data\n",
    "Vodafone_Posts.loc[Vodafone_Posts['date'].isna()][['id', 'date', 'statistic', 'reactions', 'comments', 'shares',\n",
    "                                                   'link', 'day', 'month', 'year', 'hour']]\n",
    "# id=4803895769628848 -> 10 de novembro de 2021 \n",
    "# id=816220098396455 -> 4 de agosto de 2014  "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Preencher os valores ausentes para os posts espec√≠ficos\n",
    "Vodafone_Posts.loc[Vodafone_Posts['id'] == 4803895769628848, ['date','day', 'month', 'year']] = ['10 de novembro de 2021', 10, 11, 2021]\n",
    "Vodafone_Posts.loc[Vodafone_Posts['id'] == 816220098396455, ['date', 'day', 'month', 'year']] = ['4 de agosto de 2014', 4, 8, 2014]\n",
    "Vodafone_Posts.loc[Vodafone_Posts['date'].isna()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verficar os posts sem data\n",
    "MEO_Posts.loc[MEO_Posts['date'].isna()][['id', 'date', 'statistic', 'reactions', 'comments', 'shares',\n",
    "                                         'link', 'day', 'month', 'year', 'hour']]\n",
    "\n",
    "# id=10159699593554570 -> 17 de fevereiro de 2022  \n",
    "# id=10158462942399570 -> 3 de julho de 2020\n",
    "# id=10157587366084570 -> 27 de setembro de 2019"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "MEO_Posts.loc[MEO_Posts['id'] == 10159699593554570, ['date', 'day', 'month', 'year']] = ['17 de fevereiro de 2022', 17, 2, 2022]\n",
    "MEO_Posts.loc[MEO_Posts['id'] == 10158462942399570, ['date', 'day', 'month', 'year']] = ['3 de julho de 2020', 3, 7, 2020]\n",
    "MEO_Posts.loc[MEO_Posts['id'] == 10157587366084570, ['date', 'day', 'month', 'year']] = ['27 de setembro de 2019', 27, 9, 2019]\n",
    "\n",
    "MEO_Posts.loc[MEO_Posts['date'].isna()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verficar os posts sem data\n",
    "NOS_Posts.loc[NOS_Posts['date'].isna()][['id', 'date', 'statistic', 'reactions', 'comments', 'shares',\n",
    "                                         'link', 'day', 'month', 'year', 'hour']].head(2)\n",
    "\n",
    "# id=4065429463470835 -> 11 de mar√ßo de 2020   \n",
    "# id=4046227842057664 -> 5 de mar√ßo de 2020 \n",
    "# id=3899087906771659 -> 22 de janeiro de 2020  \n",
    "# id=3809573049056479 -> 26 de dezembro de 2019    \n",
    "# id=3761930650487386 -> 12 de dezembro de 2019  \n",
    "# id=3657561994257586 -> 14 de novembro de 2019 \n",
    "# id=3655654907781628 -> 13 de novembro de 2019   \n",
    "# id=3637105972969855 -> 7 de novembro de 2019  \n",
    "# id=3637235422956910 -> 7 de novembro de 2019\n",
    "# id=3637483906265395 -> 6 de novembro de 2019   \n",
    "# id=3565227623491024 -> 16 de outubro de 2019 \n",
    "# id=3481141765232944 -> 19 de setembro de 2019 \n",
    "# id=3478912185455902 -> 18 de setembro de 2019  \n",
    "# id=3360182660662189 -> 7 de agosto de 2019 \n",
    "# id=3339248812755574 -> 1 de agosto de 2019 \n",
    "# id=3339418216071967 -> 1 de agosto de 2019 \n",
    "# id=3334213879925734 -> 30 de julho de 2019   \n",
    "# id=3334226749924447 -> 23 de julho de 2019 \n",
    "# id=3261012333912556 -> 4 de julho de 2019   \n",
    "# id=3260235300656926 -> 3 de julho de 2019  \n",
    "# id=2870867862927007 -> 30 de janeiro de 2019 \n",
    "# id=2328100363870429 -> 6 de maio de 2018\n",
    "# id=2257507547596378 -> 20 de mar√ßo de 2018\n",
    "# id=2220974227916377 -> 25 de fevereiro de 2018\n",
    "# id=2216293261717807 -> 22 de fevereiro de 2018  \n",
    "# id=2195420687138398 -> 8 de fevereiro de 2018\n",
    "# id=2182424318438035 -> 30 de janeiro de 2018\n",
    "# id=2112488385431629 -> 14 de dezembro de 2017\n",
    "# id=2114231681923966 -> 14 de dezembro de 2017\n",
    "# id=2071184759561992 -> 16 de novembro de 2017\n",
    "# id=1988800464467089 -> 14 de setembro de 2017 \n",
    "# id=1987497767930692 -> 14 de setembro de 2017 \n",
    "# id=1952915148055621 -> 18 de agosto de 2017 \n",
    "# id=1886694318011038 -> 3 de julho de 2017\n",
    "# id=1770035123010292 -> 19 de abril de 2017\n",
    "# id=1758176494196155 -> 7 de abril de 2017\n",
    "# id=1680323778648094 -> 12 de fevereiro de 2017\n",
    "# id=1369232023090606 -> 17 de junho de 2016\n",
    "# id=10154156638114936 -> 22 de abril de 2016 \n",
    "# id=1308687155811760 -> 4 de abril de 2016\n",
    "# id=1185082574838886 -> 14 de outubro de 2015 \n",
    "# id=1185622034784940 -> 12 de outubro de 2015 \n",
    "# id=1179538832059927 -> 4 de outubro de 2015 \n",
    "# id=1176678262345984 -> 30 de setembro de 2015\n",
    "# id=1175475505799593 -> 27 de setembro de 2015 \n",
    "# id=1174869942526816 -> 24 de setembro de 2015  \n",
    "# id=1172121419468335 -> 22 de setembro de 2015 \n",
    "# id=1166175000062977 -> 12 de setembro de 2015 \n",
    "# id=1174868475860296 -> 16 de setembro de 2015 \n",
    "# id=1146375825376228 -> 18 de agosto de 2015 \n",
    "# id=1146410702039407 -> 18 de agosto de 2015\n",
    "# id=1092711547409323 -> 3 de junho de 2015\n",
    "# id=1085959991417812 -> 22 de maio de 2015\n",
    "# id=1084226478257830 -> 21 de maio de 2015\n",
    "# id=1082889498391528 -> 16 de maio de 2015 \n",
    "\n",
    "# NO TOTAL S√ÉO 57 OBSERVA√á√ïES SEM DATA"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Preencher os valores ausentes para os posts espec√≠ficos\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 4065429463470835, ['date', 'day', 'month', 'year']] = ['11 de mar√ßo de 2020', 11, 3, 2020]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 4046227842057664, ['date', 'day', 'month', 'year']] = ['5 de mar√ßo de 2020', 5, 3, 2020]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3899087906771659, ['date', 'day', 'month', 'year']] = ['22 de janeiro de 2020', 22, 1, 2020]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3809573049056479, ['date', 'day', 'month', 'year']] = ['26 de dezembro de 2019', 26, 12, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3761930650487386, ['date', 'day', 'month', 'year']] = ['12 de dezembro de 2019', 12, 12, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3657561994257586, ['date', 'day', 'month', 'year']] = ['14 de novembro de 2019', 14, 11, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3655654907781628, ['date', 'day', 'month', 'year']] = ['13 de novembro de 2019', 13, 11, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3637105972969855, ['date', 'day', 'month', 'year']] = ['7 de novembro de 2019', 7, 11, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3637235422956910, ['date', 'day', 'month', 'year']] = ['7 de novembro de 2019', 7, 11, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3637483906265395, ['date', 'day', 'month', 'year']] = ['6 de novembro de 2019', 6, 11, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3565227623491024, ['date', 'day', 'month', 'year']] = ['16 de outubro de 2019', 16, 10, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3481141765232944, ['date', 'day', 'month', 'year']] = ['19 de setembro de 2019', 19, 9, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3478912185455902, ['date', 'day', 'month', 'year']] = ['18 de setembro de 2019', 18, 9, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3360182660662189, ['date', 'day', 'month', 'year']] = ['7 de agosto de 2019', 7, 8, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3339248812755574, ['date', 'day', 'month', 'year']] = ['1 de agosto de 2019', 1, 8, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3339418216071967, ['date', 'day', 'month', 'year']] = ['1 de agosto de 2019', 1, 8, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3334213879925734, ['date', 'day', 'month', 'year']] = ['30 de julho de 2019', 30, 7, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3334226749924447, ['date', 'day', 'month', 'year']] = ['23 de julho de 2019', 23, 7, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3261012333912556, ['date', 'day', 'month', 'year']] = ['4 de julho de 2019', 4, 7, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 3260235300656926, ['date', 'day', 'month', 'year']] = ['3 de julho de 2019', 3, 7, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2870867862927007, ['date', 'day', 'month', 'year']] = ['30 de janeiro de 2019', 30, 1, 2019]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2328100363870429, ['date', 'day', 'month', 'year']] = ['6 de maio de 2018', 6, 5, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2257507547596378, ['date', 'day', 'month', 'year']] = ['20 de mar√ßo de 2018', 20, 3, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2220974227916377, ['date', 'day', 'month', 'year']] = ['25 de fevereiro de 2018', 25, 2, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2216293261717807, ['date', 'day', 'month', 'year']] = ['22 de fevereiro de 2018', 22, 2, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2195420687138398, ['date', 'day', 'month', 'year']] = ['8 de fevereiro de 2018', 8, 2, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2182424318438035, ['date', 'day', 'month', 'year']] = ['30 de janeiro de 2018', 30, 1, 2018]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2112488385431629, ['date', 'day', 'month', 'year']] = ['14 de dezembro de 2017', 14, 12, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2114231681923966, ['date', 'day', 'month', 'year']] = ['14 de dezembro de 2017', 14, 12, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 2071184759561992, ['date', 'day', 'month', 'year']] = ['16 de novembro de 2017', 16, 11, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1987499774597158, ['date', 'day', 'month', 'year']] = ['14 de setembro de 2017', 14, 9, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1988800464467089, ['date', 'day', 'month', 'year']] = ['14 de setembro de 2017', 14, 9, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1987497767930692, ['date', 'day', 'month', 'year']] = ['14 de setembro de 2017', 14, 9, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1961601103853692, ['date', 'day', 'month', 'year']] = ['14 de setembro de 2017', 14, 9, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1952915148055621, ['date', 'day', 'month', 'year']] = ['18 de agosto de 2017', 18, 8, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1886694318011038, ['date', 'day', 'month', 'year']] = ['3 de julho de 2017', 3, 7, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1770035123010292, ['date', 'day', 'month', 'year']] = ['19 de abril de 2017', 19, 4, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1758176494196155, ['date', 'day', 'month', 'year']] = ['7 de abril de 2017', 7, 4, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1680323778648094, ['date', 'day', 'month', 'year']] = ['12 de fevereiro de 2017', 12, 2, 2017]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1369232023090606, ['date', 'day', 'month', 'year']] = ['17 de junho de 2016', 17, 6, 2016]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 10154156638114936,['date', 'day', 'month', 'year']] = ['22 de abril de 2016', 22, 4, 2016]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1308687155811760, ['date', 'day', 'month', 'year']] = ['4 de abril de 2016', 4, 4, 2016]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1185082574838886, ['date', 'day', 'month', 'year']] = ['14 de outubro de 2015', 14, 10, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1185622034784940, ['date', 'day', 'month', 'year']] = ['12 de outubro de 2015', 12, 10, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1179538832059927, ['date', 'day', 'month', 'year']] = ['4 de outubro de 2015', 4, 10, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1176678262345984, ['date', 'day', 'month', 'year']] = ['30 de setembro de 2015', 30, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1175475505799593, ['date', 'day', 'month', 'year']] = ['27 de setembro de 2015', 27, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1174869942526816, ['date', 'day', 'month', 'year']] = ['24 de setembro de 2015', 24, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1172121419468335, ['date', 'day', 'month', 'year']] = ['22 de setembro de 2015', 22, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1166175000062977, ['date', 'day', 'month', 'year']] = ['12 de setembro de 2015', 12, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1174868475860296, ['date', 'day', 'month', 'year']] = ['16 de setembro de 2015', 16, 9, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1146375825376228, ['date', 'day', 'month', 'year']] = ['18 de agosto de 2015', 18, 8, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1146410702039407, ['date', 'day', 'month', 'year']] = ['18 de agosto de 2015', 18, 8, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1092711547409323, ['date', 'day', 'month', 'year']] = ['3 de junho de 2015', 3, 6, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1085959991417812, ['date', 'day', 'month', 'year']] = ['22 de maio de 2015', 22, 5, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1084226478257830, ['date', 'day', 'month', 'year']] = ['21 de maio de 2015', 21, 5, 2015]\n",
    "NOS_Posts.loc[NOS_Posts['id'] == 1082889498391528, ['date', 'day', 'month', 'year']] = ['16 de maio de 2015', 16, 5, 2015]\n",
    "                                                                                        \n",
    "                                                                                        \n",
    "NOS_Posts.loc[NOS_Posts['date'].isna()]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Elimina as observa√µes de posts sem data - N√£o ser√° nenhuma pq foi preenchida antes\n",
    "print(\"\\033[1mVodafone\\033[0m:\")\n",
    "print(\"Antes:\", len(Vodafone_Posts))\n",
    "Vodafone_Posts = Vodafone_Posts.dropna(subset=['date'])\n",
    "print(\"Depois:\", len(Vodafone_Posts))\n",
    "\n",
    "print(\"\\n\\033[1mMEO\\033[0m:\")\n",
    "print(\"Antes:\", len(MEO_Posts))\n",
    "MEO_Posts = MEO_Posts.dropna(subset=['date'])\n",
    "print(\"Depois:\", len(MEO_Posts))\n",
    "\n",
    "print(\"\\n\\033[1mNOS\\033[0m:\")\n",
    "print(\"Antes:\", len(NOS_Posts))\n",
    "NOS_Posts = NOS_Posts.dropna(subset=['date'])\n",
    "print(\"Depois:\", len(NOS_Posts))\n",
    "\n",
    "print(\"\\n\\033[1mDIGI News\\033[0m:\")\n",
    "print(\"Antes:\", len(DIGI_News_Posts))\n",
    "DIGI_News_Posts = DIGI_News_Posts.dropna(subset=['date'])\n",
    "print(\"Depois:\", len(DIGI_News_Posts))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Substituir o ano corrente que est√° em \"NaN\" para 2024\n",
    "print(\"\\033[1mN¬∫ de Posts [Vodafone] sem ano == Posts de 2024:\\033[0m\", len(Vodafone_Posts[Vodafone_Posts['year'].isna()]))\n",
    "Vodafone_Posts['year'] = Vodafone_Posts['year'].fillna(2024)\n",
    "print(\"N¬∫ de Posts [Vodafone] sem ano:\", len(Vodafone_Posts[Vodafone_Posts['year'].isna()]), '\\n')\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Posts [MEO] sem ano == Posts de 2024:\\033[0m\", len(MEO_Posts[MEO_Posts['year'].isna()]))\n",
    "MEO_Posts['year'] = MEO_Posts['year'].fillna(2024)\n",
    "print(\"N¬∫ de Posts [MEO] sem ano:\", len(MEO_Posts[MEO_Posts['year'].isna()]), '\\n')\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Posts [NOS] sem ano == Posts de 2024:\\033[0m\", len(NOS_Posts[NOS_Posts['year'].isna()]))\n",
    "NOS_Posts['year'] = NOS_Posts['year'].fillna(2024)\n",
    "print(\"N¬∫ de Posts [NOS] sem ano:\", len(NOS_Posts[NOS_Posts['year'].isna()]), '\\n')\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Posts [DIGI_News] sem ano == Posts de 2024:\\033[0m\", len(DIGI_News_Posts[DIGI_News_Posts['year'].isna()]))\n",
    "DIGI_News_Posts['year'] = DIGI_News_Posts['year'].fillna(2024)\n",
    "print(\"N¬∫ de Posts [DIGI_News] sem ano:\", len(DIGI_News_Posts[DIGI_News_Posts['year'].isna()]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Unindo as colunas 'day', 'month' e 'year' em uma √∫nica coluna 'data'\n",
    "Vodafone_Posts['day'] = Vodafone_Posts['day'].astype('int')\n",
    "Vodafone_Posts['month'] = Vodafone_Posts['month'].astype('int')\n",
    "Vodafone_Posts['year'] = Vodafone_Posts['year'].astype('int')\n",
    "Vodafone_Posts['date_clean'] = pd.to_datetime(Vodafone_Posts['year'].astype(str) + '-' + Vodafone_Posts['month'].astype(str) + '-' + Vodafone_Posts['day'].astype(str), \n",
    "                                   format='%Y-%m-%d',\n",
    "                                   errors='coerce')\n",
    "\n",
    "MEO_Posts['day'] = MEO_Posts['day'].astype('int')\n",
    "MEO_Posts['month'] = MEO_Posts['month'].astype('int')\n",
    "MEO_Posts['year'] = MEO_Posts['year'].astype('int')\n",
    "MEO_Posts['date_clean'] = pd.to_datetime(MEO_Posts['year'].astype(str) + '-' + MEO_Posts['month'].astype(str) + '-' + MEO_Posts['day'].astype(str), \n",
    "                                   format='%Y-%m-%d',\n",
    "                                   errors='coerce')\n",
    "\n",
    "NOS_Posts['day'] = NOS_Posts['day'].astype('int')\n",
    "NOS_Posts['month'] = NOS_Posts['month'].astype('int')\n",
    "NOS_Posts['year'] = NOS_Posts['year'].astype('int')\n",
    "NOS_Posts['date_clean'] = pd.to_datetime(NOS_Posts['year'].astype(str) + '-' + NOS_Posts['month'].astype(str) + '-' + NOS_Posts['day'].astype(str), \n",
    "                                   format='%Y-%m-%d',\n",
    "                                   errors='coerce')\n",
    "\n",
    "DIGI_News_Posts['day'] = DIGI_News_Posts['day'].astype('int')\n",
    "DIGI_News_Posts['month'] = DIGI_News_Posts['month'].astype('int')\n",
    "DIGI_News_Posts['year'] = DIGI_News_Posts['year'].astype('int')\n",
    "DIGI_News_Posts['date_clean'] = pd.to_datetime(DIGI_News_Posts['year'].astype(str) + '-' + DIGI_News_Posts['month'].astype(str) + '-' + DIGI_News_Posts['day'].astype(str), \n",
    "                                   format='%Y-%m-%d',\n",
    "                                   errors='coerce')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar a correta limpeza da vari√°vel 'date'\n",
    "pd.DataFrame(Vodafone_Posts[['date', 'day', 'month', 'year','hour','date_clean']].sample(5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar valores omissos\n",
    "NAs = pd.DataFrame({\n",
    "    'n Omissos':pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])[['day', 'month', 'year', 'hour', 'date_clean']].isna().sum(),\n",
    "    '% Omissos':round(((pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])[['day', 'month', 'year', 'hour', 'date_clean']].isna().sum()\n",
    "                 / len(pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts]))) * 100),2)})\n",
    "\n",
    "# Print dos resultados\n",
    "NAs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar o intervalo de tempo dos posts obtidos\n",
    "print(\"\\033[1mIntervalo de Tempo dos Posts da [Vodafone]:\\033[0m\", \n",
    "      Vodafone_Posts['date_clean'].min().strftime('%d/%m/%Y'), \"at√©\", Vodafone_Posts['date_clean'].max().strftime('%d/%m/%Y')) \n",
    "print(\"\\033[1mIntervalo de Tempo dos Posts da [MEO]:\\033[0m\", \n",
    "      MEO_Posts['date_clean'].min().strftime('%d/%m/%Y'), \"at√©\", MEO_Posts['date_clean'].max().strftime('%d/%m/%Y')) \n",
    "print(\"\\033[1mIntervalo de Tempo dos Posts da [NOS]:\\033[0m\", \n",
    "      NOS_Posts['date_clean'].min().strftime('%d/%m/%Y'), \"at√©\", NOS_Posts['date_clean'].max().strftime('%d/%m/%Y')) \n",
    "print(\"\\033[1mIntervalo de Tempo dos Posts da [DIGI News]:\\033[0m\", \n",
    "      DIGI_News_Posts['date_clean'].min().strftime('%d/%m/%Y'), \"at√©\", DIGI_News_Posts['date_clean'].max().strftime('%d/%m/%Y')) "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar 'outliers' - Contar o N¬∫ de posts por data e classificar em ordem decrescente\n",
    "pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])['date_clean'].value_counts()\\\n",
    "    .reset_index('date_clean').set_index('date_clean').head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> N√£o aparenta ter valores anormais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `N¬∫ de Rea√ß√µes, Coment√°rios e Partilhas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Valores poss√≠veis da vari√°vel 'reactions', 'comments' e 'shares'\n",
    "for operadora, nome_operadora in zip([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts], ['Vodafone', 'MEO', 'NOS', 'DIGI']):\n",
    "    print(f'\\033[1m{nome_operadora} Rea√ß√µes\\033[0m    ', operadora['reactions'].unique()[:7])\n",
    "    print(f'\\033[1m{nome_operadora} Coment√°rios\\033[0m', operadora['comments'].unique()[:(3 if operadora is DIGI_News_Posts else 7)])\n",
    "    print(f'\\033[1m{nome_operadora} Partilhas\\033[0m  ', operadora['shares'].unique()[:(3 if operadora is DIGI_News_Posts else 7)], '\\n')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Encontrar observa√ß√µes com \",\" ou \"mil\" nas colunas 'comments' e 'shares'\n",
    "print(\"\\033[1mVodafone_Posts:\\033[0m\\n\")\n",
    "print(\"Observa√ß√µes com ',' ou 'mil' na coluna 'comments':\",\n",
    "     len(Vodafone_Posts[Vodafone_Posts['shares'].str.contains(',') | Vodafone_Posts['shares'].str.contains('mil')]))\n",
    "# Vodafone_Posts[Vodafone_Posts['comments'].str.contains(',') | Vodafone_Posts['comments'].str.contains('mil')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\nObserva√ß√µes com ',' ou 'mil' na coluna 'shares':\",\n",
    "     len(Vodafone_Posts[Vodafone_Posts['shares'].str.contains(',') | Vodafone_Posts['shares'].str.contains('mil')]))\n",
    "# Vodafone_Posts[Vodafone_Posts['shares'].str.contains(',') | Vodafone_Posts['shares'].str.contains('mil')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\n\\033[1mMEO_Posts:\\033[0m\\n\")\n",
    "print(\"Observa√ß√µes com ',' ou 'mil' na coluna 'comments':\",\n",
    "     len(MEO_Posts[MEO_Posts['comments'].str.contains(',') | MEO_Posts['comments'].str.contains('mil')]))\n",
    "# MEO_Posts[MEO_Posts['comments'].str.contains(',') | MEO_Posts['comments'].str.contains('mil')].sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\nObserva√ß√µes com ',' ou 'mil' na coluna 'shares':\", len(MEO_Posts[MEO_Posts['shares'].str.contains(',') | MEO_Posts['shares'].str.contains('mil')]))\n",
    "# MEO_Posts[MEO_Posts['shares'].str.contains(',') | MEO_Posts['shares'].str.contains('mil')].sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\n\\033[1mNOS_Posts:\\033[0m\\n\")\n",
    "print(\"Observa√ß√µes com ',' ou 'mil' na coluna 'comments':\", \n",
    "      len(NOS_Posts[NOS_Posts['comments'].str.contains(',') | NOS_Posts['comments'].str.contains('mil')]))\n",
    "# NOS_Posts[NOS_Posts['comments'].str.contains(',') | NOS_Posts['comments'].str.contains('mil')].sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\nObserva√ß√µes com ',' ou 'mil' na coluna 'shares':\", \n",
    "      len(NOS_Posts[NOS_Posts['shares'].str.contains(',') | NOS_Posts['shares'].str.contains('mil')]))\n",
    "# NOS_Posts[NOS_Posts['shares'].str.contains(',') | NOS_Posts['shares'].str.contains('mil')].sample(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Para o c√≥digo seguinte funcionar, retira-se os casos especiais\n",
    "# Vodafone_Posts.loc[Vodafone_Posts['comments'].str.contains('√Ålbum'), 'comments'] = np.NaN\n",
    "# MEO_Posts.loc[MEO_Posts['comments'].str.contains('√Ålbum'), 'comments'] = np.NaN\n",
    "# NOS_Posts.loc[NOS_Posts['comments'].str.contains('√Ålbum'), 'comments'] = np.NaN"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fun√ß√£o para limpar os n√∫meros\n",
    "def limpar_mils(texto):\n",
    "    # Limpar as palavras \"coment√°rios\", \"partilhas\"\n",
    "    if 'coment√°rio' in str(texto) or 'partilha' in str(texto):\n",
    "        texto = texto.replace('coment√°rios', '')\n",
    "        texto = texto.replace('coment√°rio', '')\n",
    "        texto = texto.replace('partilhas', '')\n",
    "        texto = texto.replace('partilha', '')\n",
    "        \n",
    "    # Substitui o caractere de espa√ßo n√£o quebr√°vel por um espa√ßo em branco\n",
    "    if isinstance(texto, str):\n",
    "        texto = texto.replace('\\xa0', '')\n",
    "        \n",
    "    # Verifica se '√Ålbum' est√° presente no texto - Casos Particular\n",
    "    if '√Ålbum' in str(texto):\n",
    "        return np.NaN\n",
    "        \n",
    "    # Verifica se 'mil' est√° presente no texto\n",
    "    if 'mil' == texto:\n",
    "        return 1000\n",
    "    \n",
    "    if 'mil' in str(texto):\n",
    "        # Remove v√≠rgulas e 'mil'\n",
    "        numeros = texto.replace('mil', '')\n",
    "\n",
    "        # Verifica se o texto cont√©m uma v√≠rgula\n",
    "        if ',' in numeros:\n",
    "            # Divide o texto nas partes antes e depois da v√≠rgula\n",
    "            partes = numeros.split(',')\n",
    "\n",
    "            # Se houver duas partes (parte inteira e parte decimal), multiplica apenas a parte inteira por mil [Exemplo: '3,4 mil']\n",
    "            if len(partes) == 2:\n",
    "                numeros = partes[0] + partes[1].ljust(3, '0')\n",
    "            else:\n",
    "                # Caso contr√°rio, apenas remove a v√≠rgula e acrescenta tr√™s zeros [Exemplo: '12, mil' ]\n",
    "                numeros = numeros.replace(',', '') + '000'\n",
    "        else:\n",
    "            # Caso seja sem ',' [Exemplo: '12 mil']\n",
    "            numeros = numeros + '000'\n",
    "        # Converte para inteiro\n",
    "        return int(numeros) if numeros else 0\n",
    "    \n",
    "    # Se for apenas um espa√ßo ' '\n",
    "    if str(texto).isspace():\n",
    "        return np.NaN\n",
    "    \n",
    "    if texto is np.NaN:\n",
    "        return np.NaN\n",
    "    \n",
    "    # Se 'mil' n√£o estiver presente, apenas remove v√≠rgulas e converte para inteiro\n",
    "    else:\n",
    "        return int(texto) if texto else 0\n",
    "\n",
    "# Aplicar a fun√ß√£o √† coluna de 'reactions'\n",
    "Vodafone_Posts['reactions_clean'] = Vodafone_Posts['reactions'].apply(limpar_mils)\n",
    "MEO_Posts['reactions_clean'] = MEO_Posts['reactions'].apply(limpar_mils) \n",
    "NOS_Posts['reactions_clean'] = NOS_Posts['reactions'].apply(limpar_mils)\n",
    "DIGI_News_Posts['reactions_clean'] = DIGI_News_Posts['reactions'].apply(limpar_mils)\n",
    "\n",
    "# Aplicar a fun√ß√£o √† coluna de 'comments'\n",
    "Vodafone_Posts['comments_clean'] = Vodafone_Posts['comments'].apply(limpar_mils)\n",
    "MEO_Posts['comments_clean'] = MEO_Posts['comments'].apply(limpar_mils)\n",
    "NOS_Posts['comments_clean'] = NOS_Posts['comments'].apply(limpar_mils)\n",
    "DIGI_News_Posts['comments_clean'] = DIGI_News_Posts['comments'].apply(limpar_mils)\n",
    "\n",
    "# Aplicar a fun√ß√£o √† coluna de 'shares'\n",
    "Vodafone_Posts['shares_clean'] = Vodafone_Posts['shares'].apply(limpar_mils)\n",
    "MEO_Posts['shares_clean'] = MEO_Posts['shares'].apply(limpar_mils)\n",
    "NOS_Posts['shares_clean'] = NOS_Posts['shares'].apply(limpar_mils)\n",
    "DIGI_News_Posts['shares_clean'] = DIGI_News_Posts['shares'].apply(limpar_mils)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Confimar se fez bem a convers√£o dos 'mil'\n",
    "# Vodafone_Posts[Vodafone_Posts['statistic'].str.contains('mil')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ver casos extra com o cont√©m\n",
    "# Vodafone_Posts[Vodafone_Posts['reactions'] == '11\\xa00']\n",
    "# Vodafone_Posts[Vodafone_Posts['comments'].str.contains('2s')]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar a correta limpeza da vari√°vel 'reactions', 'comments' e 'shares'\n",
    "pd.DataFrame(NOS_Posts[['reactions','reactions_clean', 'comments','comments_clean', 'shares', 'shares_clean']].sample(5))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Resumo das Estat√≠sticas dos Posts por operadora\n",
    "def print_formatted_info(posts, name):\n",
    "    print(\"\\033[1m{}\\033[0m\\n\".format(name).replace(',', '.'))\n",
    "    print(\"Total de Posts:\", \"{:,}\".format(len(posts)).replace(',', '.'))\n",
    "    print(\"Total de Rea√ß√µes:\", \"{:,}\".format(int(posts['reactions_clean'].sum())).replace(',', '.'))\n",
    "    print(\"Total de Coment√°rios:\", \"{:,}\".format(int(posts['comments_clean'].sum())).replace(',', '.'))\n",
    "    print(\"Total de Compartilhamentos:\", \"{:,}\".format(int(posts['shares_clean'].sum())).replace(',', '.'), \"\\n\")\n",
    "\n",
    "print_formatted_info(Vodafone_Posts, \"Vodafone\")\n",
    "print_formatted_info(MEO_Posts, \"MEO\")\n",
    "print_formatted_info(NOS_Posts, \"NOS\")\n",
    "print_formatted_info(DIGI_News_Posts, \"DIGI [News]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar valores omissos\n",
    "NAs = pd.DataFrame({\n",
    "    'n Omissos':pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])[['reactions_clean','comments_clean','shares_clean']].isna().sum(),\n",
    "    '% Omissos':round(((pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])[['reactions_clean','comments_clean','shares_clean']].isna().sum()\n",
    "                 / len(pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts]))) * 100),2)})\n",
    "\n",
    "# Print dos resultados\n",
    "NAs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar os valores omissos da vari√°vel 'comments_clean'\n",
    "pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts]) \\\n",
    "    [pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts])['comments_clean'].isna()].head(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ver os Posts com mais coment√°rios das Operadoras\n",
    "print(\"\\033[1mTop 5 Posts com Mais Coment√°rios da Vodafone\\033[0m\")\n",
    "Vodafone_Posts.nlargest(5, 'comments_clean')[['id', 'text','date', 'comments_clean', 'reactions_clean', 'shares_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mTop 5 Posts com Mais Coment√°rios da MEO\\033[0m\")\n",
    "MEO_Posts.nlargest(5, 'comments_clean')[['id', 'text','date', 'comments', 'comments_clean', 'reactions_clean', 'shares_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Procurar algum post especifico\n",
    "MEO_Posts[MEO_Posts['id'] == 10157972150519570]['link'].iloc[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mTop 5 Posts com Mais Coment√°rios da NOS\\033[0m\")\n",
    "NOS_Posts.nlargest(5, 'comments_clean')[['id', 'text', 'date','comments_clean', 'reactions_clean', 'shares_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mTop 5 Posts com Mais Coment√°rios da NOS\\033[0m\")\n",
    "DIGI_News_Posts.nlargest(5, 'comments_clean')[['page', 'post_text','news_text', 'date','comments_clean', 'reactions_clean', 'shares_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Links das Publica√ß√µes do JdN\n",
    "# https://www.facebook.com/jornalnoticias/posts/pfbid02WPYdh34GuAHAh3rUzRx7GmLrt8r61u8tMtAb1FhRoPa5SEeNYTTaGZKJyrJBYoBol\n",
    "# https://www.facebook.com/jornalnoticias/posts/pfbid024vG54K7o1FSiiuXqzHVPmz9VE945N1ZETv7HWWi1tWxe9mjMgaaL6bZ82eJXy5npl\n",
    "# https://www.facebook.com/jornalnoticias/posts/pfbid02XU1mDj8G5wtGBmwJcSx4mdhJafVN5dvUi683WgrgN58hUyCqFMo12awtgANe8dzXl"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### üìë Tabela Sintese dos *Posts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tabela Sintese da Informa√ß√£o obtida dos Posts\n",
    "summary_table = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'Total de Posts': [len(Vodafone_Posts), len(MEO_Posts), len(NOS_Posts), len(DIGI_News_Posts)],\n",
    "    'Total de Rea√ß√µes': [int(Vodafone_Posts['reactions_clean'].sum()), int(MEO_Posts['reactions_clean'].sum()), \n",
    "                         int(NOS_Posts['reactions_clean'].sum()), int(DIGI_News_Posts['reactions_clean'].sum())],\n",
    "    'Total de Coment√°rios': [int(Vodafone_Posts['comments_clean'].sum()), int(MEO_Posts['comments_clean'].sum()), \n",
    "                             int(NOS_Posts['comments_clean'].sum()), int(DIGI_News_Posts['comments_clean'].sum())],\n",
    "    'Total de Partilhas': [int(Vodafone_Posts['shares_clean'].sum()), int(MEO_Posts['shares_clean'].sum()), \n",
    "                           int(NOS_Posts['shares_clean'].sum()), int(DIGI_News_Posts['shares_clean'].sum())],\n",
    "    'Data Inicial': [Vodafone_Posts['date_clean'].min().strftime('%d/%m/%Y'), MEO_Posts['date_clean'].min().strftime('%d/%m/%Y'), \n",
    "                     NOS_Posts['date_clean'].min().strftime('%d/%m/%Y'), DIGI_News_Posts['date_clean'].min().strftime('%d/%m/%Y')],\n",
    "    'Data Final': [Vodafone_Posts['date_clean'].max().strftime('%d/%m/%Y'), MEO_Posts['date_clean'].max().strftime('%d/%m/%Y'), \n",
    "                   NOS_Posts['date_clean'].max().strftime('%d/%m/%Y'), DIGI_News_Posts['date_clean'].max().strftime('%d/%m/%Y')]\n",
    "})\n",
    "summary_table.set_index('Operadora', inplace=True)\n",
    "summary_table"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Selecionar Colunas a guardar dos 'Posts'\n",
    "selected_columns = ['id', 'text', 'date_clean', 'reactions_clean','comments_clean', \n",
    "                    'shares_clean', 'day','month', 'year', 'hour', 'link']\n",
    "\n",
    "Vodafone_Posts = Vodafone_Posts[selected_columns]\n",
    "MEO_Posts = MEO_Posts[selected_columns]\n",
    "NOS_Posts = NOS_Posts[selected_columns]\n",
    "DIGI_News_Posts = DIGI_News_Posts[['page', 'id', 'post_text', 'news_text', 'date_clean', 'reactions_clean',\n",
    "                                   'comments_clean', 'shares_clean', 'day', 'month', 'year', 'hour', 'link']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Renomear Colunas dos 'Posts'\n",
    "column_post_rename = {\n",
    "    'id': 'post_id',\n",
    "    'text': 'post_text',\n",
    "    'date_clean': 'post_date',\n",
    "    'reactions_clean': 'post_reactions',\n",
    "    'comments_clean': 'post_comments',\n",
    "    'shares_clean': 'post_shares',\n",
    "    'day': 'post_day',\n",
    "    'month': 'post_month',\n",
    "    'year': 'post_year',\n",
    "    'hour': 'post_hour',\n",
    "    'link': 'post_link'\n",
    "}\n",
    "\n",
    "Vodafone_Posts = Vodafone_Posts.rename(columns=column_post_rename)\n",
    "MEO_Posts = MEO_Posts.rename(columns=column_post_rename)\n",
    "NOS_Posts = NOS_Posts.rename(columns=column_post_rename)\n",
    "DIGI_News_Posts = DIGI_News_Posts.rename(columns=column_post_rename)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Vodafone_Posts[Vodafone_Posts.apply(lambda x: x.str.strip() == '', axis=1).any(axis=1)]      # S√≥ posts sem texto\n",
    "# MEO_Posts[MEO_Posts.apply(lambda x: x.str.strip() == '', axis=1).any(axis=1)]                # S√≥ posts sem texto\n",
    "# NOS_Posts[NOS_Posts.apply(lambda x: x.str.strip() == '', axis=1).any(axis=1)]                # S√≥ posts sem texto\n",
    "# DIGI_News_Posts[DIGI_News_Posts.apply(lambda x: x.str.strip() == '', axis=1).any(axis=1)]    # S√≥ posts sem texto"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Adicionar a coluna 'page' com o nome da operadora em cada DataFrame\n",
    "Vodafone_Posts['page'] = 'Vodafone'\n",
    "MEO_Posts['page'] = 'MEO'\n",
    "NOS_Posts['page'] = 'NOS'\n",
    "DIGI_News_Posts['page'] = 'DIGI News[' + DIGI_News_Posts['page'] + ']'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Substituir o texto do post pelo texto da not√≠cia apenas para a operadora DIGI_News quando o texto do post for NaN\n",
    "DIGI_News_Posts.loc[DIGI_News_Posts['news_text'].isna(), 'news_text'] = DIGI_News_Posts['post_text']\n",
    "DIGI_News_Posts.drop('post_text', axis=1, inplace=True)\n",
    "DIGI_News_Posts.rename(columns={'news_text':'post_text'}, inplace=True)\n",
    "\n",
    "df_Posts = pd.concat([Vodafone_Posts, MEO_Posts, NOS_Posts, DIGI_News_Posts], ignore_index=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Guardar a Compila√ß√£o de Posts em formato .txt\n",
    "df_Posts.to_csv('Datasets_Vodafone/Facebook_Posts.txt', sep='\\t', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üí≠ Coment√°rios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### **`Comment_Date`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Valores poss√≠veis da vari√°vel 'comment_date'\n",
    "print('\\033[1mVodafone\\033[0m\\n', Vodafone_Comments['comment_date'].unique())\n",
    "print('\\033[1mMEO\\033[0m\\n', MEO_Comments['comment_date'].unique())\n",
    "print('\\033[1mNOS\\033[0m\\n', NOS_Comments['comment_date'].unique())\n",
    "print('\\033[1mDIGI News\\033[0m\\n', DIGI_News_Comments['comment_date'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Converter a vari√°vel 'comment_date' para 'comment_day_ago'\n",
    "def convert_to_days_ago(date_str):\n",
    "    if isinstance(date_str, str):\n",
    "        if 'sem' in date_str:\n",
    "            days_ago = int(date_str.split()[0]) * 7\n",
    "        elif 'ano' in date_str:\n",
    "            years = int(date_str.split()[0])\n",
    "            # Considerando um ano com 365 dias\n",
    "            days_ago = years * 365\n",
    "        else:\n",
    "            # Se n√£o estiver definido como \"sem\" ou \"ano\", assume-se que seja \"dia(s)\"\n",
    "            days_ago = int(date_str.split()[0])\n",
    "        return days_ago\n",
    "    else:\n",
    "        return np.nan  # Retorna NaN para valores n√£o strings\n",
    "\n",
    "# Aplicar a fun√ß√£o para limpar a vari√°vel 'comment_date'\n",
    "Vodafone_Comments['comment_day_ago'] = Vodafone_Comments['comment_date'].apply(convert_to_days_ago)\n",
    "MEO_Comments['comment_day_ago'] = MEO_Comments['comment_date'].apply(convert_to_days_ago)\n",
    "NOS_Comments['comment_day_ago'] = NOS_Comments['comment_date'].apply(convert_to_days_ago)\n",
    "DIGI_News_Comments['comment_day_ago'] = DIGI_News_Comments['comment_date'].apply(convert_to_days_ago)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar a correta limpeza da vari√°vel 'comment_date'\n",
    "pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments]).groupby(['comment_date', 'comment_day_ago']) \\\n",
    "    .size().reset_index(name='count')[['comment_date', 'comment_day_ago']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar valores omissos\n",
    "NAs = pd.DataFrame({\n",
    "    'n Omissos':pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments])[['comment_day_ago']].isna().sum(),\n",
    "    '% Omissos':round(((pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments])[['comment_day_ago']].isna().sum()\n",
    "                 / len(pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments]))) * 100),2)})\n",
    "\n",
    "# Print dos resultados\n",
    "NAs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### **`Comment_Reactions`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Valores poss√≠veis da vari√°vel 'comment_reactions'\n",
    "print('\\033[1mVodafone\\n\\033[0m', Vodafone_Comments['comment_reactions'].unique())\n",
    "print('\\033[1mMEO\\n\\033[0m', MEO_Comments['comment_reactions'].unique())\n",
    "print('\\033[1mNOS\\n\\033[0m', NOS_Comments['comment_reactions'].unique())\n",
    "print('\\033[1mDIGI News\\n\\033[0m', DIGI_News_Comments['comment_reactions'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Substituir '' por 0\n",
    "Vodafone_Comments['comment_reactions_clean'] = Vodafone_Comments['comment_reactions'].replace('', 0)\n",
    "MEO_Comments['comment_reactions_clean'] = MEO_Comments['comment_reactions'].replace('', 0)\n",
    "NOS_Comments['comment_reactions_clean'] = NOS_Comments['comment_reactions'].replace('', 0)\n",
    "DIGI_News_Comments['comment_reactions_clean'] = DIGI_News_Comments['comment_reactions'].replace('', 0)\n",
    "\n",
    "# Limpar as rea√ß√µes com 'mil' e converter para o tipo num√©rico\n",
    "Vodafone_Comments['comment_reactions_clean'] = pd.to_numeric(Vodafone_Comments['comment_reactions'].apply(limpar_mils))\n",
    "MEO_Comments['comment_reactions_clean'] = pd.to_numeric(MEO_Comments['comment_reactions'].apply(limpar_mils))\n",
    "NOS_Comments['comment_reactions_clean'] = pd.to_numeric(NOS_Comments['comment_reactions'].apply(limpar_mils))\n",
    "DIGI_News_Comments['comment_reactions_clean'] = pd.to_numeric(DIGI_News_Comments['comment_reactions'].apply(limpar_mils))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar a correta limpeza da vari√°vel 'comment_reactions'\n",
    "pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments]) \\\n",
    "    .groupby(['comment_reactions', 'comment_reactions_clean']) \\\n",
    "    .size().reset_index(name='count')[['comment_reactions', 'comment_reactions_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Comment_Responses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Valores poss√≠veis da vari√°vel 'comment_responses'\n",
    "print('\\033[1mVodafone\\n\\033[0m', Vodafone_Comments['comment_responses'].unique()[:15])\n",
    "print('\\033[1mMEO\\n\\033[0m', MEO_Comments['comment_responses'].unique()[:15])\n",
    "print('\\033[1mNOS\\n\\033[0m', NOS_Comments['comment_responses'].unique()[:15])\n",
    "print('\\033[1mDIGI\\n\\033[0m', DIGI_News_Comments['comment_responses'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fun√ß√£o para limpar a vari√°vel 'comment_responses'\n",
    "def clean_comment_responses(df):\n",
    "    # Extrair o n√∫mero de respostas e a operadora que respondeu [1]\n",
    "    df['comment_num_responses'] = df['comment_responses'].str.extract(r'(\\d+)').fillna(0).astype(int)\n",
    "    df['comment_operator_responded'] = df['comment_responses'].str.contains(r'(Vodafone|MEO|NOS)', case=False, na=False)\n",
    "    \n",
    "    # Preencher c√©lulas vazias com False [0]\n",
    "    df['comment_operator_responded'] = df['comment_operator_responded'].fillna(False)\n",
    "    \n",
    "    # Converter True/False em 0/1\n",
    "    df['comment_operator_responded'] = df['comment_operator_responded'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Limpar a vari√°vel 'comment_responses'\n",
    "Vodafone_Comments = clean_comment_responses(Vodafone_Comments)\n",
    "MEO_Comments = clean_comment_responses(MEO_Comments)\n",
    "NOS_Comments = clean_comment_responses(NOS_Comments)\n",
    "DIGI_News_Comments = clean_comment_responses(DIGI_News_Comments)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar a correta limpeza da vari√°vel 'comment_responses'\n",
    "pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments]) \\\n",
    "    .groupby(['comment_responses','comment_num_responses', 'comment_operator_responded']).size().reset_index(name='count').sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar valores omissos\n",
    "NAs = pd.DataFrame({\n",
    "    'n Omissos':pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments])[['comment_reactions_clean','comment_num_responses','comment_operator_responded']].isna().sum(),\n",
    "    '% Omissos':round(((pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments])[['comment_reactions_clean','comment_num_responses','comment_operator_responded']].isna().sum()\n",
    "                 / len(pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments]))) * 100),2)})\n",
    "\n",
    "# Print dos resultados\n",
    "NAs"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### üìë Tabela Sintese dos *Coment√°rios*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# Calcular a m√©dia de palavras no 'comment_text'\n",
    "comment_length_avg = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'M√©dia de Caracteres': [Vodafone_Comments['comment_text'].str.len().mean(),\n",
    "                            MEO_Comments['comment_text'].str.len().mean(),\n",
    "                            NOS_Comments['comment_text'].str.len().mean(),\n",
    "                            DIGI_News_Comments['comment_text'].str.len().mean()]\n",
    "})\n",
    "\n",
    "# Fun√ß√£o para calcular o n√∫mero m√©dio de palavras em um texto\n",
    "def count_avg_words(text):\n",
    "    words = text.split()  # Dividir o texto em palavras\n",
    "    return len(words)  # Retornar o n√∫mero de palavras\n",
    "\n",
    "# Calcular a m√©dia de palavras no 'comment_text'\n",
    "comment_word_avg = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'M√©dia de Palavras': [Vodafone_Comments['comment_text'].apply(count_avg_words).mean(),\n",
    "                           MEO_Comments['comment_text'].apply(count_avg_words).mean(),\n",
    "                           NOS_Comments['comment_text'].apply(count_avg_words).mean(),\n",
    "                           DIGI_News_Comments['comment_text'].apply(count_avg_words).mean()]\n",
    "})\n",
    "\n",
    "\n",
    "# Calcular a m√©dia de coment√°rios por User\n",
    "avg_comments_per_user = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'M√©dia de Coment√°rios por Utilizador': [Vodafone_Comments.groupby('user_link').size().mean(),\n",
    "                                          MEO_Comments.groupby('user_link').size().mean(),\n",
    "                                          NOS_Comments.groupby('user_link').size().mean(),\n",
    "                                          DIGI_News_Comments.groupby('user_link').size().mean()]\n",
    "})\n",
    "\n",
    "# Calcular a m√©dia de 'comment_reactions' e 'comment_responses'\n",
    "comment_reactions_avg = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'M√©dia de Rea√ß√µes por Coment√°rio': [Vodafone_Comments['comment_reactions_clean'].astype(int).mean(),\n",
    "                                         MEO_Comments['comment_reactions_clean'].astype(int).mean(),\n",
    "                                         NOS_Comments['comment_reactions_clean'].astype(int).mean(),\n",
    "                                         DIGI_News_Comments['comment_reactions_clean'].astype(int).mean()]\n",
    "})\n",
    "\n",
    "comment_responses_avg = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'M√©dia de Respostas por Coment√°rio': [Vodafone_Comments['comment_num_responses'].mean(),\n",
    "                                          MEO_Comments['comment_num_responses'].mean(),\n",
    "                                          NOS_Comments['comment_num_responses'].mean(),\n",
    "                                          DIGI_News_Comments['comment_num_responses'].mean()]\n",
    "})\n",
    "\n",
    "# Calcular o n√∫mero total de coment√°rios respondidos pela operadora\n",
    "total_comments_responded = pd.DataFrame({\n",
    "    'Operadora': ['Vodafone', 'MEO', 'NOS', 'DIGI [News]'],\n",
    "    'Total de Coment√°rios Respondidos': [Vodafone_Comments[Vodafone_Comments['comment_operator_responded'] == 1].shape[0],\n",
    "                                          MEO_Comments[MEO_Comments['comment_operator_responded'] == 1].shape[0],\n",
    "                                          NOS_Comments[NOS_Comments['comment_operator_responded'] == 1].shape[0],\n",
    "                                          DIGI_News_Comments[DIGI_News_Comments['comment_operator_responded'] == 1].shape[0]]\n",
    "})\n",
    "\n",
    "# Concatenar todos os dataframes em uma √∫nica tabela de s√≠ntese\n",
    "comments_summary_table = pd.concat([comment_length_avg, comment_word_avg, avg_comments_per_user, comment_reactions_avg, comment_responses_avg, total_comments_responded], \n",
    "                                   axis=1)\n",
    "comments_summary_table.set_index('Operadora', inplace=True)\n",
    "comments_summary_table.index = ['Vodafone', 'MEO', 'NOS', 'DIGI [News]']\n",
    "round(comments_summary_table,2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Adicionar a coluna 'page' com o nome da operadora em cada DataFrame\n",
    "Vodafone_Comments.insert(0, 'page', 'Vodafone')\n",
    "MEO_Comments.insert(0, 'page', 'MEO')\n",
    "NOS_Comments.insert(0, 'page', 'NOS')\n",
    "\n",
    "# Criar um dicion√°rio de mapeamento com base no ID do post e a p√°gina correspondente\n",
    "mapping_dict = dict(zip(DIGI_News_Posts['post_id'], DIGI_News_Posts['page']))\n",
    "\n",
    "# Adicionar a coluna 'page' ao DIGI_News_Comments usando o m√©todo map e o dicion√°rio de mapeamento\n",
    "DIGI_News_Comments['page'] = DIGI_News_Comments['post_id'].map(mapping_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Dropar a coluna 'comment_reactions' e renomear a coluna 'comment_reactions_clean' para 'comment_reactions'\n",
    "df_Comments = pd.concat([Vodafone_Comments, MEO_Comments, NOS_Comments, DIGI_News_Comments], ignore_index=True) \\\n",
    "                .drop(columns=['comment_reactions', 'comment_responses'], axis=1) \\\n",
    "                .rename(columns={'comment_reactions_clean': 'comment_reactions'})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Simplificar a coluna 'user_link'\n",
    "df_Comments['user_link'] = df_Comments['user_link'].apply(lambda x: re.sub(r'\\?comment_id=.*$', '', x))\n",
    "df_Comments['user_link'] = df_Comments['user_link'].apply(lambda x: re.sub(r'&comment_id=.*$', '', x))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Guardar a Compila√ß√£o de Comments em formato .txt\n",
    "df_Comments.to_csv('Datasets_Vodafone/Facebook_Comments.txt', sep='\\t', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "afd0ccd8a9fd46c8bdc20bdbf516c869",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "### üí• Valores Omissos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a3fedc73d70a409c8e65746290fc0826",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "def check_missing_values(df, operator_name):\n",
    "    print(f'\\033[1mOperadora: {operator_name}\\033[0m')\n",
    "    for col in df.columns:\n",
    "        # Verificar se o tipo de dado na coluna √© string\n",
    "        if df[col].dtype == 'object':\n",
    "            # Contar valores vazios ou nulos\n",
    "            empty_values = df[col].apply(lambda x: x.strip() == '' if isinstance(x, str) else False).sum()\n",
    "        else:\n",
    "            # Contar valores nulos\n",
    "            empty_values = df[col].isna().sum()\n",
    "        if empty_values > 0:\n",
    "            percentage = round(empty_values / len(df) * 100, 2)\n",
    "            print(f'{col:<35}: {empty_values:<6} ({percentage}%)')\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a3fedc73d70a409c8e65746290fc0826",
    "deepnote_cell_type": "code",
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "print('\\033[1m====================== N¬∫ de Valores Omissos [POSTS] ======================\\033[0m')\n",
    "check_missing_values(Vodafone_Posts, 'Vodafone')\n",
    "check_missing_values(MEO_Posts, 'MEO')\n",
    "check_missing_values(NOS_Posts, 'NOS')\n",
    "check_missing_values(DIGI_News_Posts, 'DIGI [News]')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print('\\033[1m====================== N¬∫ de Valores Omissos [COMMENTS] ======================\\033[0m')\n",
    "check_missing_values(Vodafone_Comments, 'Vodafone')\n",
    "check_missing_values(MEO_Comments, 'MEO')\n",
    "check_missing_values(NOS_Comments, 'NOS')\n",
    "check_missing_values(DIGI_News_Comments, 'DIGI [News]')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5b9eb915b5784eb2a24eca91ec642ef8",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## ü©∫ Verificar a Qualidade do *Scraping* dos Coment√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Merge dos posts e coment√°rios para cada operadora\n",
    "Vodafone = pd.merge(Vodafone_Posts, Vodafone_Comments, how='inner', left_on='post_id', right_on='post_id')\n",
    "MEO = pd.merge(MEO_Posts, MEO_Comments, how='inner', left_on='post_id', right_on='post_id')\n",
    "NOS = pd.merge(NOS_Posts, NOS_Comments, how='inner', left_on='post_id', right_on='post_id')\n",
    "DIGI_News = pd.merge(DIGI_News_Posts, DIGI_News_Comments, how='inner', left_on='post_id', right_on='post_id')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ver os posts que ficou\n",
    "# Ordenar os DataFrames pelo 'post_date' e encontrar o 'post_id' mais antigo \n",
    "print(\"\\033[1mPost ID + Antigo [Vodafone]:\\033[0m\", Vodafone.sort_values(by='post_date').iloc[0]['post_id'])\n",
    "print(\"\\033[1mPost ID + Antigo [MEO]:\\033[0m\", MEO.sort_values(by='post_date').iloc[0]['post_id'])\n",
    "print(\"\\033[1mPost ID + Antigo [NOS]:\\033[0m\", NOS.sort_values(by='post_date').iloc[0]['post_id'])\n",
    "print(\"\\033[1mPost ID + Antigo [DIGI News]:\\033[0m\", DIGI_News.sort_values(by='post_date').iloc[0]['post_id'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vodafone_Posts.index[Vodafone_Posts['post_id'] == 786361258049006][0]\n",
    "# 2420 de 2420 - Todos"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Vodafone_Posts.tail(2) # Conseguiu todos!!!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Escolher o limite m√°ximo de coment√°rios que conseguem ser recolhidos numa p√°gina \n",
    "# - 'comment_count' s√£o apenas os coment√°rios com texto, mas na p√°gina foram carregados outros com stickers e fotos\n",
    "Vodafone_Comments.groupby('post_id') \\\n",
    "    .size() \\\n",
    "    .reset_index(name='comment_count') \\\n",
    "    .sort_values(by='comment_count', ascending=False)\\\n",
    "    .head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "MEO_Posts.index[MEO_Posts['post_id'] == 10155069344574570][0]\n",
    "# 2280 de 2280"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# MEO.index[MEO['post_id'] == 10155069693164570][0]\n",
    "# MEO.index[MEO['post_id'] == 10155069344574570][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "MEO_Posts.tail(2) # Conseguiu todos!!!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Escolher o limite m√°ximo de coment√°rios que conseguem ser recolhidos numa p√°gina \n",
    "# - 'comment_count' s√£o apenas os coment√°rios com texto, mas na p√°gina foram carregados outros com stickers e fotos\n",
    "MEO_Comments.groupby('post_id') \\\n",
    "    .size() \\\n",
    "    .reset_index(name='comment_count') \\\n",
    "    .sort_values(by='comment_count', ascending=False)\\\n",
    "    .head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "NOS_Posts.index[NOS_Posts['post_id'] == 1989089567771512][0]\n",
    "# 1212 de 1900"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# NOS_Posts[NOS_Posts.index > 719] # Faltam 1180\n",
    "NOS.sort_values(by='post_date')[['post_id', 'post_date', 'comment_id', 'user_name']] \\\n",
    "    .groupby('post_id') \\\n",
    "    .first() \\\n",
    "    .sort_values(by='post_date')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "NOS_Posts.tail(2) # Conseguiu todos!!!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Escolher o limite m√°ximo de coment√°rios que conseguem ser recolhidos numa p√°gina \n",
    "# - 'comment_count' s√£o apenas os coment√°rios com texto, mas na p√°gina foram carregados outros com stickers e fotos\n",
    "NOS_Comments.groupby('post_id') \\\n",
    "    .size() \\\n",
    "    .reset_index(name='comment_count') \\\n",
    "    .sort_values(by='comment_count', ascending=False)\\\n",
    "    .head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "NOS_Posts.index[NOS_Posts['post_id'] == 2693202527360209][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4ca3e54d05d24bfe96b35c48f41783c5",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "552cec19eaff47a5a9fe417e1aa004d5",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "### üìë Tabela de Frequ√™ncias Absolutas e Relativas  e  üìä Gr√°ficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ac6355286a864c3fa76513907ef49fa2",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# Tabela de frequ√™ncia absoluta e relativa para o ano\n",
    "vodafone_freq_df = pd.DataFrame({'n': Vodafone_Posts['post_year'].value_counts(), \n",
    "                                 '%': round(Vodafone_Posts['post_year'].value_counts(normalize=True) * 100, 2),\n",
    "                                 'n C': Vodafone_Posts.groupby('post_year')['post_comments'].sum(),\n",
    "                                 '% C': round(Vodafone_Posts.groupby('post_year')['post_comments'].sum() / \n",
    "                                                        Vodafone_Posts['post_comments'].sum()* 100, 2)})\n",
    "meo_freq_df = pd.DataFrame({'n': MEO_Posts['post_year'].value_counts(), \n",
    "                            '%': round(MEO_Posts['post_year'].value_counts(normalize=True) * 100, 2),\n",
    "                            'n C': MEO_Posts.groupby('post_year')['post_comments'].sum(),\n",
    "                            '% C': round(MEO_Posts.groupby('post_year')['post_comments'].sum() / \n",
    "                                                        MEO_Posts['post_comments'].sum()* 100, 2)})\n",
    "nos_freq_df = pd.DataFrame({'n': NOS_Posts['post_year'].value_counts(), \n",
    "                            '%': round(NOS_Posts['post_year'].value_counts(normalize=True) * 100, 2),\n",
    "                            'n C': NOS_Posts.groupby('post_year')['post_comments'].sum(),\n",
    "                            '% C': round(NOS_Posts.groupby('post_year')['post_comments'].sum() / \n",
    "                                                        NOS_Posts['post_comments'].sum()* 100, 2)})\n",
    "digi_freq_df = pd.DataFrame({'n': DIGI_News_Posts['post_year'].value_counts(), \n",
    "                            '%': round(DIGI_News_Posts['post_year'].value_counts(normalize=True) * 100, 2),\n",
    "                            'n C': DIGI_News_Posts.groupby('post_year')['post_comments'].sum(),\n",
    "                            '% C': round(DIGI_News_Posts.groupby('post_year')['post_comments'].sum() / \n",
    "                                                        DIGI_News_Posts['post_comments'].sum()* 100, 2)})\n",
    "\n",
    "# Nomear o √≠ndice\n",
    "vodafone_freq_df.index.name = 'Ano'\n",
    "meo_freq_df.index.name = 'Ano'\n",
    "nos_freq_df.index.name = 'Ano'\n",
    "digi_freq_df.index.name = 'Ano'\n",
    "\n",
    "# Exibir as tabelas de frequ√™ncia absoluta e relativa para cada operadora\n",
    "print(\"\\033[1mTabelas de Frequ√™ncias Absolutas e Relativas para cada operadora (Posts) \\033[0m\\n\")\n",
    "display_side_by_side(vodafone_freq_df.sort_index(), meo_freq_df.sort_index(), \n",
    "                     nos_freq_df.sort_index(), digi_freq_df.sort_index(),\n",
    "                     super_title = \"Posts Obtidos por Ano\",\n",
    "                     titles=['Vodafone','MEO', 'NOS', 'DIGI'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tabela de frequ√™ncia absoluta e relativa para o ano dos coment√°rios de cada operadora\n",
    "vodafone_comments_freq_df = pd.DataFrame({'n': Vodafone['post_year'].value_counts(), \n",
    "                                          '%': round(Vodafone['post_year'].value_counts(normalize=True) * 100, 2)})\n",
    "meo_comments_freq_df = pd.DataFrame({'n': MEO['post_year'].value_counts(), \n",
    "                                     '%': round(MEO['post_year'].value_counts(normalize=True) * 100, 2)})\n",
    "nos_comments_freq_df = pd.DataFrame({'n': NOS['post_year'].value_counts(), \n",
    "                                     '%': round(NOS['post_year'].value_counts(normalize=True) * 100, 2)})\n",
    "digi_comments_freq_df = pd.DataFrame({'n': DIGI_News['post_year'].value_counts(), \n",
    "                                      '%': round(DIGI_News['post_year'].value_counts(normalize=True) * 100, 2)})\n",
    "\n",
    "# Nomear o √≠ndice\n",
    "vodafone_comments_freq_df.index.name = 'Ano'\n",
    "meo_comments_freq_df.index.name = 'Ano'\n",
    "nos_comments_freq_df.index.name = 'Ano'\n",
    "digi_comments_freq_df.index.name = 'Ano'\n",
    "\n",
    "# Exibir as tabelas de frequ√™ncia absoluta e relativa para cada operadora\n",
    "print(\"\\033[1mTabelas de Frequ√™ncias Absolutas e Relativas para cada operadora (Coment√°rios) \\033[0m\\n\")\n",
    "display_side_by_side(vodafone_comments_freq_df.sort_index(), meo_comments_freq_df.sort_index(), \n",
    "                     nos_comments_freq_df.sort_index(), digi_comments_freq_df.sort_index(),\n",
    "                     super_title = \"Coment√°rios Obtidos por Ano\",\n",
    "                     titles=['Vodafone','MEO', 'NOS', 'DIGI'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Calcular o r√°cio de coment√°rios esperados em rela√ß√£o aos coment√°rios obtidos\n",
    "vodafone_comments = pd.DataFrame({'n Obtido': vodafone_comments_freq_df['n'],\n",
    "                                  'n Esperado': vodafone_freq_df['n C']}).fillna(0).astype(int)\n",
    "vodafone_comments['R√°cio Coment√°rios'] = round(vodafone_comments['n Obtido'] / vodafone_comments['n Esperado'] * 100, 2)\n",
    "\n",
    "meo_comments = pd.DataFrame({'n Obtido': meo_comments_freq_df['n'],\n",
    "                             'n Esperado': meo_freq_df['n C']}).fillna(0).astype(int)\n",
    "meo_comments['R√°cio Coment√°rios'] = round(meo_comments['n Obtido'] / meo_comments['n Esperado'] * 100, 2)\n",
    "\n",
    "nos_comments = pd.DataFrame({'n Obtido': nos_comments_freq_df['n'],\n",
    "                             'n Esperado': nos_freq_df['n C']}).fillna(0).astype(int)\n",
    "nos_comments['R√°cio Coment√°rios'] = round(nos_comments['n Obtido'] / nos_comments['n Esperado'] * 100, 2)\n",
    "\n",
    "digi_comments = pd.DataFrame({'n Obtido': digi_comments_freq_df['n'],\n",
    "                              'n Esperado': digi_freq_df['n C']}).fillna(0).astype(int)\n",
    "\n",
    "# Filtrar o DataFrame para remover as linhas onde 'n Obtido' √© igual a zero\n",
    "digi_comments = digi_comments[digi_comments['n Obtido'] != 0]\n",
    "\n",
    "digi_comments['RC'] = round(digi_comments['n Obtido'] / digi_comments['n Esperado'] * 100, 2)\n",
    "\n",
    "# Exibir as tabelas lado a lado\n",
    "display_side_by_side(vodafone_comments.sort_index()[vodafone_comments.index >= 2017], \n",
    "                     meo_comments.sort_index()[meo_comments.index >= 2017], \n",
    "                     nos_comments.sort_index()[nos_comments.index >= 2017], \n",
    "                     digi_comments.sort_index(),\n",
    "                     super_title = \"R√°cio de Coment√°rios por Ano\",\n",
    "                     titles=['Vodafone','MEO', 'NOS', 'DIGI [News]'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='1.3'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>AED | An√°lise Explorat√≥ria de Dados</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "321c6705349c44b6a5760f8f5508ea8a",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## üîé AED - An√°lise Explorat√≥ria de Dados "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Facebook_Posts = pd.read_csv('Datasets_Vodafone/Facebook_Posts.txt', sep='\\t', encoding='utf-8')\n",
    "Facebook_Comments = pd.read_csv('Datasets_Vodafone/Facebook_Comments.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Garantir que as colunas 'post_id' e 'page' s√£o do tipo str\n",
    "Facebook_Comments['post_id'] = Facebook_Comments['post_id'].astype(str)\n",
    "Facebook_Comments['page'] = Facebook_Comments['page'].astype(str)\n",
    "\n",
    "Facebook_Posts['post_id'] = Facebook_Posts['post_id'].astype(str)\n",
    "Facebook_Posts['page'] = Facebook_Posts['page'].astype(str)\n",
    "\n",
    "# Copiar os valores da coluna 'page' antes da substitui√ß√£o\n",
    "DIGI_News_Fontes = Facebook_Posts.loc[Facebook_Posts['page'].str.contains('DIGI'), 'page'].copy()\n",
    "\n",
    "# Limpar os nomes das fontes de dados\n",
    "DIGI_News_Fontes = DIGI_News_Fontes.str.extract(r'\\[([^\\]]+)\\]', expand=False)\n",
    "\n",
    "Facebook_Posts.loc[Facebook_Posts['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "Facebook_Comments.loc[Facebook_Comments['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Juntar as tabelas\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Comments, Facebook_Posts, how='outer', on=['post_id', 'page'])\n",
    "# Facebook_Posts_Comments.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Verificar todos os DataFrames quanto √† presen√ßa do caractere 'ÔøΩ'\n",
    "for df_name, df in zip(['Facebook_Posts', 'Facebook_Comments'], [Facebook_Posts, Facebook_Comments]):\n",
    "    print(f\"\\033[1mVerificando o DataFrame {df_name}...\\033[0m\")\n",
    "    for column in df.columns:\n",
    "        contains_character = df[column].astype(str).str.contains('ÔøΩ', na=False).any()\n",
    "        if contains_character:\n",
    "            print(f\"Caractere 'ÔøΩ' encontrado na coluna '{column}' do DataFrame {df_name}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"\\033[1mPosts S/ Coment√°rios:\\033[0m\", len(Facebook_Posts_Comments[Facebook_Posts_Comments['comment_id'].isna()]))\n",
    "print(\"\\033[1mDuplicados:\\033[0m\", len(Facebook_Posts_Comments) - len(Facebook_Posts_Comments.drop_duplicates(subset=['comment_id'], keep='first')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Lista de Colunas da Base de Dados\n",
    "df_colunas = ['page',\n",
    "              'post_id', 'post_link', 'post_date', 'post_reactions', 'post_comments', 'post_shares',\n",
    "              'post_text', 'post_text_clean', 'post_sentiment_label',\n",
    "\n",
    "              'TXRBSF_post_sentiment_label', 'TXRBSF_post_sentiment_score',\n",
    "              'mDeBERTa_post_sentiment_label', 'mDeBERTa_post_sentiment_score',\n",
    "              'post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI',\n",
    "\n",
    "              'mDeBERTa_post_topic_label_1','mDeBERTa_post_topic_score_1',\n",
    "              'mDeBERTa_post_topic_label_2', 'mDeBERTa_post_topic_score_2',\n",
    "              'mDeBERTa_post_topic_label_3', 'mDeBERTa_post_topic_score_3',\n",
    "              'mDeBERTa_post_CR_label', 'mDeBERTa_post_CR_score',\n",
    "\n",
    "              'comment_id', 'comment_link', 'comment_reactions', 'comment_num_responses',\n",
    "              'comment_operator_responded', 'comment_text', 'comment_text_clean', 'comment_sentiment_label',\n",
    "              'TXRBSF_comment_sentiment_label', 'TXRBSF_comment_sentiment_score',\n",
    "              'mDeBERTa_comment_sentiment_label', 'mDeBERTa_comment_sentiment_score',\n",
    "              'comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI',\n",
    "              'mDeBERTa_comment_topic_label_1', 'mDeBERTa_comment_topic_score_1',\n",
    "              'mDeBERTa_comment_topic_label_2', 'mDeBERTa_comment_topic_score_2',\n",
    "              'mDeBERTa_comment_topic_label_3', 'mDeBERTa_comment_topic_score_3',\n",
    "              'mDeBERTa_comment_CR_label', 'mDeBERTa_comment_CR_score',\n",
    "\n",
    "              'user_name', 'user_link', 'user_freguesia', 'user_concelho', 'user_distrito',\n",
    "              'user_pais', 'user_predicted_genre']\n",
    "\n",
    "\n",
    "# Import da Base de Dados com An√°lise\n",
    "# Caching: https://docs.streamlit.io/develop/concepts/architecture/caching\n",
    "# @st.cache_resource\n",
    "def load_data(columns=None):\n",
    "    # Carregar as partes dos arquivos\n",
    "    parts = []\n",
    "    for i in range(3):\n",
    "        part = pd.read_pickle(f'Datasets_Vodafone/Facebook_PCU_Analysis_part_{i+1}.pkl')\n",
    "        if columns is not None:\n",
    "            part = part[columns]\n",
    "        parts.append(part)\n",
    "\n",
    "    # Concatenar as partes para obter o DataFrame completo\n",
    "    Facebook_PCU_Analysis = pd.concat(parts)\n",
    "\n",
    "    # Alterar tipos de dados\n",
    "    Facebook_PCU_Analysis['post_date'] = pd.to_datetime(Facebook_PCU_Analysis['post_date'])\n",
    "    Facebook_PCU_Analysis['post_id'] = Facebook_PCU_Analysis['post_id'].astype(str)\n",
    "\n",
    "    # Selecionar colunas, se necess√°rio\n",
    "    if columns is not None:\n",
    "        Facebook_PCU_Analysis = Facebook_PCU_Analysis[columns]\n",
    "\n",
    "    return Facebook_PCU_Analysis"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_PCU_Analysis = load_data()\n",
    "Facebook_PCU_Analysis = Facebook_PCU_Analysis[Facebook_PCU_Analysis['page'] == 'Vodafone']"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gr√°ficos e Tabelas `N¬∫ de Posts e Coment√°rios`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gr√°fico Circular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar figura e eixos\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "df_graficos = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]\n",
    "df_graficos.loc[df_graficos['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Contar a frequ√™ncia de posts √∫nicos por p√°gina\n",
    "posts_frequencia = df_graficos[df_graficos['post_id'].notna()].groupby('page')['post_id'].nunique()\n",
    "\n",
    "# Configura√ß√µes do gr√°fico de pizza de posts\n",
    "labels_posts = posts_frequencia.index\n",
    "valores_posts = posts_frequencia.values\n",
    "colors_posts = ['#FFE807', '#007E8D','#555555','#E60001'] # Selecionar tons de azul para cada categoria\n",
    "explode_posts = (0.00,0.02, 0.02, 0.02)\n",
    "\n",
    "# Cria√ß√£o do gr√°fico de pizza de posts\n",
    "axs[0].pie(valores_posts, colors=colors_posts, autopct='%1.1f%%', \n",
    "           startangle=270, pctdistance=0.65, labeldistance=1.1, \n",
    "           textprops={'color': 'white', 'weight': 'bold', 'size': '12', 'horizontalalignment':'center'},\n",
    "           explode=explode_posts)\n",
    "axs[0].axis('equal')\n",
    "axs[0].set_title('Propor√ß√£o de Posts por Operadora', fontsize=14, fontweight='bold')\n",
    "\n",
    "legend_properties = {'weight':'bold', 'size':'14'}\n",
    "axs[0].legend(labels=labels_posts, title='Operadora',title_fontproperties=legend_properties, \n",
    "              fontsize=12, loc='upper right', bbox_to_anchor=(1.25, 1), frameon=False)\n",
    "\n",
    "# Contar a frequ√™ncia de comments por p√°gina\n",
    "comments_frequencia = df_graficos[df_graficos['comment_id'].notna()]['page'].value_counts()\n",
    "\n",
    "# Configura√ß√µes do gr√°fico de pizza de comments\n",
    "labels_comments = comments_frequencia.index\n",
    "valores_comments = comments_frequencia.values\n",
    "colors_comments = ['#007E8D','#555555','#E60001','#FFE807'] # Selecionar tons de azul para cada categoria\n",
    "explode_comments = (0.00, 0.02, 0.02, 0.02)\n",
    "\n",
    "# Cria√ß√£o do gr√°fico de pizza de comments\n",
    "axs[1].pie(valores_comments, colors=colors_comments, autopct='%1.1f%%', \n",
    "           startangle=270, pctdistance=0.65, labeldistance=1.5, \n",
    "           textprops={'color': 'white', 'weight': 'bold', 'size': '12', 'horizontalalignment':'center'},\n",
    "           explode=explode_comments)\n",
    "axs[1].axis('equal')\n",
    "axs[1].set_title('Propor√ß√£o de Comments por Operadora', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/2_Gr√°ficos_Circulares.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definir cores para as fontes de dados\n",
    "colors_fontes = {'CNN Portugal':'#333333', 'Pplware':'#00205F','Observador':'#539DD6',\n",
    "                 '4gnews':'#EB852D', 'Jornal de Not√≠cias':'#03649C', 'P√∫blico':'#D71921'}\n",
    "\n",
    "# Criar figura e eixo\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "# Contar a frequ√™ncia de posts √∫nicos por p√°gina\n",
    "fontes_frequencia = DIGI_News_Fontes.value_counts()\n",
    "\n",
    "# Configura√ß√µes do gr√°fico de pizza de fontes de dados\n",
    "labels_fontes = fontes_frequencia.index\n",
    "valores_fontes = fontes_frequencia.values\n",
    "\n",
    "# Mapear as cores corretas para cada p√°gina usando o dicion√°rio colors_fontes\n",
    "cores = [colors_fontes[fonte] for fonte in labels_fontes]\n",
    "\n",
    "# Criar gr√°fico de pizza de fontes de dados\n",
    "explode = (0.01, 0.01, 0.01, 0.01, 0.01, 0.01)\n",
    "ax.pie(fontes_frequencia, labels=fontes_frequencia.index, autopct='%1.1f%%', colors=cores,\n",
    "       startangle=90, pctdistance=0.65, labeldistance=1.05,\n",
    "       textprops={'color': 'white', 'weight': 'bold', 'size': '13'}, \n",
    "       wedgeprops=dict(edgecolor='w'), explode = explode)\n",
    "ax.axis('equal')\n",
    "ax.set_title('Propor√ß√£o de Fontes de Dados das Not√≠cias da DIGI', fontsize=16, fontweight='bold')\n",
    "\n",
    "legend_properties = {'weight': 'bold', 'size': '14'}\n",
    "ax.legend(labels=labels_fontes, title='Fonte de Dados', title_fontproperties=legend_properties,\n",
    "              fontsize=12, loc='upper right', bbox_to_anchor=(1.3, 1), frameon=False)\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/2.2_Gr√°ficos_Circulares.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S√©rie Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import matplotlib.lines as mlines\n",
    "\n",
    "# Plotar a s√©rie temporal\n",
    "palette = sns.color_palette('colorblind', 4)\n",
    "fig, ax1 = plt.subplots(figsize=(17,6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Converter a coluna 'post_date' para o tipo datetime, caso ainda n√£o esteja no formato correto\n",
    "df_serie_temporal = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]\n",
    "df_serie_temporal['post_date'] = pd.to_datetime(df_serie_temporal['post_date'])\n",
    "\n",
    "# Calcular o n√∫mero total de coment√°rios por m√™s para cada p√°gina\n",
    "posts_per_month = df_serie_temporal.groupby(['page', pd.Grouper(key='post_date', freq='M')])['post_id'] \\\n",
    "    .nunique().reset_index(name='Total de Publica√ß√µes')\n",
    "comments_per_month = df_serie_temporal.groupby(['page', pd.Grouper(key='post_date', freq='M')])['comment_id'] \\\n",
    "    .size().reset_index(name='Total de Coment√°rios')\n",
    "\n",
    "# Juntar as 'pages' DIGI News\n",
    "posts_per_month.loc[posts_per_month['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "comments_per_month.loc[comments_per_month['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Definir a paleta de cores\n",
    "palette = ['#FFE807','#007E8D', '#555555','#E60001'] \n",
    "\n",
    "# Plotar a s√©rie temporal do N¬∫ de coment√°rios e posts por m√™s para cada p√°gina\n",
    "sns.lineplot(data=posts_per_month, x='post_date', y='Total de Publica√ß√µes', hue='page', \n",
    "             palette=palette, ax=ax1, linestyle='--', alpha=0.3, linewidth=1, marker='s', markersize=3)\n",
    "sns.lineplot(data=comments_per_month, x='post_date', y='Total de Coment√°rios', hue='page', \n",
    "             palette=palette, ax=ax2, linewidth=1, marker='o', markersize=3)\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax1.set_title('S√©rie Temporal do N¬∫ de Publica√ß√µes e Coment√°rios por M√™s para Cada Operadora\\n', \n",
    "          fontsize=24, fontweight='bold')\n",
    "ax1.set_ylabel('N¬∫ Total de Publica√ß√µes\\n', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('\\nN¬∫ Total de Coment√°rios', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Aumentar o tamanho das letras dos r√≥tulos dos eixos X e Y\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "for lab in ax1.get_xticklabels():\n",
    "    # if lab.get_text() == \"Crit√©rio\":\n",
    "    lab.set_fontweight('bold')\n",
    "\n",
    "# # Mostrar a legenda\n",
    "legend_properties = {'weight':'bold', 'size':'12'}\n",
    "legend_labels = ['Publica√ß√µes', 'Coment√°rios']\n",
    "legend_lines = [mlines.Line2D([], [], color='black', linestyle='--', alpha=0.3),\n",
    "                mlines.Line2D([], [], color='black')]\n",
    "ax1.legend(legend_lines, legend_labels, title='Total', title_fontproperties=legend_properties, fontsize='10', \n",
    "           bbox_to_anchor=(0.4, 1.02), loc='upper left', frameon=False)\n",
    "ax2.legend(title='Operadora', title_fontproperties=legend_properties, fontsize='10', \n",
    "           bbox_to_anchor=(0.6, 1.02), loc='upper right',frameon=False)\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "sns.despine(top=True, right=False)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/1_S√©rie_Temporal_Posts e Comments.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabelas de Frequ√™ncias dos Posts por M√™s por Operadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos comments por m√™s por operadora\n",
    "df_serie_temporal.loc[df_serie_temporal['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Calcular o dia da semana (0 = segunda-feira, 6 = domingo)\n",
    "df_serie_temporal['post_weekday'] = df_serie_temporal['post_date'].dt.weekday\n",
    "\n",
    "# Dicion√°rio para mapear os n√∫meros de dia da semana para seus equivalentes por extenso\n",
    "weekday_names = {\n",
    "    0: 'Segunda-feira',\n",
    "    1: 'Ter√ßa-feira',\n",
    "    2: 'Quarta-feira',\n",
    "    3: 'Quinta-feira',\n",
    "    4: 'Sexta-feira',\n",
    "    5: 'S√°bado',\n",
    "    6: 'Domingo'\n",
    "}\n",
    "\n",
    "# Aplicar a convers√£o para extenso\n",
    "df_serie_temporal['post_weekday'] = df_serie_temporal['post_weekday'].map(weekday_names)\n",
    "\n",
    "# Calcular a tabela de frequ√™ncia dos posts por dia da semana\n",
    "posts_per_weekday = pd.pivot_table(df_serie_temporal, \n",
    "                                   index='post_weekday', \n",
    "                                   columns='page', \n",
    "                                   values='post_id', \n",
    "                                   aggfunc='nunique')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "posts_per_weekday = posts_per_weekday[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "\n",
    "# Lista de dias da semana em ordem cronol√≥gica\n",
    "ordered_weekdays = ['Segunda-feira', 'Ter√ßa-feira', 'Quarta-feira', 'Quinta-feira', 'Sexta-feira', 'S√°bado', 'Domingo']\n",
    "\n",
    "# Reindexar a tabela de acordo com a lista de dias da semana ordenada\n",
    "posts_per_weekday = posts_per_weekday.reindex(ordered_weekdays)\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "posts_per_weekday.columns = [f'{col}' for col in posts_per_weekday.columns]\n",
    "posts_per_weekday.index.name = 'Dia da Semana'\n",
    "posts_per_weekday_percent = round(posts_per_weekday.div(posts_per_weekday.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular o dia do m√™s\n",
    "df_serie_temporal['day_of_month'] = df_serie_temporal['post_date'].dt.day\n",
    "\n",
    "# Calcular a tabela de frequ√™ncia dos posts por dia do m√™s\n",
    "posts_per_day_of_month = pd.pivot_table(df_serie_temporal, \n",
    "                                        index='day_of_month', \n",
    "                                        columns='page', \n",
    "                                        values='post_id', \n",
    "                                        aggfunc='nunique')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "posts_per_day_of_month = posts_per_day_of_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "posts_per_day_of_month.columns = [f'{col}' for col in posts_per_day_of_month.columns]\n",
    "posts_per_day_of_month.index.name = 'Dia do M√™s'\n",
    "posts_per_day_of_month_percent = round(posts_per_day_of_month.div(posts_per_day_of_month.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular o dia da semana\n",
    "df_serie_temporal['day_of_week'] = df_serie_temporal['post_date'].dt.day_of_week\n",
    "\n",
    "# Calcular a semana do m√™s\n",
    "df_serie_temporal['week_of_month'] = ((df_serie_temporal['day_of_month'] - 1) // 7) + 1\n",
    "\n",
    "# Calcular a tabela de frequ√™ncia dos posts por semana do m√™s\n",
    "posts_per_week_of_month = pd.pivot_table(df_serie_temporal, \n",
    "                                         index='week_of_month', \n",
    "                                         columns='page', \n",
    "                                         values='post_id', \n",
    "                                         aggfunc='nunique')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "posts_per_week_of_month = posts_per_week_of_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "posts_per_week_of_month.columns = [f'{col}' for col in posts_per_week_of_month.columns]\n",
    "posts_per_week_of_month.index.name = 'Semana do M√™s'\n",
    "posts_per_week_of_month_percent = round(posts_per_week_of_month.div(posts_per_week_of_month.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos posts por m√™s por operadora\n",
    "df_serie_temporal.loc[df_serie_temporal['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "df_serie_temporal['post_month'] = df_serie_temporal['post_month'].astype(int)\n",
    "\n",
    "# Dicion√°rio para mapear os n√∫meros de m√™s para seus equivalentes por extenso\n",
    "month_names = {1: 'Janeiro', 2: 'Fevereiro', 3: 'Mar√ßo', 4: 'Abril', 5: 'Maio', 6: 'Junho',\n",
    "               7: 'Julho', 8: 'Agosto', 9: 'Setembro', 10: 'Outubro', 11: 'Novembro', 12: 'Dezembro'}\n",
    "\n",
    "# Aplicar a convers√£o para extenso\n",
    "df_serie_temporal['post_month'] = df_serie_temporal['post_month'].map(month_names)\n",
    "\n",
    "posts_per_month = pd.pivot_table(df_serie_temporal, \n",
    "                                 index= pd.Grouper(key='post_month'), #, freq='Y'), \n",
    "                                 columns='page', \n",
    "                                 values='post_id', \n",
    "                                 aggfunc='nunique')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "posts_per_month = posts_per_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Lista de meses em ordem cronol√≥gica\n",
    "ordered_months = ['Janeiro', 'Fevereiro', 'Mar√ßo', 'Abril', 'Maio', 'Junho', 'Julho', \n",
    "                  'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "# Reindexar a tabela de acordo com a lista de meses ordenada\n",
    "posts_per_month = posts_per_month.reindex(ordered_months)\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "posts_per_month.columns = [f'{col}' for col in posts_per_month.columns]\n",
    "posts_per_month.index.name = 'M√™s'\n",
    "posts_per_month_percent = round(posts_per_month.div(posts_per_month.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos posts por ano\n",
    "df_serie_temporal['post_year'] = df_serie_temporal['post_year'].astype(int)\n",
    "posts_per_year = pd.pivot_table(df_serie_temporal, \n",
    "                                index='post_year', \n",
    "                                columns='page', \n",
    "                                values='post_id', \n",
    "                                aggfunc='nunique')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "posts_per_year = posts_per_year[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "posts_per_year.columns = [f'{col}' for col in posts_per_year.columns]\n",
    "posts_per_year.index.name = 'Ano'\n",
    "\n",
    "# Calcular a %\n",
    "posts_per_year_percent = round(posts_per_year.div(posts_per_year.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "display_side_by_side(posts_per_weekday_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     # posts_per_day_of_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     # posts_per_week_of_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     posts_per_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     posts_per_year_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     super_title = \"An√°lise dos Posts\",\n",
    "                     titles=['Dia da Semana', 'M√™s', 'Ano'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Para os Coment√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos comments por dia da semana\n",
    "comments_per_weekday = pd.pivot_table(df_serie_temporal, \n",
    "                                   index='post_weekday', \n",
    "                                   columns='page', \n",
    "                                   values='post_id', \n",
    "                                   aggfunc='count')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "comments_per_weekday = comments_per_weekday[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "\n",
    "# Lista de dias da semana em ordem cronol√≥gica\n",
    "ordered_weekdays = ['Segunda-feira', 'Ter√ßa-feira', 'Quarta-feira', 'Quinta-feira', 'Sexta-feira', 'S√°bado', 'Domingo']\n",
    "\n",
    "# Reindexar a tabela de acordo com a lista de dias da semana ordenada\n",
    "comments_per_weekday = comments_per_weekday.reindex(ordered_weekdays)\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "comments_per_weekday.columns = [f'{col}' for col in comments_per_weekday.columns]\n",
    "comments_per_weekday.index.name = 'Dia da Semana'\n",
    "comments_per_weekday_percent = round(comments_per_weekday.div(comments_per_weekday.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos comments por dia do m√™s\n",
    "comments_per_day_of_month = pd.pivot_table(df_serie_temporal, \n",
    "                                        index='day_of_month', \n",
    "                                        columns='page', \n",
    "                                        values='post_id', \n",
    "                                        aggfunc='count')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "comments_per_day_of_month = comments_per_day_of_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "comments_per_day_of_month.columns = [f'{col}' for col in comments_per_day_of_month.columns]\n",
    "comments_per_day_of_month.index.name = 'Dia do M√™s'\n",
    "comments_per_day_of_month_percent = round(comments_per_day_of_month.div(comments_per_day_of_month.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos coment√°rios por semana do m√™s\n",
    "comments_per_week_of_month = pd.pivot_table(df_serie_temporal, \n",
    "                                         index='week_of_month', \n",
    "                                         columns='page', \n",
    "                                         values='post_id', \n",
    "                                         aggfunc='count')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "comments_per_week_of_month = comments_per_week_of_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "comments_per_week_of_month.columns = [f'{col}' for col in comments_per_week_of_month.columns]\n",
    "comments_per_week_of_month.index.name = 'Semana do M√™s'\n",
    "comments_per_week_of_month_percent = round(comments_per_week_of_month.div(comments_per_week_of_month.sum(axis=0), \n",
    "                                                                          axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos comments por m√™s\n",
    "comments_per_month = pd.pivot_table(df_serie_temporal, \n",
    "                                 index= pd.Grouper(key='post_month'), #, freq='Y'), \n",
    "                                 columns='page', \n",
    "                                 values='post_id', \n",
    "                                 aggfunc='count')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "comments_per_month = comments_per_month[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Lista de meses em ordem cronol√≥gica\n",
    "ordered_months = ['Janeiro', 'Fevereiro', 'Mar√ßo', 'Abril', 'Maio', 'Junho', 'Julho', 'Agosto', 'Setembro', 'Outubro', 'Novembro', 'Dezembro']\n",
    "\n",
    "# Reindexar a tabela de acordo com a lista de meses ordenada\n",
    "comments_per_month = comments_per_month.reindex(ordered_months)\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "comments_per_month.columns = [f'{col}' for col in comments_per_month.columns]\n",
    "comments_per_month.index.name = 'M√™s'\n",
    "comments_per_month_percent = round(comments_per_month.div(comments_per_month.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular a tabela de frequ√™ncia dos comments por ano\n",
    "comments_per_year = pd.pivot_table(df_serie_temporal, \n",
    "                                index='post_year', \n",
    "                                columns='page', \n",
    "                                values='post_id', \n",
    "                                aggfunc='count')\n",
    "\n",
    "# Reordenar as colunas de acordo com a ordem desejada\n",
    "comments_per_year = comments_per_year[['Vodafone', 'MEO', 'NOS', 'DIGI News']]\n",
    "\n",
    "# Renomear as colunas para tornar mais claro o conte√∫do\n",
    "comments_per_year.columns = [f'{col}' for col in posts_per_year.columns]\n",
    "comments_per_year.index.name = 'Ano'\n",
    "comments_per_year_percent = round(comments_per_year.div(comments_per_year.sum(axis=0), axis=1) * 100,1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "display_side_by_side(comments_per_weekday_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     # comments_per_day_of_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     # comments_per_week_of_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     comments_per_month_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     comments_per_year_percent[['Vodafone', 'MEO', 'NOS']],\n",
    "                     super_title = \"An√°lise dos Coment√°rios\",\n",
    "                     titles=['Dia da Semana', 'M√™s', 'Ano'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gr√°ficos `Posts | N¬∫ de Rea√ß√µes, Coment√°rios e Partilhas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definir as cores para cada operadora com transpar√™ncia\n",
    "colors = {'Vodafone': '#E60001', 'MEO': '#007E8D', 'NOS': '#555555', 'DIGI News':'#FFE807'}\n",
    "\n",
    "# Criar figura e eixos\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "legend_properties = {'weight':'bold', 'size':'10'}\n",
    "\n",
    "# Iterar sobre as vari√°veis e plotar os histogramas\n",
    "for i, variavel in enumerate(['post_reactions', 'post_comments', 'post_shares']):\n",
    "    # Agrupar os dados pelo identificador √∫nico do post e selecionar o primeiro registro de cada grupo\n",
    "    posts_data = df_graficos.groupby('post_id').first()\n",
    "\n",
    "    # Iterar sobre as operadoras e plotar os histogramas\n",
    "    for operadora, dados in posts_data.groupby('page'):\n",
    "        axs[i].hist(dados[variavel], bins=20, color=colors[operadora], alpha=0.5, label=operadora)\n",
    "\n",
    "    # Definir os r√≥tulos dos eixos x e y e o t√≠tulo do gr√°fico\n",
    "    axs[i].set_xlabel('\\n'+variavel.replace('_', ' ').title(), fontweight='bold', fontsize=14)\n",
    "    axs[i].set_title(f'Distribui√ß√£o de {variavel.replace(\"_\", \" \").title()} por Operadora \\n', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Adicionar a legenda\n",
    "    axs[i].legend(title='\\nOperadora\\n', title_fontproperties=legend_properties, fontsize='8', \n",
    "           bbox_to_anchor=(0.4, 1), loc='upper left', frameon=False)\n",
    "\n",
    "axs[0].set_ylabel('Frequ√™ncia\\n', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"Relat√≥rio/Gr√°ficos/4.1_Histograma_RCS.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definir as cores para cada operadora com transpar√™ncia\n",
    "colors = {'Vodafone': '#E60001', 'MEO': '#007E8D', 'NOS': '#555555', 'DIGI News':'#FFE807'}\n",
    "\n",
    "# Criar figura e eixos\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "legend_properties = {'weight':'bold', 'size':'10'}\n",
    "\n",
    "# Definir os limites dos bins\n",
    "bins_limits = {\n",
    "    'post_reactions': (0, 20000),\n",
    "    'post_comments': (0, 1000),\n",
    "    'post_shares': (0, 1000)\n",
    "}\n",
    "\n",
    "# Iterar sobre as vari√°veis e plotar os histogramas\n",
    "for i, variavel in enumerate(['post_reactions', 'post_comments', 'post_shares']):\n",
    "    # Agrupar os dados pelo identificador √∫nico do post e selecionar o primeiro registro de cada grupo\n",
    "    posts_data = df_graficos.groupby('post_id').first()\n",
    "\n",
    "    # Definir os bins com base nos limites especificados\n",
    "    bins = np.linspace(bins_limits[variavel][0], bins_limits[variavel][1], 20)  # 20 bins uniformes\n",
    "\n",
    "    # Iterar sobre as operadoras e plotar os histogramas\n",
    "    for operadora, dados in posts_data.groupby('page'):\n",
    "        axs[i].hist(dados[variavel], bins=bins, color=colors[operadora], alpha=0.6, label=operadora)\n",
    "\n",
    "    # Definir os r√≥tulos dos eixos x e y e o t√≠tulo do gr√°fico\n",
    "    axs[i].set_xlabel('\\n'+variavel.replace('_', ' ').title(), fontweight='bold', fontsize=14)\n",
    "    axs[i].set_title(f'Distribui√ß√£o de {variavel.replace(\"_\", \" \").title()} por Operadora \\n', fontsize=14, fontweight='bold')\n",
    "\n",
    "    # Adicionar a legenda\n",
    "    axs[i].legend(title='\\nOperadora\\n', title_fontproperties=legend_properties, fontsize='8', \n",
    "           bbox_to_anchor=(0.4, 1), loc='upper left', frameon=False)\n",
    "    \n",
    "    # Definir o limite do eixo x para evitar outliers\n",
    "    if variavel == 'post_reactions':\n",
    "        axs[i].set_xlim(0, 15000)  # Defina o limite do eixo x para post_reactions\n",
    "    elif variavel == 'post_comments':\n",
    "        axs[i].set_xlim(0, 1000)   # Defina o limite do eixo x para post_comments\n",
    "    elif variavel == 'post_shares':\n",
    "        axs[i].set_xlim(0, 1000)   # Defina o limite do eixo x para post_shares\n",
    "    \n",
    "axs[0].set_ylabel('Frequ√™ncia\\n', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/4.2_Histograma_RCS.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S√©rie Temporal do `N¬∫ de Rea√ß√µes & N¬∫ de Partilhas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plotar a s√©rie temporal\n",
    "fig, ax1 = plt.subplots(figsize=(17,6))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Converter a coluna 'post_date' para o tipo datetime, caso ainda n√£o esteja no formato correto\n",
    "df_serie_temporal_posts = Facebook_Posts[Facebook_Posts['post_year'] >= 2019]\n",
    "df_serie_temporal_posts['post_date'] = pd.to_datetime(df_serie_temporal_posts['post_date'])\n",
    "\n",
    "# Calcular o n√∫mero total de coment√°rios por m√™s para cada p√°gina\n",
    "reactions_per_month = df_serie_temporal_posts.groupby(['page', pd.Grouper(key='post_date', freq='M')])['post_reactions'] \\\n",
    "    .sum().reset_index(name='Total de Rea√ß√µes')\n",
    "shares_per_month = df_serie_temporal_posts.groupby(['page', pd.Grouper(key='post_date', freq='M')])['post_shares'] \\\n",
    "    .sum().reset_index(name='Total de Partilhas')\n",
    "\n",
    "# Juntar as 'pages' DIGI News\n",
    "reactions_per_month.loc[reactions_per_month['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "shares_per_month.loc[shares_per_month['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Definir a paleta de cores\n",
    "palette = ['#FFE807','#007E8D', '#555555','#E60001'] \n",
    "\n",
    "# Plotar a s√©rie temporal do N¬∫ de rea√ß√µes e posts por m√™s para cada p√°gina\n",
    "sns.lineplot(data=reactions_per_month, x='post_date', y='Total de Rea√ß√µes', hue='page', \n",
    "             palette=palette, ax=ax1, linewidth=1, marker='^', markersize=3)\n",
    "sns.lineplot(data=shares_per_month, x='post_date', y='Total de Partilhas', hue='page', \n",
    "             palette=palette, ax=ax2, linestyle='--', alpha=0.3, linewidth=1, marker='s', markersize=3)\n",
    "\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax1.set_title('S√©rie Temporal do N¬∫ de Partilhas e Rea√ß√µes por M√™s para Cada Operadora\\n', \n",
    "          fontsize=24, fontweight='bold')\n",
    "ax1.set_ylabel('N¬∫ Total de Rea√ß√µes\\n', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_ylabel('\\nN¬∫ Total de Partilhas', fontweight='bold', fontsize=14)\n",
    "\n",
    "# Aumentar o tamanho das letras dos r√≥tulos dos eixos X e Y\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "ax2.tick_params(axis='both', which='major', labelsize=12)\n",
    "for lab in ax1.get_xticklabels():\n",
    "    # if lab.get_text() == \"Crit√©rio\":\n",
    "    lab.set_fontweight('bold')\n",
    "\n",
    "# Mostrar a legenda\n",
    "legend_properties = {'weight':'bold', 'size':'12'}\n",
    "legend_labels = ['Rea√ß√µes', 'Partilhas']\n",
    "legend_lines = [mlines.Line2D([], [], color='black'),\n",
    "                mlines.Line2D([], [], color='black', linestyle='--', alpha=0.3)]\n",
    "ax1.legend(legend_lines, legend_labels, title='Total', title_fontproperties=legend_properties, fontsize='10', \n",
    "           bbox_to_anchor=(0.4, 1.02), loc='upper left', frameon=False)\n",
    "ax2.legend(title='Operadora', title_fontproperties=legend_properties, fontsize='10', \n",
    "           bbox_to_anchor=(0.6, 1.02), loc='upper right',frameon=False)\n",
    "\n",
    "\n",
    "# Criar um mapeamento entre o nome da p√°gina e a cor correspondente\n",
    "color_mapping = {'Vodafone': '#E60001', 'MEO': '#007E8D', 'NOS': '#555555', 'DIGI News': '#FFE807'}\n",
    "\n",
    "# Encontrar os valores m√°ximos de cada s√©rie temporal\n",
    "max_reactions = reactions_per_month.groupby('page')['Total de Rea√ß√µes'].max()\n",
    "# max_shares = shares_per_month.groupby('page')['Total de Partilhas'].max()\n",
    "\n",
    "# Obter os √≠ndices dos valores m√°ximos\n",
    "idx_max_reactions = reactions_per_month.loc[reactions_per_month.groupby('page')['Total de Rea√ß√µes'].idxmax()]\n",
    "# idx_max_shares = shares_per_month.loc[shares_per_month.groupby('page')['Total de Partilhas'].idxmax()]\n",
    "\n",
    "# Adicionar texto nos pontos m√°ximos indicando o m√™s associado\n",
    "for ax, data, max_data, idx_max_data in zip([ax1], [reactions_per_month],[max_reactions], [idx_max_reactions]):\n",
    "    for page, max_value, idx_max in zip(max_data.index, max_data.values, idx_max_data.iterrows()):\n",
    "        ax.text(idx_max[1]['post_date'], max_value, idx_max[1]['post_date'].strftime('%B'),\n",
    "                color=color_mapping[page], fontsize=12, fontweight='bold', ha='left', va='bottom')\n",
    "\n",
    "\n",
    "# Mostrar o gr√°fico\n",
    "sns.despine(top=True, right=False)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/5_S√©rie_Temporal_RP.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# reactions_per_month.sort_values(by='Total de Rea√ß√µes', ascending= False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================== comment_reactions VS comment_num_responses ================================\n",
    "# Definir as cores para cada operadora\n",
    "colors = {'Vodafone': '#E60001', 'MEO': '#007E8D', 'NOS': '#555555', 'DIGI News': '#FFE807'}\n",
    "\n",
    "# Filtrar os dados para remover valores nulos em ambas as colunas\n",
    "filtered_data = df_graficos.dropna(subset=['comment_reactions', 'comment_num_responses'])\n",
    "\n",
    "# Criar o gr√°fico de dispers√£o com cores diferenciadas por operadora\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(data=filtered_data, \n",
    "                x='comment_reactions', \n",
    "                y='comment_num_responses', \n",
    "                hue='page', \n",
    "                palette=colors, \n",
    "                alpha=0.75\n",
    "               )\n",
    "plt.title('N¬∫ de Rea√ß√µes vs. N¬∫ de Respostas\\n', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "# plt.xlabel('\\nN¬∫ de Rea√ß√µes', fontweight='bold', fontsize=16)\n",
    "# plt.ylabel('N¬∫ de Respostas\\n', fontweight='bold', fontsize=16)\n",
    "plt.legend(title='Operadora', title_fontproperties=legend_properties, fontsize='10', \n",
    "           loc='upper right',frameon=False)\n",
    "\n",
    "plt.xlim(0,None)\n",
    "plt.ylim(0,None)\n",
    "\n",
    "\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/7_Dispers√£o_RR.png\",format='png',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================== comment_reactions VS comment_num_responses ================================\n",
    "# Definir as cores para cada operadora\n",
    "colors = {'Vodafone': '#E60001', 'MEO': '#007E8D', 'NOS': '#555555', 'DIGI News': '#FFE807'}\n",
    "\n",
    "# Filtrar os dados para remover valores nulos em ambas as colunas\n",
    "filtered_data = df_graficos.dropna(subset=['comment_reactions', 'comment_num_responses'])\n",
    "\n",
    "# Criar o gr√°fico de dispers√£o com cores diferenciadas por operadora\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(data=filtered_data, \n",
    "                x='comment_reactions', \n",
    "                y='comment_num_responses', hue='page', \n",
    "                palette=colors, \n",
    "                alpha=0.75,\n",
    "                legend=None)\n",
    "\n",
    "# plt.title('Gr√°fico de Dispers√£o: Rea√ß√µes vs. Respostas\\n', fontsize=20, fontweight='bold')\n",
    "plt.xlabel('\\nN¬∫ de Rea√ß√µes', fontweight='bold', fontsize=16)\n",
    "plt.ylabel('N¬∫ de Respostas\\n', fontweight='bold', fontsize=16)\n",
    "# plt.legend(title='Operadora', title_fontproperties=legend_properties, fontsize='10', \n",
    "#            loc='upper right',frameon=False)\n",
    "\n",
    "plt.xlim(0,150)\n",
    "plt.ylim(0,80)\n",
    "\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/7.2_Dispers√£o_RR.png\",format='png',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "################################### comment_operator_responded ###################################\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "# Filtrar os dados para remover valores nulos em 'comment_operator_responded' e 'page'\n",
    "filtered_data_2 = df_graficos.dropna(subset=['comment_operator_responded', 'page'])\n",
    "filtered_data_2 = filtered_data_2[filtered_data_2['page'] != 'DIGI News']\n",
    "\n",
    "# Criar o gr√°fico de barras\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.countplot(data=filtered_data_2, \n",
    "              x='comment_operator_responded', \n",
    "              hue='page', \n",
    "              palette=colors,\n",
    "              ax=ax\n",
    "             )\n",
    "\n",
    "# Adicionar t√≠tulo e r√≥tulos dos eixos\n",
    "ax.set_title('Respostas aos Coment√°rios por Operadora\\n', \n",
    "          fontsize=18, fontweight='bold')\n",
    "ax.set_ylabel('N¬∫ de Respostas (n)\\n', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('\\nOperadora Respondeu?', fontweight='bold', fontsize=14)\n",
    "ax.set_xticklabels(['\\nN√£o', '\\nSim'], fontweight='bold', fontsize=12)\n",
    "ax.legend(title='Operadora', title_fontproperties=legend_properties, fontsize='10',\n",
    "          loc='upper right',frameon=False)\n",
    "\n",
    "# Definir uma fun√ß√£o para formatar os valores como porcentagens\n",
    "def percentage(x, pos):\n",
    "    return '{:.1f}%'.format(x)\n",
    "  \n",
    "page_label = ['Vodafone','Vodafone','MEO','MEO','NOS','NOS']\n",
    "\n",
    "# Adicionar os valores em percentagem acima das barras\n",
    "for i, p in enumerate(ax.patches):\n",
    "    height = p.get_height()\n",
    "    total_page_responses = len(filtered_data_2[filtered_data_2['page'] == page_label[i]])\n",
    "    percentage_value = (height / total_page_responses) * 100\n",
    "    ax.text(p.get_x() + p.get_width()/2., \n",
    "            (height * 1.02), \n",
    "            '{:.1f}%'.format(percentage_value), \n",
    "            ha=\"center\",\n",
    "            fontsize=12, \n",
    "            fontweight='bold')\n",
    "    \n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/8_Barras Resposta da Operadora.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calcular o n√∫mero de respostas por operadora\n",
    "responses_by_operator = filtered_data_2.groupby('page')['comment_operator_responded'].value_counts().unstack()\n",
    "\n",
    "# Calcular a porcentagem de respostas \"Sim\" por operadora\n",
    "responses_by_operator['% N√£o'] = (responses_by_operator[0] / responses_by_operator.sum(axis=1)) * 100\n",
    "responses_by_operator['% Sim'] = (responses_by_operator[1] / responses_by_operator.sum(axis=1)) * 100\n",
    "\n",
    "# Renomear as colunas para \"N√£o\" e \"Sim\"\n",
    "responses_by_operator = responses_by_operator.rename(columns={0: 'N√£o', 1: 'Sim'})\n",
    "responses_by_operator"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßñAn√°lise dos Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "df_users_2019 = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]\n",
    "df_users_2019.loc[df_users_2019['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# N¬∫ de link √∫nicos\n",
    "print('\\033[1mN¬∫ de Users √önicos a Comentar a partir de 2019:\\033[0m',len(df_users_2019['user_link'].unique()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================ N¬∫ de Coment√°rios por User ==================================\n",
    "\n",
    "# N√∫mero m√°ximo de coment√°rios de um user √∫nico:\n",
    "max_comments_per_user = df_users_2019.groupby(['user_link'])['comment_id'].count().reset_index()\n",
    "max_comments_per_user = max_comments_per_user.groupby('user_link').max()\n",
    "\n",
    "# Contagem do n√∫mero de coment√°rios por users\n",
    "comment_counts = max_comments_per_user['comment_id'].value_counts()\n",
    "\n",
    "# Calcular a % de users para cada n√∫mero de coment√°rios\n",
    "stratified_comments = pd.DataFrame({'N¬∫ de Coment√°rios': comment_counts.index, \n",
    "                                    'n Users': comment_counts.values,\n",
    "                                    '% Users': round(comment_counts / len(max_comments_per_user) * 100,1)})\n",
    "\n",
    "# Calcular a % acumulada de usu√°rios para cada n√∫mero de coment√°rios\n",
    "stratified_comments['% Users >= X'] = (100 - stratified_comments['% Users'].cumsum()).shift(-1).fillna(0)\n",
    "\n",
    "# Ordenar o DataFrame pelo n√∫mero de coment√°rios\n",
    "stratified_comments.sort_values(by='N¬∫ de Coment√°rios', inplace=True)\n",
    "stratified_comments.set_index('N¬∫ de Coment√°rios', inplace=True)\n",
    "stratified_comments"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Estudar os casos com +150 coment√°rios\n",
    "stratified_comments.loc[stratified_comments.index >= 150]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identificar o user_link que fez mais de 150 coment√°rios\n",
    "user_with_over_150_comments = df_users_2019['user_link'].value_counts() \\\n",
    "    [df_users_2019['user_link'].value_counts() >= 150]\n",
    "\n",
    "# Filtrar os coment√°rios dos usu√°rios com mais de 150 coment√°rios\n",
    "comments_of_users_over_150 = df_users_2019[df_users_2019['user_link'].isin(user_with_over_150_comments.index)]\n",
    "\n",
    "print('\\033[1mN¬∫ de Users com + 150 Coment√°rios\\033[0m',len(comments_of_users_over_150.groupby(['user_link'])))\n",
    "comments_per_user_per_post = comments_of_users_over_150.groupby(['post_link', 'user_name']) \\\n",
    "                                                       .size() \\\n",
    "                                                       .reset_index(name='comment_count')\n",
    "\n",
    "# comments_per_user_per_post[(comments_per_user_per_post['user_name'] != 'Carlos Pereira') & \n",
    "#                            (comments_per_user_per_post['user_name'] != 'Lopes Bastos')]\n",
    "\n",
    "# Verifica-se que s√£o users ativos em v√°rios posts diferentes! [CGPT]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# comments_of_users_over_150_comments.groupby(['user_link']).sum()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================ N¬∫ de Coment√°rios por User [Agrupado de '> 10'] ==================================\n",
    "# Identificar os √≠ndices onde o n√∫mero de coment√°rios √© maior ou igual a 10\n",
    "indices_over_10 = stratified_comments.index >= 10\n",
    "\n",
    "# Calcular a soma do n√∫mero de usu√°rios e a porcentagem acumulada para esses √≠ndices\n",
    "total_users_over_10 = stratified_comments.loc[indices_over_10, 'n Users'].sum()\n",
    "percent_users_over_10 = stratified_comments.loc[indices_over_10, '% Users'].sum()\n",
    "\n",
    "\n",
    "# Criar uma nova linha para '> 10' com os valores calculados, remover  as linhas abaixo do √≠ndice 10 e renomear para '> 10'\n",
    "stratified_comments.loc[10] = [total_users_over_10, percent_users_over_10, percent_users_over_10]\n",
    "stratified_comments = stratified_comments.loc[stratified_comments.index <= 10]\n",
    "stratified_comments.rename(index={10: '> 10'}, inplace=True)\n",
    "display_side_by_side(stratified_comments,\n",
    "                     super_title = \"An√°lise dos Utilizadores\",\n",
    "                     titles=['N¬∫ de Coment√°rios por User <br></br>'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Agrupar os dados pelo ano, pelo link do usu√°rio e pela operadora, contando o n√∫mero de links de usu√°rios √∫nicos\n",
    "df_users_2019['post_year'] = df_users_2019['post_year'].astype(int)\n",
    "unique_users_per_year = df_users_2019.groupby(['post_year', 'user_link', 'page']).size().reset_index(name='unique_user_count')\n",
    "unique_users_per_year = unique_users_per_year.drop_duplicates()\n",
    "unique_users_per_year.fillna(0, inplace=True)\n",
    "unique_users_per_year_pivot = unique_users_per_year.pivot_table(index='post_year', columns='page', values='unique_user_count', aggfunc='sum').fillna(0)\n",
    "unique_users_per_year_pivot.columns.name = ''\n",
    "unique_users_per_year_pivot.index.name = 'Ano'\n",
    "display_side_by_side(unique_users_per_year_pivot,\n",
    "                     super_title = \" \",\n",
    "                     titles=['N¬∫ de Users √önicos por Ano e Operadora <br></br>'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================ Users que comentam em diferentes operadoras ==================================\n",
    "# Contagem do n√∫mero de operadoras em que cada user comentou\n",
    "user_operators_count = df_users_2019.groupby(['user_link', 'user_name'])['page'].nunique().reset_index()\n",
    "\n",
    "# Calcular a contagem de users por n√∫mero de operadoras\n",
    "users_per_operator_count = user_operators_count.groupby('page').size().reset_index(name='count')\n",
    "\n",
    "# Calcular a percentagem de users para cada n√∫mero de operadoras\n",
    "users_per_operator_count['%'] = round((users_per_operator_count['count'] / len(user_operators_count)) * 100,1)\n",
    "\n",
    "# Calcular a percentagem de users que comentaram em mais de 1, 2, 3 operadoras\n",
    "percentages = {}\n",
    "for i in range(0, users_per_operator_count['page'].max()):\n",
    "    percentage = users_per_operator_count[users_per_operator_count['page'] > i]['%'].sum()\n",
    "    percentages[f'> {i}'] = percentage\n",
    "\n",
    "users_per_operator_count_1 = pd.DataFrame.from_dict(percentages, orient='index', columns=['%'])\n",
    "users_per_operator_count_1.index.name = 'N¬∫ de Operadoras'\n",
    "\n",
    "users_per_operator_count.rename(columns={'page': 'N¬∫ de Operadoras', 'count':'n'}, inplace=True)\n",
    "display_side_by_side(users_per_operator_count.set_index('N¬∫ de Operadoras'),\n",
    "                     users_per_operator_count_1,\n",
    "                     super_title = \"\",\n",
    "                     titles=['N¬∫ de Users por Operadora <br></br>', '<br></br>'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Etapa 1: Agrupe os dados pelo usu√°rio e obtenha uma lista de operadoras em que cada usu√°rio comentou\n",
    "user_comments_by_operator = df_users_2019.groupby('user_link')['page'].apply(set).reset_index()\n",
    "\n",
    "# Etapa 2: Calcular a porcentagem de usu√°rios que comentam entre operadoras\n",
    "# Criar um DataFrame vazio para armazenar os resultados\n",
    "operator_matrix = pd.DataFrame(index=df_users_2019['page'].unique(), columns=df_users_2019['page'].unique())\n",
    "\n",
    "# Preencher o DataFrame com a porcentagem de usu√°rios que comentam entre operadoras\n",
    "for operator1 in operator_matrix.index:\n",
    "    for operator2 in operator_matrix.columns:\n",
    "        if operator1 == operator2:\n",
    "            # Se for a mesma operadora, definir como NaN\n",
    "            operator_matrix.loc[operator1, operator2] = 100\n",
    "        else:\n",
    "            # Calcular a % de usu√°rios que comentaram em ambas as operadoras\n",
    "            users_in_both_operators = user_comments_by_operator[user_comments_by_operator['page'].apply(lambda x: operator1 in x and operator2 in x)]\n",
    "            users_in_operator1 = user_comments_by_operator[user_comments_by_operator['page'].apply(lambda x: operator1 in x)]\n",
    "            if not users_in_operator1.empty:\n",
    "                percentage = len(users_in_both_operators) / len(users_in_operator1)\n",
    "                operator_matrix.loc[operator1, operator2] = round(percentage * 100,1)\n",
    "            else:\n",
    "                # Se n√£o houver usu√°rios que comentaram na operadora 1, definir como 0\n",
    "                operator_matrix.loc[operator1, operator2] = 0\n",
    "\n",
    "display_side_by_side(operator_matrix,\n",
    "                     super_title = \"\",\n",
    "                     titles=['Matriz de % Horizontais de Users que comentam entre Operadoras <br></br>'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='2'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>3 | Data Preparation</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_Posts = pd.read_csv('Datasets_Vodafone/Facebook_Posts.txt', sep='\\t', encoding='utf-8')\n",
    "Facebook_Comments = pd.read_csv('Datasets_Vodafone/Facebook_Comments.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Garantir que as colunas 'post_id' e 'page' s√£o do tipo str\n",
    "Facebook_Comments['post_id'] = Facebook_Comments['post_id'].astype(str)\n",
    "Facebook_Comments['page'] = Facebook_Comments['page'].astype(str)\n",
    "\n",
    "Facebook_Posts['post_id'] = Facebook_Posts['post_id'].astype(str)\n",
    "Facebook_Posts['page'] = Facebook_Posts['page'].astype(str)\n",
    "\n",
    "# Copiar os valores da coluna 'page' antes da substitui√ß√£o\n",
    "DIGI_News_Fontes = Facebook_Posts.loc[Facebook_Posts['page'].str.contains('DIGI'), 'page'].copy()\n",
    "\n",
    "# Limpar os nomes das fontes de dados\n",
    "DIGI_News_Fontes = DIGI_News_Fontes.str.extract(r'\\[([^\\]]+)\\]', expand=False)\n",
    "\n",
    "Facebook_Posts.loc[Facebook_Posts['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "Facebook_Comments.loc[Facebook_Comments['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "\n",
    "# Juntar as tabelas\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Comments, Facebook_Posts, how='outer', on=['post_id', 'page'])\n",
    "# Facebook_Posts_Comments.info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_Posts_Comments_2019_24 = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]\n",
    "print(\"\\033[1mN¬∫ de Publica√ß√µes entre 2019-2024:\\033[0m {:>12}\".format(\n",
    "    len(Facebook_Posts_Comments_2019_24[~Facebook_Posts_Comments_2019_24.duplicated(subset=['post_id'])])))\n",
    "print(\"\\033[1mN¬∫ de Coment√°rios entre 2019-2024:\\033[0m {:>14}\".format(\n",
    "    len(Facebook_Posts_Comments_2019_24)))\n",
    "print(\"\\033[1mN¬∫ de Utilizadores √önicos entre 2019-2024:\\033[0m {:>3}\".format(\n",
    "    len(Facebook_Posts_Comments_2019_24[~Facebook_Posts_Comments_2019_24.duplicated(subset=['user_link'])])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='4.2'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Users | Local de Resid√™ncia + G√©nero</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üôé‚Äç‚ôÇÔ∏èüôé‚Äç‚ôÄÔ∏è Utilizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar os users.json\n",
    "Facebook_Users = pd.read_json('Datasets_Vodafone/Facebook_Users.json', encoding='utf-8')\n",
    "Facebook_Users.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\033[1mN¬∫ de Utilizadores √önicos Obtidos:\\033[0m {:>3}\".format(len(Facebook_Users)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar a Base de Dados da DIGI_News_users\n",
    "DIGI_News_Users = pd.read_json('Datasets_Vodafone/Facebook_DIGI_News_users_2024-03-21.json', encoding='utf-8')\n",
    "DIGI_News_Users.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar todos os DataFrames quanto √† presen√ßa do caractere 'ÔøΩ'\n",
    "for df_name, df in zip(['Facebook_Users'], [Facebook_Users]):\n",
    "    print(f\"\\033[1mVerificando o DataFrame {df_name}...\\033[0m\")\n",
    "    for column in df.columns:\n",
    "        contains_character = df[column].astype(str).str.contains('ÔøΩ', na=False).any()\n",
    "        if contains_character:\n",
    "            print(f\"Caractere 'ÔøΩ' encontrado na coluna '{column}' do DataFrame {df_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar as linhas da vari√°vel 'user_current_city'  com caracter especial 'ÔøΩ'\n",
    "CE_Users_Current_City = Facebook_Users.dropna(subset=['user_current_city']) \\\n",
    "    [Facebook_Users.dropna(subset=['user_current_city'])['user_current_city'].str.contains('ÔøΩ')]\n",
    "print('\\033[1mTotal Observa√ß√µes com Caracter Especial = \\033[0m', len(CE_Users_Current_City))\n",
    "CE_Users_Current_City.drop_duplicates(subset='user_current_city').reset_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar as linhas da vari√°vel 'user_hometown'  com caracter especial 'ÔøΩ'\n",
    "CE_Users_Hometown = Facebook_Users.dropna(subset=['user_hometown']) \\\n",
    "    [Facebook_Users.dropna(subset=['user_hometown'])['user_hometown'].str.contains('ÔøΩ')]\n",
    "print('\\033[1mTotal Observa√ß√µes com Caracter Especial = \\033[0m', len(CE_Users_Hometown))\n",
    "CE_Users_Hometown.drop_duplicates(subset='user_hometown').reset_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Estes valores **ser√£o limpos** mais adiante!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **`Local de Resid√™ncia`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ler o CSV com os Distritos, Concelhos e Freguesias de Portugal [Total: 18 Distritos | 308 Concelhos | 3 091 Freguesias]\n",
    "# Fonte: https://dados.gov.pt/pt/datasets/freguesias-de-portugal/#resources\n",
    "dcf_portugal = pd.read_csv('Datasets_Vodafone/Auxiliares/Distritos_Conselhos_Freguesias_Portugal_GOV.csv',sep=\";\")\n",
    "dcf_portugal.head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fonte de Dados: https://public.opendatasoft.com/explore/dataset/georef-portugal-freguesia/table\n",
    "# Optou-se por usar esta para ser poss√≠vel criar as visualiza√ß√µes gr√°ficas, uma vez que faz match com o ficheiro .GeoJSON\n",
    "dcf_portugal_2 = pd.read_excel('Datasets/georef-portugal-freguesia.xlsx')\n",
    "dcf_portugal_2[['Official Name District', 'Official Name Municipality', 'Official Name Parish']].head(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Agrupar por 'distrito' e 'concelho' e contar o n√∫mero de ocorr√™ncias em cada grupo\n",
    "distritos_concelhos = dcf_portugal_2.groupby(['Official Name District', 'Official Name Municipality']).first().reset_index()\n",
    "\n",
    "print('\\033[1mN¬∫ de Concelhos Duplicados:\\033[0m', distritos_concelhos.duplicated(subset=['Official Name Municipality'], keep=False).sum())\n",
    "distritos_concelhos[distritos_concelhos.duplicated(subset=['Official Name Municipality'], keep=False)]\\\n",
    "    [['Official Name District', 'Official Name Municipality', 'Official Name Parish']] \\\n",
    "    .sort_values(by='Official Name Municipality')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\033[1mN¬∫ de Freguesias Duplicadas:\\033[0m', (dcf_portugal_2.duplicated(subset=['Official Name Parish'], keep=False)).sum())\n",
    "dcf_portugal_2[dcf_portugal_2.duplicated(subset=['Official Name Parish'], keep=False)] \\\n",
    "        [['Official Name District', 'Official Name Municipality', 'Official Name Parish']] \\\n",
    "        .sort_values(by='Official Name Parish')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Valores poss√≠veis da vari√°vel 'user_current_city' e 'user_hometown'\n",
    "print('\\033[1mUsers | Current City\\033[0m')\n",
    "Facebook_Users['user_current_city'].unique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\033[1mUsers | Hometown\\033[0m') \n",
    "Facebook_Users['user_hometown'].unique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Limpar 'user_current_city'\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].str.replace('Cidade atual', '')\n",
    "# Limpar espa√ßos extras resultantes da remo√ß√£o de 'Cidade atual'\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].str.strip()\n",
    "\n",
    "# Limpar 'user_hometown'\n",
    "Facebook_Users['user_hometown'] = Facebook_Users['user_hometown'].str.replace('Naturalidade', '')\n",
    "# Limpar espa√ßos extras resultantes da remo√ß√£o de 'Naturalidade'\n",
    "Facebook_Users['user_hometown'] = Facebook_Users['user_hometown'].str.strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(Facebook_Users['user_current_city'].unique())\n",
    "print(Facebook_Users['user_hometown'].unique())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Passo 1: Completar os casos que n√£o cont√™m nada ('') como 'USI' - Facebook_Users Sem Informa√ß√£o\n",
    "Facebook_Users.loc[Facebook_Users['user_current_city'] == '', 'user_current_city'] = 'USI'\n",
    "print(\"\\033[1mN¬∫ de Facebook_Users com 'user_current_city' como ''\\033[0m\", \n",
    "      len(Facebook_Users.loc[Facebook_Users['user_current_city'] == '', 'user_current_city']))\n",
    "Facebook_Users[['user_current_city','user_hometown']].head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(f\"\\033[1mTotal de Linhas completas com USI =\\033[0m {len(Facebook_Users[Facebook_Users['user_current_city'] == 'USI'])} (\\\n",
    "{round(len(Facebook_Users[Facebook_Users['user_current_city'] == 'USI'])/len(Facebook_Users)*100,2)}%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Passo 2: Substituir 'user_hometown' nos casos que n√£o tenha 'user_current_city' se tiverem diferentes de ''\n",
    "print(f\"\\033[1mTotal de Linhas Substituidas = \\033[0m {len(Facebook_Users[(Facebook_Users['user_current_city'] == Facebook_Users['user_hometown']) & (Facebook_Users['user_current_city'] != 'USI') & (Facebook_Users['user_hometown'] != '')])}\\\n",
    "({round(len(Facebook_Users[(Facebook_Users['user_current_city'] == Facebook_Users['user_hometown']) & (Facebook_Users['user_current_city'] != 'USI') & (Facebook_Users['user_hometown'] != '')])/len(Facebook_Users)*100,2)}%)\")\n",
    "\n",
    "Facebook_Users.loc[Facebook_Users['user_hometown'] != '', 'user_current_city'] = \\\n",
    "Facebook_Users.loc[Facebook_Users['user_hometown'] != '', 'user_current_city'] \\\n",
    "    .apply(lambda x: x if x != 'USI' else Facebook_Users.loc[Facebook_Users['user_hometown'] != '', 'user_hometown'].values[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_Users[['user_current_city','user_hometown']].sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Passo 3: Atribuir √†s colunas 'freguesia', 'concelho', 'distrito', 'pais' o termo 'USI' quando 'user_current_city' √© 'USI'\n",
    "Facebook_Users.loc[Facebook_Users['user_current_city'] == 'USI', \n",
    "                   ['user_freguesia', 'user_concelho', 'user_distrito', 'user_pais']] = 'USI'\n",
    "Facebook_Users[['user_current_city','user_freguesia', 'user_concelho', 'user_distrito', 'user_pais']].sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_Users[['user_current_city']].sample(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Passo 4: Verificar e Resolver casos com 'ÔøΩ' - Caracter Especial\n",
    "print(f\"\\033[1mTotal de Observa√ß√µes com ÔøΩ = \\033[0m {len(Facebook_Users[Facebook_Users['user_current_city'].str.contains('ÔøΩ', na=False)])}\\\n",
    " ({round(len(Facebook_Users[Facebook_Users['user_current_city'].str.contains('ÔøΩ', na=False)])/len(Facebook_Users),2)}%)\")\n",
    "Facebook_Users[Facebook_Users['user_current_city'] \\\n",
    "               .str.contains('ÔøΩ', na=False)][['user_current_city']].drop_duplicates(subset=['user_current_city'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Corrigir os caso '√çlhavo, Aveiro' | '√Årvore, Porto' | '√Åguas Santas, Porto' | '√Ågua Longa, Porto' | \n",
    "#                  'Olhos De √Ågua, Faro' | .........\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩLhavo, Aveiro, Portugal', '√çlhavo, Aveiro, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩRvore, Porto, Portugal', '√Årvore, Porto, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩGuas Santas, Porto, Portugal', '√Åguas Santas, Porto, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩGua Longa, Porto, Portugal', '√Ågua Longa, Porto, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('Olhos De ÔøΩGua, Faro, Portugal', 'Olhos De √Ågua, Faro, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩGua De Pau, Azores, Portugal', '√Ågua De Pau, Azores, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('Entre ÔøΩGuas, Madeira, Portugal', 'Entre √Åguas, Madeira, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('ÔøΩGua De Pau, Azores, Portugal', '√Ågua De Pau, Azores, Portugal')\n",
    "Facebook_Users['user_current_city'] = Facebook_Users['user_current_city'].replace('Pinheiro De ÔøΩZere, Viseu, Portugal', 'Pinheiro De √Åzere, Viseu, Portugal')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Confirmar a limpeza\n",
    "Facebook_Users[Facebook_Users['user_current_city'].str.contains('ÔøΩ', na=False)][['user_current_city']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Passo 5: Extrair as informa√ß√µes de freguesia, concelho e distrito da vari√°vel 'user_current_city'\n",
    "\n",
    "# Verificar se o valor √© uma cidade de Portugal\n",
    "portugal_freguesias_normalized = [unidecode(freguesia.strip().lower())for freguesia in dcf_portugal_2['Official Name Parish']]\n",
    "portugal_concelhos_normalized =  [unidecode(concelho.strip().lower()) for concelho  in dcf_portugal_2['Official Name Municipality']]\n",
    "portugal_distritos_normalized =  [unidecode(distrito.strip().lower()) for distrito  in dcf_portugal_2['Official Name District']]\n",
    "\n",
    "# Obter uma lista com todos os termos de freguesias duplicadas\n",
    "freguesias_duplicadas = dcf_portugal_2[dcf_portugal_2.duplicated(subset=['Official Name Parish'], keep=False)]['Official Name Parish'].unique().tolist()\n",
    "freguesias_duplicadas_normalized = [unidecode(freguesia_duplicada.strip().lower()) for freguesia_duplicada in \n",
    "                         dcf_portugal_2[dcf_portugal_2.duplicated(subset=['Official Name Parish'], keep=False)]['Official Name Parish'].unique().tolist()]\n",
    "\n",
    "# Fun√ß√£o para limpar os valores da coluna 'user_current_city' e separ√°-los nas colunas correspondentes\n",
    "def limpar_cidades(city):\n",
    "    # Verificar se o valor √© 'USI' ou est√° vazio\n",
    "    if pd.isna(city) or city.strip().lower() in ['usi', 'us√≠']:\n",
    "        return 'USI', 'USI', 'USI', 'USI', 'N/A'  # Retorna 'USI' em todas as colunas e 'N/A' para o pa√≠s\n",
    "    else:\n",
    "        # Normalizar o texto da cidade atual removendo acentos\n",
    "        city_normalized = unidecode(city.strip().lower())\n",
    "        \n",
    "        # Se tiver 'Azores' como city, substituir para 'A√ßores'\n",
    "        if 'Azores' in city:\n",
    "            city = city.replace('Azores','A√ßores')\n",
    "        \n",
    "        # Verificar se h√° m√∫ltiplas cidades separadas por v√≠rgula\n",
    "        cities = [unidecode(c.strip().lower()) for c in city.split(',')]\n",
    "\n",
    "        # Inicializar valores padr√£o\n",
    "        freguesia = 'N/A'\n",
    "        concelho = 'N/A'\n",
    "        distrito = 'N/A'\n",
    "                \n",
    "        # Verificar cada cidade individualmente\n",
    "        for c in cities:\n",
    "            \n",
    "            # Verificar se a cidade cont√©m informa√ß√µes entre par√™nteses\n",
    "            match = re.search(r'\\((.*?)\\)', c)\n",
    "            \n",
    "            if match:\n",
    "                # Extrair a informa√ß√£o entre par√™nteses como uma nova cidade\n",
    "                new_city = match.group(1).strip()\n",
    "                cities.append(new_city)\n",
    "\n",
    "                # Remover as informa√ß√µes entre par√™nteses da cidade original\n",
    "                c = c.replace(match.group(0), '').strip()\n",
    "                \n",
    "            if c in portugal_freguesias_normalized:\n",
    "                \n",
    "                idx = portugal_freguesias_normalized.index(c)\n",
    "                freguesia = dcf_portugal_2['Official Name Parish'].iloc[idx]\n",
    "                pais = 'Portugal'\n",
    "                \n",
    "                # Verificar se a freguesia identificada est√° no grupo de freguesias duplicadas\n",
    "                if c in freguesias_duplicadas_normalized:\n",
    "                    # Verificar se h√° mais cidades ap√≥s a freguesia identificada\n",
    "                    if len(cities) > 1:\n",
    "                        \n",
    "                        # Iterar sobre as cidades ap√≥s a freguesia identificada\n",
    "                        for next_city in cities[1:]:\n",
    "                            \n",
    "                            # Tentar identificar o concelho e distrito das cidades seguintes\n",
    "                            if next_city in portugal_concelhos_normalized:\n",
    "                                idx = portugal_concelhos_normalized.index(next_city)\n",
    "                                concelho = dcf_portugal_2['Official Name Municipality'].iloc[idx]\n",
    "                                distrito = dcf_portugal_2['Official Name District'].iloc[idx]\n",
    "                                break  # Interromper ap√≥s encontrar o primeiro concelho\n",
    "                                \n",
    "                            elif next_city in portugal_distritos_normalized:\n",
    "                                idx = portugal_distritos_normalized.index(next_city)\n",
    "                                distrito = dcf_portugal_2['Official Name District'].iloc[idx]\n",
    "                                \n",
    "                                # Deduzir o concelho correspondente √† freguesia duplicada no distrito encontrado\n",
    "                                concelhos_distrito = dcf_portugal_2[dcf_portugal_2['Official Name District'] == distrito]['Official Name Municipality'].tolist()\n",
    "                                concelhos_normalized = [unidecode(concelho.strip().lower()) for concelho in concelhos_distrito]\n",
    "                                if freguesia.lower() in ' '.join(concelhos_normalized):\n",
    "                                    concelho = freguesia\n",
    "                                    \n",
    "                                break  # Interromper ap√≥s encontrar o primeiro distrito\n",
    "                                                \n",
    "                # Se for uma freguesia de Portugal que n√£o seja duplicada, preencher as colunas correspondentes\n",
    "                else:\n",
    "                    concelho = dcf_portugal_2['Official Name Municipality'].iloc[idx]\n",
    "                    distrito = dcf_portugal_2['Official Name District'].iloc[idx]\n",
    "                \n",
    "                return freguesia, concelho, distrito, pais, 'N/A'\n",
    "            \n",
    "            elif c in portugal_concelhos_normalized:\n",
    "                # Se for um concelho de Portugal, preencher as colunas correspondentes\n",
    "                idx = portugal_concelhos_normalized.index(c)\n",
    "                concelho = dcf_portugal_2['Official Name Municipality'].iloc[idx]\n",
    "                distrito = dcf_portugal_2['Official Name District'].iloc[idx]\n",
    "                \n",
    "            elif c in portugal_distritos_normalized:\n",
    "                # Se for um distrito de Portugal, preencher as colunas correspondentes\n",
    "                idx = portugal_distritos_normalized.index(c)\n",
    "                distrito = dcf_portugal_2['Official Name District'].iloc[idx]\n",
    "                \n",
    "        # Verificar se alguma cidade corresponde a uma freguesia, concelho ou distrito\n",
    "        if freguesia != 'N/A':\n",
    "            pais = 'Portugal'\n",
    "            return freguesia, concelho, distrito, pais, 'N/A'\n",
    "        \n",
    "        elif concelho != 'N/A':\n",
    "            return 'N/A', concelho, distrito, 'Portugal', 'N/A'\n",
    "        \n",
    "        elif distrito != 'N/A':\n",
    "            return 'N/A', 'N/A', distrito, 'Portugal', 'N/A'\n",
    "        \n",
    "        else:\n",
    "            # Se nenhuma corresponder, retornar os valores padr√£o\n",
    "            return 'N/A', 'N/A', 'N/A', 'N/A', city.strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Aplicar a fun√ß√£o √† coluna 'user_current_city' do DataFrame Users\n",
    "Facebook_Users[['user_freguesia', 'user_concelho', 'user_distrito', 'user_pais', 'user_city_not_portugal']] = \\\n",
    "    Facebook_Users['user_current_city'].apply(limpar_cidades).apply(pd.Series)\n",
    "Facebook_Users[['user_current_city', 'user_freguesia', 'user_concelho', 'user_distrito','user_pais', 'user_city_not_portugal']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar os casos que n√£o conseguiu atribuir em nenhuma vari√°vel apesar de ter 'user_current_city' !='USI'\n",
    "Facebook_Users_N_USI = Facebook_Users[(Facebook_Users['user_current_city'] != 'USI') & \n",
    "                    (Facebook_Users['user_freguesia'] == 'N/A') & \n",
    "                    (Facebook_Users['user_concelho'] == 'N/A') & \n",
    "                    (Facebook_Users['user_distrito'] == 'N/A') & \n",
    "                    (Facebook_Users['user_pais'] == 'N/A')]\n",
    "#       & (Facebook_Users['user_city_not_portugal'] == 'N/A')]\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Facebook_Users com 'user_current_city' != 'USI'\\033[0m     \", len(Facebook_Users_N_USI), \n",
    "      \"(\", round(len(Facebook_Users_N_USI)/len(Facebook_Users),2),\"%)\")\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Facebook_Users com 'user_city_not_portugal' == 'N/A'\\033[0m\", \n",
    "      len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A')]), \n",
    "      \"(\", round(len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A')])/len(Facebook_Users),2),\"%)\")\n",
    "\n",
    "Facebook_Users_N_USI['user_city_not_portugal'].unique()\n",
    "\n",
    "\n",
    "# ------------------------------ Coment√°rio do C√≥digo a dia 5/4/2024 ----------------------------------\n",
    "# 'Gavi√£o (Portugal)', 'Const√¢ncia (Portugal)', 'Mira (Portugal)', 'Alenquer (Portugal)',\n",
    "# 'Albergaria-a-Velha (freguesia)'\n",
    "# 'Ap√∫lia (Esposende)',\n",
    "# 'Santana (Madeira)', 'Ribeira Brava (Madeira)', 'Santa Cruz (Madeira)',\n",
    "# 'Madalena (A√ßores)',\n",
    "# 'Ribeira Grande (Santo Ant√£o)',\n",
    "\n",
    "# Ainda se verifica valores que n√£o devia"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar os casos em que as Freguesia duplicadas aparecem no 'user_current_city' \n",
    "Facebook_Users[Facebook_Users['user_freguesia'].isin(freguesias_duplicadas) \n",
    "               & (Facebook_Users['user_freguesia'] != 'N/A')] \\\n",
    "    [['user_current_city','user_freguesia','user_concelho','user_distrito']].drop_duplicates()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **N¬∫ de Casos com Freguesias Ambiguas =** $311$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar os casos que t√™m 'Freguesia', mas t√™m 'N\\A' no 'Concelho' e Distrito\n",
    "freguesias_sCD = Facebook_Users[(Facebook_Users['user_freguesia'] != 'N/A') \n",
    "                                & (Facebook_Users['user_concelho'] == 'N/A') \n",
    "                                & (Facebook_Users['user_distrito'] == 'N/A')] \\\n",
    "    [['user_current_city','user_freguesia','user_concelho','user_distrito']].drop_duplicates()\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Freguesias sem Concelho e sem Distrito\\033[0m\", len(freguesias_sCD))\n",
    "freguesias_sCD"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar os casos que t√™m 'Freguesia' e 'Distrito', mas t√™m 'N/A' no 'Concelho'\n",
    "freguesias_dC = Facebook_Users[(Facebook_Users['user_freguesia'] != 'N/A')\n",
    "                              & (Facebook_Users['user_distrito'] != 'N/A')\n",
    "                              & (Facebook_Users['user_concelho'] == 'N/A')] \\\n",
    "    [['user_current_city','user_hometown','user_freguesia','user_concelho','user_distrito']].drop_duplicates()\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Freguesias com Distrito, mas sem Concelho\\033[0m\", len(freguesias_dC))\n",
    "freguesias_dC"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar a 'Naturalidade' dos Casos em que a Cidade Atual =/ de Portugal\n",
    "Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A') \n",
    "               & (Facebook_Users['user_hometown'] != '')] \\\n",
    "    [['user_current_city', 'user_hometown', 'user_city_not_portugal']] #.drop_duplicates()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Filtrar os casos em que a vari√°vel 'user_city_not_portugal' √© diferente de 'N/A' e a vari√°vel 'user_hometown' n√£o est√° vazia\n",
    "casos_hometown = Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A') & (Facebook_Users['user_hometown'] != '')]\n",
    "n_por_limpar = len(casos_hometown)\n",
    "\n",
    "# Aplicar a fun√ß√£o 'limpar_cidades' na vari√°vel 'user_hometown' destes casos e armazenar o resultado em uma nova vari√°vel\n",
    "casos_hometown[['user_freguesia', 'user_concelho', 'user_distrito', 'user_pais', 'user_city_not_portugal']] = casos_hometown['user_hometown'] \\\n",
    "     .apply(limpar_cidades).apply(pd.Series)\n",
    "\n",
    "# Atualizar as vari√°veis 'user_freguesia', 'user_concelho', 'user_distrito', 'user_pais' e 'user_city_not_portugal'\n",
    "# com os valores da nova vari√°vel criada no passo anterior\n",
    "Facebook_Users.loc[casos_hometown.index, ['user_freguesia', 'user_concelho', 'user_distrito', 'user_pais', 'user_city_not_portugal']] = \\\n",
    "    casos_hometown[['user_freguesia', 'user_concelho', 'user_distrito', 'user_pais', 'user_city_not_portugal']].values"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"\\033[1mN¬∫ de Casos Limpos com a 'Naturalidade' =\\033[0m\", \n",
    "      n_por_limpar - len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A') & (Facebook_Users['user_hometown'] != '')]),\n",
    "      \"(\", round((n_por_limpar - len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A') & (Facebook_Users['user_hometown'] != '')]))/len(Facebook_Users)*100,2),\"%)\")\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Casos Fora de Portugal =\\033[0m\", \n",
    "      len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A')][['user_current_city','user_hometown','user_city_not_portugal']].drop_duplicates()),\n",
    "      \"(\", round(len(Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A')][['user_current_city','user_hometown','user_city_not_portugal']].drop_duplicates())/len(Facebook_Users)*100,2),\"%)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================= Corre√ß√£o Manual destes casos =============================\n",
    "# Guardar os casos a rever num ficheiro .CSV\n",
    "Facebook_Users[(Facebook_Users['user_city_not_portugal'] != 'N/A')] \\\n",
    "    [['user_current_city','user_hometown','user_city_not_portugal']] \\\n",
    "    .drop_duplicates() \\\n",
    "    .to_csv('cidades_not_Portugal.csv', sep='|', index=False, encoding='utf-8')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "############################################ EXTRA ####################################################\n",
    "# Converter o DataFrame em markdown - S√≥ os locais\n",
    "markdown_content = Facebook_Users.drop_duplicates(subset=['user_current_city'])\n",
    "markdown_content = markdown_content[['user_current_city', 'user_freguesia','user_concelho', 'user_distrito',\n",
    "                                     'user_pais', 'user_city_not_portugal']].to_markdown()\n",
    "\n",
    "# Abrir um arquivo de texto para escrita\n",
    "with open('limpar_cidades.txt', 'w', encoding='utf-8') as file:\n",
    "    # Escrever o conte√∫do markdown no arquivo\n",
    "    file.write(markdown_content)\n",
    "    \n",
    "# Converter o DataFrame em markdown\n",
    "markdown_content = Facebook_Users.to_markdown()\n",
    "\n",
    "# Abrir um arquivo de texto para escrita\n",
    "with open('limpar_cidades_users.txt', 'w', encoding='utf-8') as file:\n",
    "    # Escrever o conte√∫do markdown no arquivo\n",
    "    file.write(markdown_content)\n",
    "    \n",
    "########################################################################################################"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar valores omissos - Substituir '' por NaN\n",
    "Users_NAs = Facebook_Users.replace({'': np.nan, 'N/A': np.nan, 'USI':np.nan})\n",
    "NAs = pd.DataFrame({\n",
    "    'n Omissos':Users_NAs[Facebook_Users.columns].isna().sum(),\n",
    "    '% Omissos': round(((Users_NAs[Facebook_Users.columns].isna().sum() / len(Users_NAs) * 100)),2)})\n",
    "\n",
    "# Print dos resultados\n",
    "NAs # .iloc[2:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a1310d4541a5423e8cd00f23674bb9b8",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "#### üü™üü¶`G√©nero`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Carregar os arquivos CSV dos nomes poss√≠veis com o g√©nero associado em Portugal\n",
    "# Fonte: https://irn.justica.gov.pt/Servicos/Cidadao/Nascimento/Composicao-do-nome\n",
    "nomes_masculinos = pd.read_csv('Datasets/Lista-Nomes-Pr√≥prios_Masculino.csv',sep=\";\", encoding='latin1')\n",
    "nomes_femininos = pd.read_csv('Datasets/Lista-Nomes-Pr√≥prios_Feminino.csv',sep=\";\",encoding='latin1')\n",
    "\n",
    "print('\\033[1mN¬∫ de Nomes Pr√≥prios Masculinos:\\033[0m', len(nomes_masculinos))\n",
    "print('\\033[1mN¬∫ de Nomes Pr√≥prios Femininos:\\033[0m', len(nomes_femininos))\n",
    "\n",
    "pd.concat([nomes_masculinos,nomes_femininos]).sample(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar dicion√°rios para mapear nomes para g√©neros\n",
    "dict_nomes_masculinos = {unidecode(nome.lower()): 'Masculino' for nome in nomes_masculinos['NOME']}\n",
    "dict_nomes_femininos = {unidecode(nome.lower()): 'Feminino' for nome in nomes_femininos['NOME']}"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fun√ß√£o para determinar o g√™nero a partir do primeiro nome\n",
    "def determinar_genero(nome_completo):\n",
    "    \n",
    "    # Extrair o primeiro nome e converter para min√∫sculas\n",
    "    primeiro_nome = nome_completo.split()[0].lower()\n",
    "    \n",
    "    # Remover acentos do primeiro nome\n",
    "    primeiro_nome_sem_acentos = unidecode(primeiro_nome)\n",
    "    \n",
    "    if primeiro_nome_sem_acentos.strip() in dict_nomes_masculinos:\n",
    "        return 'Masculino'\n",
    "    elif primeiro_nome_sem_acentos.strip() in dict_nomes_femininos:\n",
    "        return 'Feminino'\n",
    "    else:\n",
    "        return 'Indeterminado'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Teste do M√©todo de Atribui√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# DIGI_News_Users['genero'].unique()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Substituir valores n√£o aceit√°veis por NaN, sendo a lista de valores aceit√°veis para g√™nero ['Feminino', 'Masculino']\n",
    "DIGI_News_Users.loc[~DIGI_News_Users['genero'].isin(['Feminino', 'Masculino']), 'genero'] = pd.NA\n",
    "print('\\033[1mN¬∫ de Observa√ß√µes sem g√©nero:\\033[0m', len(DIGI_News_Users) - len(DIGI_News_Users.dropna(subset=['genero'])), \n",
    "      'de', len(DIGI_News_Users),\n",
    "      '(',round((len(DIGI_News_Users) - len(DIGI_News_Users.dropna(subset=['genero'])))/len(DIGI_News_Users) *100,2),'%)')\n",
    "\n",
    "# DIGI_News_Users.dropna(subset=['genero'], inplace=True)\n",
    "# DIGI_News_Users['genero'].info()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Aplica√ß√£o da fun√ß√£o √† vari√°vel 'user_name'\n",
    "DIGI_News_Users['genero_previsto'] = DIGI_News_Users['user_name'].apply(determinar_genero)\n",
    "DIGI_News_Users[['user_name','genero', 'genero_previsto']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar casos 'Indeterminados'\n",
    "print(\"\\033[1mN¬∫ de G√©neros Indermindados:\\033[0m\", len(DIGI_News_Users[DIGI_News_Users['genero_previsto'] == 'Indeterminado']))\n",
    "DIGI_News_Users[DIGI_News_Users['genero_previsto'] == 'Indeterminado'][['user_name', 'genero', 'genero_previsto']].head(10) # Usernames n√£o s√£o nomes "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ceee0ee95ce34ed7aab7eabb0150ce7d",
    "deepnote_cell_type": "code"
   },
   "source": [
    "# Filtrar os casos onde o g√™nero real n√£o √© NA\n",
    "df_genero_valido = DIGI_News_Users.dropna(subset=['genero'])\n",
    "\n",
    "# Contar os casos onde o g√™nero previsto √© igual ao g√™nero real\n",
    "casos_corretos = df_genero_valido[df_genero_valido['genero'] == df_genero_valido['genero_previsto']]\n",
    "print(f'% de classifica√ß√£o correta √©: {len(casos_corretos)} em {len(df_genero_valido)} -> \\033[1m{(len(casos_corretos) / len(df_genero_valido)) * 100:.2f}%\\033[0m')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Aplica√ß√£o do M√©todo ao Dataset Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%time\n",
    "# Aplicar a fun√ß√£o aos nomes dos utilizadores\n",
    "Facebook_Users['user_predicted_genre'] = Facebook_Users['user_name'].apply(determinar_genero)\n",
    "Facebook_Comments['user_predicted_genre'] = Facebook_Comments['user_name'].apply(determinar_genero)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Contagem dos casos finais de g√©neros atribuidos\n",
    "pd.DataFrame({'n':Facebook_Users[['user_predicted_genre']].value_counts(),\n",
    "              '%':round(((Facebook_Users[['user_predicted_genre']].value_counts()) / len(Facebook_Users) * 100),1)})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Guardar a Compila√ß√£o de Users pr√©-processados em formato .txt\n",
    "Facebook_Users.to_csv('Datasets_Vodafone/Facebook_Users.txt', sep='\\t', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\033[1m====================== N¬∫ de Valores Omissos [USERS] ======================\\033[0m')\n",
    "# check_missing_values(Facebook_Users, 'Vodafone | MEO | NOS | DIGI [News]')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "eaafe18a02db4258afd11c16a0b43315",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä AED | *Users* - `Local de Resid√™ncia` & `G√©nero`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import do dataset 'Facebook_Users' limpo \n",
    "df_users = pd.read_csv('Datasets_Vodafone/Facebook_Users.txt', sep='\\t', encoding='utf-8')\n",
    "df_users.sample(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selecionar as linhas no df_users que correspondam aos utilizadores que comentaram entre 2019 -2024\n",
    "# com base na coluna 'user_link'\n",
    "df_users = df_users[df_users['user_link'].isin(Facebook_Posts_Comments_2019_24['user_link'])]\n",
    "len(df_users)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `G√©nero dos Users`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tabela de Frequ√™ncia do 'user_predicted_genre'\n",
    "pd.DataFrame({'n':Facebook_Users[['user_predicted_genre']].value_counts(),\n",
    "              '%':round(((Facebook_Users[['user_predicted_genre']].value_counts()) / len(Facebook_Users) * 100),1)})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Agrupar os dados por user link e calcular o n√∫mero total de coment√°rios para cada utilizador\n",
    "users_comments_count = Facebook_Comments.groupby('user_link').size().reset_index(name='N¬∫ de Coment√°rios')\n",
    "\n",
    "# Combinar os dados do n√∫mero de coment√°rios com os dados dos g√©neros\n",
    "merged_data = users_comments_count.merge(Facebook_Comments[['user_link', 'user_predicted_genre']], on='user_link', how='inner')\n",
    "\n",
    "# Calcular a m√©dia e mediana para cada g√©nero\n",
    "mean_female = merged_data.loc[merged_data['user_predicted_genre'] == 'Feminino', 'N¬∫ de Coment√°rios'].mean()\n",
    "mean_male = merged_data.loc[merged_data['user_predicted_genre'] == 'Masculino', 'N¬∫ de Coment√°rios'].mean()\n",
    "\n",
    "median_female = merged_data.loc[merged_data['user_predicted_genre'] == 'Feminino', 'N¬∫ de Coment√°rios'].median()\n",
    "median_male = merged_data.loc[merged_data['user_predicted_genre'] == 'Masculino', 'N¬∫ de Coment√°rios'].median()\n",
    "\n",
    "# Para visualizar, resgringir at√© 'N¬∫ de Coment√°rios' n√£o superiores a 16\n",
    "merged_data = merged_data[(merged_data['user_predicted_genre'] != 'Indeterminado') &\n",
    "                          (merged_data['N¬∫ de Coment√°rios'] <= 15)]\n",
    "\n",
    "# Criar o violino\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.violinplot(x='user_predicted_genre', y='N¬∫ de Coment√°rios', data=merged_data, palette=[\"#ca1f7b\", \"#0038a8\"])\n",
    "\n",
    "# Adicionar as linhas horizontais para representar a mediana de cada g√©nero\n",
    "plt.axhline(y=median_female, xmax=0.25,\n",
    "            color='#ca1f7b', linestyle='--', linewidth=1,\n",
    "            label=f'$\\\\mathbf {{\\overline{{X}}}} = {round(mean_female,1)}$ |  $\\\\mathbf {{Me =}} {round(median_female)}$')\n",
    "plt.axhline(y=median_male, xmax=0.75,\n",
    "            color='#0038a8', linestyle='--', linewidth=1,\n",
    "            label=f'$\\\\mathbf {{\\overline{{X}}}} = {round(mean_male,1)}$ |  $\\\\mathbf {{Me =}} {round(median_male)}$')\n",
    "\n",
    "plt.xlabel('G√©nero', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('N√∫mero de Coment√°rios\\n', fontweight='bold', fontsize=14)\n",
    "plt.title('Distribui√ß√£o do N√∫mero de Coment√°rios por G√©nero\\n', fontsize=18, fontweight='bold')\n",
    "# plt.ylim(0, 18)\n",
    "legend_properties = {'weight':'bold', 'size':'14'}\n",
    "plt.legend(title='Mediana', title_fontproperties=legend_properties, fontsize='10',\n",
    "           loc='upper center',frameon=True)\n",
    "\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"Relat√≥rio/Gr√°ficos/9_G√©neroUsers.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "sd_female = merged_data.loc[merged_data['user_predicted_genre'] == 'Feminino', 'N¬∫ de Coment√°rios'].std()\n",
    "sd_male = merged_data.loc[merged_data['user_predicted_genre'] == 'Masculino', 'N¬∫ de Coment√°rios'].std()\n",
    "\n",
    "print(\"\\033[1mDesvio padr√£o dos coment√°rios feitos por usu√°rios do g√©nero feminino:\\033[0m \",  round(sd_female,2), \n",
    "      \"\\n\\033[1mDesvio padr√£o dos coment√°rios feitos por usu√°rios do g√™nero masculino:\\033[0m\", round(sd_male,2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Local de Resid√™ncia`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# pip install geopandas\n",
    "import geopandas as gpd\n",
    "\n",
    "# Carregar as coordenadas de Portugal no Geopandas - https://public.opendatasoft.com/explore/dataset/georef-portugal-freguesia\n",
    "map_dataframe = gpd.read_file('/workspaces/PFACD_Dashboard/Datasets_Vodafone/Auxiliares/georef-portugal-freguesia-simplified.geojson')\n",
    "# map_dataframe"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóæ Mapas | Relat√≥rio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "############################################ FREGUESIAS ############################################\n",
    "# Agrupar os dados por freguesia, concelho e distrito e contar o n√∫mero de usu√°rios em cada grupo\n",
    "users_freguesia_concelho_distrito = df_users.groupby(['user_freguesia', 'user_concelho', 'user_distrito']).size().reset_index(name='Freguesia')\n",
    "\n",
    "# Renomear as colunas para corresponderem √†s colunas do map_dataframe\n",
    "users_freguesia_concelho_distrito = users_freguesia_concelho_distrito.rename(columns={'user_freguesia': 'fre_name', 'user_concelho': 'con_name', 'user_distrito': 'dis_name'})\n",
    "\n",
    "# Fazer o merge com base nas tr√™s colunas (freguesia, concelho e distrito)\n",
    "merged_dataframe_F = map_dataframe.merge(users_freguesia_concelho_distrito, on=['fre_name', 'con_name', 'dis_name'], how='outer')\n",
    "\n",
    "# Preencher os valores NaN com 0\n",
    "merged_dataframe_F['Freguesia'].fillna(0, inplace=True)\n",
    "merged_dataframe_F.dropna(inplace=True)\n",
    "\n",
    "# Mapa\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))  \n",
    "merged_dataframe_F.plot(column=\"Freguesia\",cmap=\"Oranges\", linewidth=0.2, ax=ax, edgecolor=\"black\", legend=True, \n",
    "                   legend_kwds={'label': \"\\n Freguesia\\n\", 'orientation': \"vertical\"})\n",
    "ax.set_title(\"\\nMapa das Freguesias dos Users \\n\", fontweight='bold', fontsize=16)\n",
    "# # Limite Continente\n",
    "# ax.set_xlim([-10, -5])\n",
    "# ax.set_ylim([37,42.5])\n",
    "\n",
    "# Limite Madeira\n",
    "# ax.set_xlim([-17.50, -16])\n",
    "# ax.set_ylim([32.25,33.25])\n",
    "\n",
    "# Limite A√ßores\n",
    "# ax.set_xlim([-32, -25])\n",
    "# ax.set_ylim([36.8,39.8])\n",
    "\n",
    "ax.set_axis_off()\n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "############################################ CONCELHOS ############################################\n",
    "# Agrupar os usu√°rios por concelho e distrito\n",
    "users_concelho_distrito = df_users.groupby(['user_concelho', 'user_distrito']).size().reset_index(name='Concelho')\n",
    "\n",
    "# Fazer o merge com o dataframe map_dataframe usando as colunas 'con_name' e 'dis_name'\n",
    "merged_dataframe_C = map_dataframe.merge(users_concelho_distrito, left_on=['con_name', 'dis_name'], \n",
    "                                         right_on=['user_concelho', 'user_distrito'], how='outer')\n",
    "\n",
    "# Preencher os valores NaN com 0\n",
    "merged_dataframe_C['Concelho'].fillna(0, inplace=True)\n",
    "merged_dataframe_C.dropna(inplace=True, subset=['con_name'])\n",
    "\n",
    "# Mapa\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))  \n",
    "merged_dataframe_C.dissolve(by = ['dis_name','con_name'])\\\n",
    "                  .plot(column=\"Concelho\",cmap=\"Reds\", linewidth=0.2, ax=ax, edgecolor=\"black\", legend=True, \n",
    "                   legend_kwds={'label': \"\\n Concelho\\n\", 'orientation': \"vertical\"})\n",
    "ax.set_title(\"\\nMapa dos Concelhos dos Users \\n\", fontweight='bold', fontsize=16)\n",
    "# Limite Continente\n",
    "# ax.set_xlim([-10, -5])\n",
    "# ax.set_ylim([37,42.5])\n",
    "\n",
    "# Limite Madeira\n",
    "# ax.set_xlim([-17.50, -16])\n",
    "# ax.set_ylim([32.25,33.25])\n",
    "\n",
    "# Limite A√ßores\n",
    "# ax.set_xlim([-32, -25])\n",
    "# ax.set_ylim([36.8,39.8])\n",
    "\n",
    "ax.set_axis_off()\n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "############################################ DISTRITOS ############################################\n",
    "# Agrupar os users por distritos\n",
    "users_distrito = df_users.groupby('user_distrito').size().reset_index(name='Distrito')\n",
    "merged_dataframe_D = map_dataframe.merge(users_distrito, left_on=\"dis_name\", right_on=\"user_distrito\", how='outer')\n",
    "# Preencher os valores NaN com 0\n",
    "merged_dataframe_D['Distrito'].fillna(0, inplace=True)\n",
    "merged_dataframe_D.dropna(inplace=True, subset=['dis_name'])\n",
    "\n",
    "# Mapa\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))  \n",
    "merged_dataframe_D.dissolve(by = 'dis_name')\\\n",
    "                  .plot(column=\"Distrito\",cmap=\"Purples\", linewidth=0.15, ax=ax, edgecolor=\"black\", legend=True, \n",
    "                   legend_kwds={'label': \"\\n Distrito\\n\", 'orientation': \"vertical\"})\n",
    "ax.set_title(\"\\nMapa dos Distritos dos Users \\n\", fontweight='bold', fontsize=16)\n",
    "# Limite Continente\n",
    "# ax.set_xlim([-10, -5])\n",
    "# ax.set_ylim([37,42.5])\n",
    "\n",
    "# Limite Madeira\n",
    "# ax.set_xlim([-17.50, -16])\n",
    "# ax.set_ylim([32.25,33.25])\n",
    "\n",
    "# Limite A√ßores\n",
    "# ax.set_xlim([-32, -25])\n",
    "# ax.set_ylim([36.8,39.8])\n",
    "\n",
    "ax.set_axis_off()\n",
    "sns.despine(top=True, right=True, left=True, bottom=True)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üó∫Ô∏è Mapa Interativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó∫Ô∏è Fun√ß√µes Auxiliares na Representa√ß√£o dos Mapas **`Folium`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import HeatMapWithTime, HeatMap\n",
    "from folium.map import Layer\n",
    "from folium import FeatureGroup\n",
    "import jinja2\n",
    "from jinja2 import Template\n",
    "import branca\n",
    "import branca.colormap as cm\n",
    "from branca.colormap import linear\n",
    "from branca.element import Element, Figure, JavascriptLink, MacroElement\n",
    "\n",
    "# Classe para ver o Google Street View | Fonte: https://astro-geo-gis.com/open-street-view-with-python-folium-map/\n",
    "class ClickForOneMarker(folium.ClickForMarker):\n",
    "    \"\"\"\n",
    "    Description of the tool\n",
    "    \"\"\"\n",
    "    _template = Template(u\"\"\"\n",
    "        {% macro script(this, kwargs) %}\n",
    "        const fontAwesomeIcon= L.divIcon({\n",
    "            html: '<i class=\"fa-solid fa-3x fa-street-view\" style=\"color:#F69423\"></i>',\n",
    "            iconSize: [0,0],\n",
    "            iconAnchor: [15,0]\n",
    "            });\n",
    "        var new_mark = L.marker();\n",
    "        function newMarker(e){\n",
    "        new_mark.setLatLng(e.latlng).addTo({{this._parent.get_name()}});\n",
    "        new_mark.setIcon(fontAwesomeIcon);\n",
    "        new_mark.dragging.enable();\n",
    "        new_mark.on('dblclick', function(e){ {{this._parent.get_name()}}.removeLayer(e.target)})\n",
    "        var lat = e.latlng.lat.toFixed(4),\n",
    "            lng = e.latlng.lng.toFixed(4);\n",
    "        new_mark.bindPopup(\"<b>Latitude : </b>\" + lat + \"<br><b>Longitude : </b> \" + lng +\"<br> <a href=https://www.google.com/maps?layer=c&cbll=\" + lat + \",\" + lng + \" target=blank> <img src='https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Google_Street_View_icon.svg/768px-Google_Street_View_icon.svg.png' style='width:40px;margin:0 auto;display: block;margin-left: auto; margin-right: auto; margin-top:5px'></img></a>\").openPopup();\n",
    "        };\n",
    "        {{this._parent.get_name()}}.on('click', newMarker);\n",
    "        {% endmacro %}\n",
    "    \"\"\")  # noqa\n",
    "\n",
    "    def __init__(self, popup=None):\n",
    "        super(ClickForOneMarker, self).__init__(popup)\n",
    "        self._name = 'ClickForOneMarker'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# # Fun√ß√£o Auxiliar - Gerar conte√∫do do pop-up\n",
    "# def generate_freguesia_popup(row):\n",
    "#     popup = f'<b>Freguesia:</b> {row[\"fre_name\"]} <br>\\\n",
    "#               <b>Concelho:</b> {row[\"con_name\"]} <br> \\\n",
    "#               <b>Distrito:</b> {row[\"dis_name\"]} <br> \\\n",
    "#               <b>N¬∫ de Utilizadores:</b> {row[\"Freguesia\"]} <br> \\\n",
    "#               <b>% de Utilizadores:</b> {row[\"Freguesia\"]/int(row[\"Freguesia\"].sum())} [exclui <b>USI</b>]<br> \\\n",
    "#               <a href=https://www.google.com/maps/place/{row[\"fre_name\"]},{row[\"con_name\"]}target=blank><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Google_Street_View_icon.svg/768px-Google_Street_View_icon.svg.png\" style=\"width:40px;margin:0 auto;display: block;margin-left: auto; margin-right: auto; margin-top:5px\"></img></a>'\n",
    "#     return popup_freguesia"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Google Earth Engine Python API and Folium Interactive Mapping\n",
    "# Fonte: https://colab.research.google.com/github/giswqs/qgis-earthengine-examples/blob/master/Folium/ee-api-folium-setup.ipynb#scrollTo=802tXDddaTC5\n",
    "basemaps = {'Google Maps': folium.TileLayer(\n",
    "        tiles = 'https://mt1.google.com/vt/lyrs=m&x={x}&y={y}&z={z}',\n",
    "        attr = 'Google',\n",
    "        name = 'Google Maps',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "    ),\n",
    "    'Google Satellite': folium.TileLayer(\n",
    "        tiles = 'https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',\n",
    "        attr = 'Google',\n",
    "        name = 'Google Satellite',\n",
    "        overlay = False,\n",
    "        control = True\n",
    "    )}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Centrar o mapa na m√©dia das coordenadas\n",
    "latitude_central = map_dataframe.geometry.centroid.y.mean()\n",
    "longitude_central = map_dataframe.geometry.centroid.x.mean() - 8\n",
    "\n",
    "# Criar um mapa com folium\n",
    "mapa = folium.Map(location=[latitude_central, longitude_central], zoom_start=5, control_scale=True)\n",
    "basemaps['Google Maps'].add_to(mapa)                  #                                      -------------------------------\n",
    "basemaps['Google Satellite'].add_to(mapa)             #                                      -------------------------------\n",
    "plugins.Fullscreen(position='topleft').add_to(mapa)   # Op√ß√£o de Fullscreen                  -------------------------------\n",
    "mapa.add_child(plugins.MiniMap(toggle_display=True))  # Minimapa do lado direito             -------------------------------\n",
    "# plugins.Draw().add_to(mapa)                         # Op√ß√µes de Desenhar\n",
    "plugins.FloatImage(image = 'https://upload.wikimedia.org/wikipedia/en/archive/c/cc/20180621135549%21Vodafone_2017_logo.svg', \n",
    "                   bottom=8, left=2, width='40px').add_to(mapa)        # Logo Vodafone\n",
    "# mapa.add_child(ClickForOneMarker())\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "#================================================= FREGUESIAS ==============================================================\n",
    "\n",
    "# Adicionar as freguesias ao mapa como pol√≠gonos\n",
    "colormap1 = linear.OrRd_09.scale(merged_dataframe_F.Freguesia.min(), merged_dataframe_F.Freguesia.max())\n",
    "colormap1.caption = \"Users por Freguesias\"\n",
    "colormap1.width = 300\n",
    "\n",
    "merged_dataframe_F[\"Users_%\"] = round((merged_dataframe_F[\"Freguesia\"] / merged_dataframe_F[\"Freguesia\"].sum()) * 100,2)\n",
    "popup = folium.GeoJsonPopup(\n",
    "    fields=[\"fre_name\", \"con_name\", \"dis_name\", \"Freguesia\", \"Users_%\"],\n",
    "    aliases=[\"Freguesia\", \"Concelho\", \"Distrito\", \"N¬∫ de Users (n)\", \"% de Users (%)\"],\n",
    "    localize=True,\n",
    "    labels=True,\n",
    "    style=\"color:black; font-family: arial; font-size: 12px;\",\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    merged_dataframe_F,\n",
    "    name='Por Freguesias',\n",
    "    style_function= lambda feature: {\n",
    "        'fillColor': colormap1(int(feature['properties']['Freguesia'])),\n",
    "        'color': 'black',\n",
    "        'weight': 0.8,\n",
    "        \"dashArray\": \"5, 5\",\n",
    "        'fillOpacity': 0.9\n",
    "    },\n",
    "    popup=popup\n",
    ").add_to(mapa)\n",
    "\n",
    "# Adicione o colormap √† layer 'Por Freguesias'\n",
    "# colormap.add_to(mapa, name=\"Por Freguesias\")\n",
    "mapa.add_child(colormap1, name=\"Por Freguesias\") ############################# PROBLEMA\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "#================================================= CONCELHOS ==============================================================\n",
    "colormap2 = linear.Purples_09.scale(merged_dataframe_C.Concelho.min(), merged_dataframe_C.Concelho.max())\n",
    "colormap2.caption = \"Users por Concelho\"\n",
    "colormap2.width = 300\n",
    "\n",
    "merged_dataframe_C[\"Users_%\"] = round((merged_dataframe_C[\"Concelho\"] / \n",
    "                                       merged_dataframe_C.groupby('user_concelho').agg({'Concelho': 'first'})['Concelho'].sum())* 100,2)\n",
    "\n",
    "# Preencher os valores NaN nas colunas 'user_concelho' e 'user_distrito' com os valores correspondentes nas colunas 'con_name' e 'dis_name', respectivamente\n",
    "merged_dataframe_C['user_concelho'] = merged_dataframe_C['user_concelho'].fillna(merged_dataframe_C['con_name'])\n",
    "merged_dataframe_C['user_distrito'] = merged_dataframe_C['user_distrito'].fillna(merged_dataframe_C['dis_name'])\n",
    "\n",
    "popup = folium.GeoJsonPopup(\n",
    "    fields=[\"user_concelho\", \"user_distrito\", \"Concelho\", \"Users_%\"],\n",
    "    aliases=[\"Concelho\", \"Distrito\", \"N¬∫ de Users (n)\", \"% de Users (%)\"],\n",
    "    localize=True,\n",
    "    labels=True,\n",
    "    style=\"color:black; font-family: arial; font-size: 12px;\",\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    merged_dataframe_C.dissolve(by = ['dis_name','con_name']),\n",
    "    name='Por Concelho',\n",
    "    style_function= lambda feature: {\n",
    "        'fillColor': colormap2(feature['properties']['Concelho']),\n",
    "        'color': 'black',\n",
    "        'weight': 0.8,\n",
    "        \"dashArray\": \"5, 5\",\n",
    "        'fillOpacity': 0.9\n",
    "    },\n",
    "    popup=popup\n",
    ").add_to(mapa)\n",
    "\n",
    "mapa.add_child(colormap2, name=\"Por Concelho\") ############################# PROBLEMA\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "#================================================= DISTRITOS ==============================================================\n",
    "colormap3 = linear.Blues_09.scale(merged_dataframe_D.Distrito.min(), merged_dataframe_D.Distrito.max())\n",
    "colormap3.caption = \"Users por Distrito\"\n",
    "colormap3.width = 300\n",
    "\n",
    "merged_dataframe_D[\"Users_%\"] = round((merged_dataframe_D[\"Distrito\"] /\n",
    "                                       merged_dataframe_D.groupby('user_distrito').agg({'Distrito': 'first'})['Distrito'].sum()) * 100,2)\n",
    "popup = folium.GeoJsonPopup(\n",
    "    fields=[\"dis_name\", \"Distrito\", \"Users_%\"],\n",
    "    aliases=[\"Distrito\", \"N¬∫ de Users (n)\", \"% de Users (%)\"],\n",
    "    localize=True,\n",
    "    labels=True,\n",
    "    style=\"color:black; font-family: arial; font-size: 12px;\",\n",
    ")\n",
    "\n",
    "folium.GeoJson(\n",
    "    merged_dataframe_D.dissolve(by = 'user_distrito'),\n",
    "    name='Por Distrito',\n",
    "    style_function= lambda feature: {\n",
    "        'fillColor': colormap3(feature['properties']['Distrito']),\n",
    "        'color': 'black',\n",
    "        'weight': 0.8,\n",
    "        \"dashArray\": \"5, 5\",\n",
    "        'fillOpacity': 0.9\n",
    "    },\n",
    "    popup=popup\n",
    ").add_to(mapa, )\n",
    "\n",
    "mapa.add_child(colormap3, name=\"Por Distrito\") ############################# PROBLEMA\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------------------------\n",
    "# Mostrar o mapa\n",
    "mapa.add_child(folium.LayerControl(collapsed=False)) # Camadas de Diferentes Mapas\n",
    "mapa"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar um mapa com folium\n",
    "mapa = folium.Map(location=[latitude_central, longitude_central], zoom_start=5, control_scale=True)\n",
    "basemaps['Google Maps'].add_to(mapa)\n",
    "basemaps['Google Satellite'].add_to(mapa)\n",
    "plugins.Fullscreen(position='topleft').add_to(mapa)\n",
    "mapa.add_child(plugins.MiniMap(toggle_display=True))\n",
    "plugins.FloatImage(image = 'https://upload.wikimedia.org/wikipedia/en/archive/c/cc/20180621135549%21Vodafone_2017_logo.svg', bottom=8, left=2, width='40px').add_to(mapa)\n",
    "\n",
    "# Criar um FeatureGroup para cada GeoJSON map\n",
    "fg_freguesias = folium.FeatureGroup(name='Por Freguesias')\n",
    "fg_concelhos = folium.FeatureGroup(name='Por Concelho')\n",
    "fg_distritos = folium.FeatureGroup(name='Por Distrito')\n",
    "\n",
    "# Adicionar os GeoJSON maps aos respectivos FeatureGroups\n",
    "fg_freguesias.add_child(folium.GeoJson(merged_dataframe_F, style_function=lambda x: {'fillColor': colormap1(int(x['properties']['Freguesia']))}, popup=popup))\n",
    "fg_concelhos.add_child(folium.GeoJson(merged_dataframe_C.dissolve(by=['dis_name','con_name']), style_function=lambda x: {'fillColor': colormap2(x['properties']['Concelho'])}, popup=popup))\n",
    "fg_distritos.add_child(folium.GeoJson(merged_dataframe_D.dissolve(by='user_distrito'), style_function=lambda x: {'fillColor': colormap3(x['properties']['Distrito'])}, popup=popup))\n",
    "\n",
    "# Adicionar os FeatureGroups ao mapa\n",
    "fg_freguesias.add_to(mapa)\n",
    "fg_concelhos.add_to(mapa)\n",
    "fg_distritos.add_to(mapa)\n",
    "\n",
    "# Adicionar o LayerControl para controlar a visibilidade dos FeatureGroups\n",
    "folium.LayerControl().add_to(mapa)\n",
    "\n",
    "# Mostrar o mapa\n",
    "mapa"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise de `Outliers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Facebook_Posts_Comments.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fonte: Notebooks de PACDI\n",
    "from collections import Counter\n",
    "\n",
    "### Define function for outlier detection, based on IQR method\n",
    "def detect_outliers(df,feature):\n",
    "    outlier_indices = []\n",
    "    # 1st quartile\n",
    "    Q1 = np.percentile(df[feature],25)\n",
    "    # 3rd quartile\n",
    "    Q3 = np.percentile(df[feature],75)\n",
    "    # IQR \n",
    "    IQR = max(Q3 - Q1, 1)\n",
    "    # Outlier step\n",
    "    outlier_step = IQR * 1.5\n",
    "    # detect outlier and their indeces\n",
    "    outlier_list_col = df[((df[feature] < Q1 - outlier_step) | (df[feature] > Q3 + outlier_step))].index\n",
    "    # store indeces\n",
    "    outlier_indices.extend(outlier_list_col)\n",
    "    return outlier_indices"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar os valores 'outliers'\n",
    "outlier_indices_post_reactions = detect_outliers(Facebook_Posts_Comments_2019_24, 'post_reactions')\n",
    "outlier_indices_post_comments = detect_outliers(Facebook_Posts_Comments_2019_24, 'post_comments')\n",
    "outlier_indices_post_shares = detect_outliers(Facebook_Posts_Comments_2019_24, 'post_shares')\n",
    "\n",
    "outlier_indices_comment_reactions = detect_outliers(Facebook_Posts_Comments_2019_24, 'comment_reactions')\n",
    "outlier_indices_comment_responses = detect_outliers(Facebook_Posts_Comments_2019_24, 'comment_num_responses')\n",
    "\n",
    "print(\"Outliers em post_reactions:\", outlier_indices_post_reactions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Outliers em post_comments:\", outlier_indices_post_comments)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Outliers em post_shares:\", outlier_indices_post_shares)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Outliers em comment_reactions:\", outlier_indices_comment_reactions)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print(\"Outliers em comment_num_responses:\", outlier_indices_comment_responses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "4002a6fe0a5c4fbea9df644bb1b2cd15",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
