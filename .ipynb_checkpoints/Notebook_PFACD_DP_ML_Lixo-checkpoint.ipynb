{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "b1eb2e34d76b499f87299d156c5a4dbe",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<center><br> <img src=\"https://www.iscte-iul.pt/assets/images/logo_iscte_detailed.svg\" style=\"width: 300px;\"></center><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "db9cd1552ab8498e88b44eb0bbc583bc",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/archive/c/cc/20180621135549%21Vodafone_2017_logo.svg\" style=\"width: 200px;margin-top:90px;margin-bottom:100px;\" align=\"left\">\n",
    "   <div style= \"font-size: 28px;font-weight:bold;margin-top:20px;margin-bottom:20px;margin-left:360px;margin-right:150px; line-height: 1.1;color:#F91701;\"><center>An√°lise de Sentimentos nas Redes Sociais | Vodafone</center></div>\n",
    "   <div style= \"font-size: 17px;font-weight:bold;\"><center>Projeto Final Aplicado em Ci√™ncia de Dados</center></div>\n",
    "   <div><center><b>Orientador:</b> Nuno Santos (ISCTE-IUL) </center></div>\n",
    "   <div><center><b>Cliente:</b> Carlos Santos (Vodafone) </center></div>\n",
    " <br>\n",
    "    <div><center>Andr√© Silvestre N¬∫104532 </center></div>\n",
    "    <div><center>Eliane Gabriel N¬∫103303 <b>|</b> Maria Jo√£o Louren√ßo N¬∫104716 </center></div>\n",
    "    <div><center>Margarida Pereira N¬∫105877 <b>|</b> Umeima Mahomed N¬∫99239</center></div>\n",
    "    <br>\n",
    "    <div><center><b>CDC1</b> & <b>CDC2</b></center></div>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9f7b4f35253e417d916d43ca60d0ad84",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: .7px; color: white; border-radius: 300px; text-align: center;\">\n",
    "</div>\n",
    "\n",
    "# √çndice\n",
    "\n",
    "\n",
    "<ol style=\"list-style-type: upper-roman;font-weight: bold;\">\n",
    "  <li>\n",
    "    <span style=\"font-weight: normal;\">\n",
    "      <a href=\"\">Business Understanding</a>\n",
    "    </span> \n",
    "  </li> \n",
    "    \n",
    "   <li>\n",
    "    <span style=\"font-weight: normal;\">\n",
    "      <a href=\"\">Data Understanding</a>\n",
    "    </span> \n",
    "  </li> \n",
    "    \n",
    "  <li>\n",
    "    <span style=\"font-weight: normal;\">\n",
    "      <a href=\"#2\">Data Preparation</a>\n",
    "    </span> \n",
    "    <ol style=\"list-style-type:decimal;\">\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#2.4\">Pr√©-Processamento de Texto</a></span></li>\n",
    "    </ol>\n",
    "  </li>\n",
    "  <li>\n",
    "    <span style = \"font-weight: normal;\">\n",
    "        <a href=\"#3\">Modeling</a>\n",
    "    </span>\n",
    "    <ol style=\"list-style-type:decimal;\">\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#4.1\">Sentiment Analysis</a></span></li>\n",
    "      <li><span style=\"font-weight: normal;\"><a href=\"#4.2\">Topic Analysis</a></span></li>\n",
    "    </ol>\n",
    "   </li>\n",
    "  <li><span style = \"font-weight: normal;\"><a href=\"#5\">Evaluation</a></span></li>\n",
    "  <li><span style = \"font-weight: normal;\"><a href=\"#6\">Deployment [Dashboard]</a></span></li>\n",
    "</ol>\n",
    "\n",
    "<br>\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: .7px; color: white; border-radius: 300px; text-align: center;\">\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "4c1009bc8ff04db2bb664cd89ad62120",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 34,
    "execution_start": 1710005438477,
    "source_hash": null
   },
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style>ol > li::marker {font-weight: bold;font-size: 15px;}\n",
    "    .output_png {display: table-cell;text-align: center;vertical-align: middle;}</style>\"\"\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "# Fonte: https://python.plainenglish.io/displaying-multiple-dataframes-side-by-side-in-jupyter-lab-notebook-9a4649a4940\n",
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def display_side_by_side(*args, super_title, titles=cycle([''])):\n",
    "    html_str = ''\n",
    "    html_str += f'<h1 style=\"text-align: center; margin-bottom: -50px;\">{super_title}</h1><br>'\n",
    "    html_str += '<div style=\"display: flex; justify-content: center; align-items: center;\">'\n",
    "    for df, title in zip(args, chain(titles, cycle(['</br>']))):\n",
    "        html_str += f'<div style=\"margin-right: 20px;\"><h3 style=\"text-align: center;color:#555555; margin-bottom: 10px\">{title}</h3>'\n",
    "        html_str += df.to_html().replace('table', 'table style=\"display:inline; margin-right: 20px;\"')\n",
    "        html_str += '</div>'\n",
    "    html_str += '</div>'\n",
    "    display_html(html_str, raw=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "5d00703c76594fb98260e8c5af0db20d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## üìö Import das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "b5c35872252f49f7b15a2a1c7262a128",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1060,
    "execution_start": 1710005440203,
    "source_hash": null
   },
   "source": [
    "# Bibliotecas que vamos usar no Projeto\n",
    "import os                # Operating System\n",
    "from tqdm import tqdm    # Barra de Progresso\n",
    "\n",
    "# Para dados\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import unidecode\n",
    "from tqdm import tqdm                        # Barra de Progresso\n",
    "pd.set_option('display.max_colwidth', None)  # Visualizar a informa√ß√£o toda\n",
    "pd.set_option('display.max_columns', None)\n",
    "# pd.reset_option('display.max_colwidth') \n",
    "\n",
    "# Para gr√°ficos\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "\n",
    "# -------------------- Text Mining -------------------\n",
    "import collections\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "import spacy\n",
    "import string\n",
    "pd.options.mode.chained_assignment = None\n",
    "import swifter\n",
    "\n",
    "# ---------------- Sentiment/Topic Analysis ----------------\n",
    "from tqdm.auto import tqdm     # Barra de Progressos em pipelines\n",
    "tqdm.pandas()\n",
    "\n",
    "# Estilo dos Gr√°ficos    \n",
    "plt.style.use('ggplot')\n",
    "from IPython.display import display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# Semente para reprodutibilidade\n",
    "import random\n",
    "random.seed(123)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Confirmar o environment do Anaconda\n",
    "import subprocess\n",
    "print(subprocess.check_output([\"conda\", \"info\", \"--env\"], universal_newlines=True))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cfb435619d914284ba568ae7b63c42bf",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# <a class='anchor' id='2'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>3 | Data Preparation</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Importar as Bases de Dados dos Posts, Coment√°rios e Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Facebook_Posts = pd.read_csv('Datasets_Vodafone/Facebook_Posts.txt', sep='\\t', encoding='utf-8')\n",
    "Facebook_Comments = pd.read_csv('Datasets_Vodafone/Facebook_Comments.txt', sep='\\t', encoding='utf-8')\n",
    "Facebook_Users = pd.read_csv('Datasets_Vodafone/Facebook_Users.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Garantir que as colunas 'post_id' e 'page' e 'user_link' s√£o do tipo str\n",
    "Facebook_Users['user_link'] = Facebook_Users['user_link'].astype(str)\n",
    "\n",
    "Facebook_Comments['post_id'] = Facebook_Comments['post_id'].astype(str)\n",
    "Facebook_Comments['page'] = Facebook_Comments['page'].astype(str)\n",
    "\n",
    "Facebook_Posts['post_id'] = Facebook_Posts['post_id'].astype(str)\n",
    "Facebook_Posts['page'] = Facebook_Posts['page'].astype(str)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Juntar tabelas e restringir aos anos em estudo\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Comments, Facebook_Posts, how='outer', on=['post_id', 'page'])\n",
    "\n",
    "# Filtrar o DataFrame Facebook_Posts para obter os posts a partir de 2019\n",
    "Facebook_Posts = Facebook_Posts[Facebook_Posts['post_year'] >= 2019]\n",
    "\n",
    "# Filtrar o DataFrame Facebook_Comments para obter os posts a partir de 2019\n",
    "Facebook_Comments = Facebook_Posts_Comments[Facebook_Posts_Comments['post_year'] >= 2019]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "print(\"\\033[1mN¬∫ de Publica√ß√µes entre 2019-2024:\\033[0m {:>12}\".format(len(Facebook_Posts)))\n",
    "print(\"\\033[1mN¬∫ de Coment√°rios entre 2019-2024:\\033[0m {:>14}\".format(len(Facebook_Comments)))\n",
    "print(\"\\033[1mN¬∫ de Utilizadores √önicos entre 2019-2024:\\033[0m {:>3}\".format(len(\n",
    "    Facebook_Comments[~Facebook_Comments.duplicated(subset=['user_link'])])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='2.4'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Pr√©-Processamento de Texto</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Verificar posts sem texto\n",
    "print('\\033[1mN¬∫ de Posts a eliminar:\\033[0m', len(Facebook_Posts[Facebook_Posts['post_text'].isna()]),\n",
    "      '(', round(len(Facebook_Posts[Facebook_Posts['post_text'].isna()])/len(Facebook_Posts)*100, 2),'%)')\n",
    "\n",
    "Facebook_Posts['post_text'] = Facebook_Posts['post_text'].fillna(' ')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> N√£o vamos eliminar pq se n√£o elimina tmb os coment√°rios associados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Verificar coment√°rios sem texto [com emojis/s√≥ fotos/...]\n",
    "print('\\033[1mN¬∫ de Coment√°rios a eliminar:\\033[0m', len(Facebook_Comments[Facebook_Comments['comment_text'].isna()]),\n",
    "      '(', round(len(Facebook_Comments[Facebook_Comments['comment_text'].isna()])/len(Facebook_Comments)*100, 2),'%)')\n",
    "\n",
    "# Eliminar NAs\n",
    "Facebook_Comments.dropna(subset=['comment_text'], inplace=True)\n",
    "\n",
    "# Verificar\n",
    "print('\\033[1mN¬∫ de Coment√°rios a eliminar:\\033[0m', len(Facebook_Comments[Facebook_Comments['comment_text'].isna()]),\n",
    "      '(', round(len(Facebook_Comments[Facebook_Comments['comment_text'].isna()])/len(Facebook_Comments)*100, 2),'%)')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# Verificar todos os DataFrames quanto √† presen√ßa do caractere 'ÔøΩ'\n",
    "for df_name, df in zip(['Facebook_Posts', 'Facebook_Comments'], [Facebook_Posts, Facebook_Comments]):\n",
    "    print(f\"\\033[1mVerificando o DataFrame {df_name}...\\033[0m\")\n",
    "    for column in df.columns:\n",
    "        contains_character = df[column].astype(str).str.contains('ÔøΩ', na=False).any()\n",
    "        if contains_character:\n",
    "            print(f\"Caractere 'ÔøΩ' encontrado na coluna '{column}' do DataFrame {df_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "# Verificar as linhas com caracter especial 'ÔøΩ'\n",
    "CE_Comment_Text = Facebook_Comments[Facebook_Comments['comment_text'].str.contains('ÔøΩ')]\n",
    "print('\\033[1mTotal Observa√ß√µes de Coment√°rios com Caracter Especial = \\033[0m', len(CE_Comment_Text),\n",
    "     '(',round(len(CE_Comment_Text)/len(Facebook_Comments)*100,5),'%)')\n",
    "# CE_Comment_Text - Verificou-se que tamb√©m se encontrava assim no coment√°rio original\n",
    "\n",
    "# Remover os caracteres 'ÔøΩ' substituindo-os por uma string vazia ''\n",
    "Facebook_Comments['comment_text'] = Facebook_Comments['comment_text'].str.replace('ÔøΩ', '')\n",
    "\n",
    "# Verificar se a substitui√ß√£o foi bem-sucedida\n",
    "print('\\033[1mTotal Observa√ß√µes de Coment√°rios com Caracter Especial = \\033[0m', \n",
    "      len(Facebook_Comments[Facebook_Comments['comment_text'].str.contains('ÔøΩ')]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Redefinir o n¬∫ de linhas\n",
    "Facebook_Posts = Facebook_Posts.reset_index(drop=True)\n",
    "Facebook_Comments = Facebook_Comments.reset_index(drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "# Verificar as linhas a express√£o \"‚Ä¶ Ver mais\"\n",
    "VM_Post_Text = Facebook_Posts[Facebook_Posts['post_text'].str.contains('‚Ä¶ Ver mais')]\n",
    "print('\\033[1mTotal Observa√ß√µes com \"‚Ä¶ Ver mais\" = \\033[0m', len(VM_Post_Text),\n",
    "     '(',round(len(VM_Post_Text)/len(Facebook_Posts)*100,1),'%)')\n",
    "# CE_Post_Text - Verificou-se que tamb√©m se encontrava assim no coment√°rio original"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# Verificar as linhas a express√£o \"‚Ä¶ Ver mais\"\n",
    "VM_Comment_Text = Facebook_Comments[Facebook_Comments['comment_text'].str.contains('‚Ä¶ Ver mais')]\n",
    "print('\\033[1mTotal Observa√ß√µes com \"‚Ä¶ Ver mais\" = \\033[0m', len(VM_Comment_Text),\n",
    "     '(',round(len(VM_Comment_Text)/len(Facebook_Comments)*100,1),'%)')\n",
    "# CE_Comment_Text - Verificou-se que tamb√©m se encontrava assim no coment√°rio original"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# Apagar a express√£o \"‚Ä¶ Ver mais\" ============= POSTS\n",
    "Facebook_Posts['post_text'] = Facebook_Posts['post_text'].str.replace('‚Ä¶ Ver mais', '')\n",
    "\n",
    "# Verificar se a substitui√ß√£o foi bem-sucedida\n",
    "print('\\033[1mTotal Observa√ß√µes com \"‚Ä¶ Ver mais\" = \\033[0m', \n",
    "      len(Facebook_Posts[Facebook_Posts['post_text'].str.contains('‚Ä¶ Ver mais')]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# Apagar a express√£o \"‚Ä¶ Ver mais\" ============= COMMENTS\n",
    "Facebook_Comments['comment_text'] = Facebook_Comments['comment_text'].str.replace('‚Ä¶ Ver mais', '')\n",
    "\n",
    "# Verificar se a substitui√ß√£o foi bem-sucedida\n",
    "print('\\033[1mTotal Observa√ß√µes com \"‚Ä¶ Ver mais\" = \\033[0m', \n",
    "      len(Facebook_Comments[Facebook_Comments['comment_text'].str.contains('‚Ä¶ Ver mais')]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí¨ Identifica√ß√£o da Linguagem dos Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# !pip install lingua-language-detector"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "from lingua import Language, LanguageDetectorBuilder\n",
    "languages = [Language.ENGLISH, Language.PORTUGUESE]\n",
    "detector = LanguageDetectorBuilder.from_languages(*languages).build()\n",
    "\n",
    "# Fun√ß√£o para identificar o idioma do texto\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detector.detect_language_of(text)\n",
    "    except:\n",
    "        return 'Unknown'\n",
    "    \n",
    "# Identificar idioma para 'post_text'\n",
    "Facebook_Posts['post_language'] = Facebook_Posts['post_text'].swifter.apply(detect_language)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "# Identificar idioma para 'comment_text'\n",
    "Facebook_Comments['comment_language'] = Facebook_Comments['comment_text'].swifter.apply(detect_language)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Frequ√™ncia de linguagens dos 'post_text' e 'comment_language'\n",
    "post_lang_freq = Facebook_Posts['post_language'].value_counts()\n",
    "comments_lang_freq = Facebook_Comments['comment_language'].value_counts().fillna(0).astype(int)\n",
    "\n",
    "# Tabela de frequ√™ncia absoluta e relativa\n",
    "lang_freq = pd.DataFrame({'Publica√ß√µes (n)': post_lang_freq,\n",
    "                          'Publica√ß√µes (%)': round(post_lang_freq / post_lang_freq.sum()*100,1),\n",
    "                          'Coment√°rios (n)': comments_lang_freq,\n",
    "                          'Coment√°rios (%)':round(comments_lang_freq / comments_lang_freq.sum()*100,1)})\n",
    "lang_freq.index.name = 'Linguagens'\n",
    "lang_freq.rename(index={Language.PORTUGUESE: 'Portugu√™s', Language.ENGLISH: 'Ingl√™s', 'Unknown':'NI'}, inplace=True)\n",
    "lang_freq = lang_freq.reindex(index=['Portugu√™s', 'Ingl√™s', 'NI']) # NI = N√£o Identificado\n",
    "lang_freq = lang_freq.fillna(0)\n",
    "lang_freq['Publica√ß√µes (n)'] = lang_freq['Publica√ß√µes (n)'].astype(int)\n",
    "lang_freq['Coment√°rios (n)'] = lang_freq['Coment√°rios (n)'].astype(int)\n",
    "lang_freq"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# Facebook_Posts[(Facebook_Posts['post_language'] != Language.PORTUGUESE) &\n",
    "#                (Facebook_Posts['post_language'] != 'Unknown')\n",
    "#               ][['post_text', 'post_language']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# Facebook_Comments[Facebook_Comments['comment_language'] == Language.ENGLISH]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### üìö Tradu√ß√£o do Texto | HuggingFace [Translation Pipeline](https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TranslationPipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Fonte:** Workshop *Hands-on Transformers: Fine-tune your own BERT and GPT* (Moritz Laurer from HuggingFace) [Github Code](https://github.com/MoritzLaurer/summer-school-transformers-2023/blob/main/)\n",
    "\n",
    "**Machine Translation**\n",
    "\n",
    "* Open source machine translation (MT) models enable you to translate between many different languages without Google Translate.\n",
    "* [University of Helsinki](https://huggingface.co/Helsinki-NLP) uploaded models for more than 1000 language pairs to the Hugging Face hub\n",
    "* [Facebook AI](https://huggingface.co/models?search=facebook+m2m) open-sourced several multi-lingual models\n",
    "* The [EasyNMT library](https://github.com/UKPLab/EasyNMT), provides an easy wrapper for all these models\n",
    "* Most machine translation models translate between two languages in one direction (e.g. German to English, but not English to German), some can translate in multiple directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# !pip install transformers\n",
    "# The Transformers library from Hugging Face\n",
    "# !pip install sentencepiece\n",
    "# optional tokeniser, required for some models. e.g. machine translation\n",
    "# running large models with accelerate https://huggingface.co/blog/accelerate-large-models\n",
    "# NOTE: we need to restart the runtime after installing accelerate\n",
    "# !pip install accelerate\n",
    "############# https://pytorch.org/ -> Instalar"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# automatically chose CPU or GPU for inference, depending on your hardware\n",
    "import torch\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else -1\n",
    "# -1 == CPU ; 0 == GPU\n",
    "print(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'C:/HuggingFace'               #   -----------> Caso de Erro -> Alterar a paste de '.cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = 'C:/HuggingFace'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'C:/HuggingFace'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# ```Python\n",
    "from transformers import pipeline\n",
    "\n",
    "# translation pipeline docs: https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TranslationPipeline\n",
    "pipeline_translate = pipeline(\"translation\",                   # Tipo de pipeline\n",
    "                              model=\"facebook/m2m100_418M\",    # Modelo\n",
    "                              tokenizer=\"facebook/m2m100_418M\",\n",
    "                              # framework='pt',                  # 'pt' for PyTorch or 'tf' for TensorFlow\n",
    "                              device=device,                    # -1 -> CPU | 0 -> CUDA [NVIDIA]\n",
    "                              batch_size=8\n",
    "                             )\n",
    "\n",
    "# Fun√ß√£o para traduzir um texto\n",
    "def translate_text(text):\n",
    "    # Aplicar o pipeline de tradu√ß√£o ao texto\n",
    "    translated_text = pipeline_translate(text, src_lang=\"pt\", tgt_lang=\"en\")[0][\"translation_text\"]\n",
    "    return translated_text\n",
    "# ```"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Frase Teste\n",
    "text_example = \"As operadoras de telecomunica√ß√µes oferecem uma gama de servi√ßos de comunica√ß√£o.\"\n",
    "translate_text(text_example)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Aplicar a tradu√ß√£o aos posts e coment√°rios do Facebook\n",
    "# Facebook_Posts['post_text_english'] = Facebook_Posts['post_text'].progress_apply(translate_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Aplicar a tradu√ß√£o aos posts e coment√°rios do Facebook\n",
    "# Facebook_Comments['comment_text_english'] = Facebook_Comments['comment_text'].progress_apply(translate_text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "\n",
    "# Aplicar a tradu√ß√£o aos posts e coment√°rios do Facebook\n",
    "Facebook_Posts['post_text_english'] = Facebook_Posts['post_text'].progress_apply(translate_text)\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Posts[['post_id', 'post_text_english']].to_csv('Datasets_Vodafone/TextMining/post_text_english.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "post_text_english = pd.read_csv('post_text_english.csv')\n",
    "Facebook_Posts = Facebook_Posts.merge(post_text_english, on='post_id', how='left')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Aplicar a tradu√ß√£o aos posts e coment√°rios do Facebook\n",
    "Facebook_Comments['comment_text_english'] = Facebook_Comments['comment_text'].progress_apply(translate_text)\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Comments[['comment_id', 'comment_text_english']].to_csv('Datasets_Vodafone/TextMining/comment_text_english.csv', encoding='utf-8', index=False)\n",
    "\n",
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "comment_text_english = pd.read_csv('comment_text_english.csv')\n",
    "Facebook_Comments = Facebook_Comments.merge(comment_text_english, on='comment_id', how='left')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Por quest√µes de **efici√™ncia** e **poss√≠veis perdas de informa√ß√£o** √∫til √† an√°lise de sentimentos no texto, ***optou-se por n√£o traduzir o texto*** e utilizar o original na l√≠ngua que o utilizador comentou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "10c79f91ae864d188e68698f0f873c27",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "## üõ†Ô∏è Pr√©-Processamento do Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Fonte:** https://www.kaggle.com/code/sudalairajkumar/getting-started-with-text-preprocessing\n",
    "\n",
    "\n",
    "> **NLTK** is the most famous Python Natural Language Processing Toolkit.\n",
    "\n",
    "Algumas das etapas comuns de pr√©-processamento / limpeza de texto s√£o:\n",
    "\n",
    "* *Lower casing*             ‚úÖ\n",
    "* *Removal of Punctuations*  ‚úÖ\n",
    "* *Removal of URLs*          ‚úÖ\n",
    "* *Removal Extra Spaces*     ‚úÖ\n",
    "* *Chat words conversion*    ‚úÖ\n",
    "* *Spelling correction*      ‚úÖ\n",
    "\n",
    "Estes s√£o os diferentes tipos de etapas de pr√©-processamento de texto que se pode efetuar nos dados de texto. Mas n√£o √© necess√°rio fazer tudo isto sempre. √â importante escolher cuidadosamente as etapas de pr√©-processamento com base no nosso caso de utiliza√ß√£o, uma vez que isso tamb√©m desempenha um papel importante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Lower Casing\n",
    "\n",
    "***Lower Casing*** √© uma t√©cnica comum de pr√©-processamento de texto. A ideia √© converter o texto de *input* para o mesmo formato, de modo a que `'texto'`, `'Texto'` e `'TEXTO'` sejam tratados da mesma forma.\n",
    "\n",
    "Isto √© mais √∫til para as t√©cnicas de carateriza√ß√£o do texto, como a frequ√™ncia e o `tfidf`, uma vez que ajuda a combinar as mesmas palavras, reduzindo assim a duplica√ß√£o e obtendo contagens/valores `tfidf` correctos.\n",
    "\n",
    "Por padr√£o, o lower casing √© feito pela maioria dos *vecotirzers* e *tokenizers* modernos, como o **`sklearn`** `TfidfVectorizer` e o `Keras Tokenizer`. Por isso, √© preciso de os definir como falsos conforme necess√°rio, dependendo do caso de utiliza√ß√£o.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text'].str.lower()\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text'].str.lower()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removal of URLs\n",
    "\n",
    "O pr√≥ximo passo do pr√©-processamento √© remover quaisquer URLs presentes nos dados. Por exemplo, se se estiver a fazer uma an√°lise do Twitter, √© muito prov√°vel que o tweet contenha algum URL. √â prov√°vel que se precise de os remover para a an√°lise posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "source": [
    "def remove_urls(text):\n",
    "    if text:\n",
    "        url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        return url_pattern.sub(r'', text)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Teste da Fun√ß√£o\n",
    "text = \"Boa tarde https://www.siteexemplo.com/\"\n",
    "print('\\033[1mAntes:\\033[0m', text)\n",
    "print('\\033[1mDepois:\\033[0m', remove_urls(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Aplicar ao dataset\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: remove_urls(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: remove_urls(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removal of Punctuations\n",
    "\n",
    "Uma outra t√©cnica comum de pr√©-processamento de texto consiste em remover as pontua√ß√µes dos dados de texto. Trata-se, mais uma vez, de um processo de normaliza√ß√£o do texto que ajudar√° a tratar 'Ol√°' e 'Ol√°!' da mesma forma.\n",
    "\n",
    "Tamb√©m √© preciso escolher cuidadosamente a lista de pontua√ß√µes a excluir, dependendo do caso de utiliza√ß√£o. Por exemplo, o `string.punctuation` em python cont√©m os seguintes s√≠mbolos de pontua√ß√£o:\n",
    "\n",
    "`!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_{|}~`\n",
    "\n",
    "Pode-se acrescentar ou retirar mais pontua√ß√µes consoante as necessidades.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import string\n",
    "PUNCT_TO_REMOVE = string.punctuation\n",
    "def remove_punctuation(text):\n",
    "    if text:\n",
    "        \"\"\"Custom function to remove the punctuation\"\"\"\n",
    "        return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# Aplicar ao dataset\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: remove_punctuation(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: remove_punctuation(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removal Extra Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A fase seguinte envolve retirar espa√ßos extra que possam estar entre as palavras de uma frase. Para isso, vai ser criada a seguinte fun√ß√£o: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "source": [
    "def remove_extra_spaces(text):\n",
    "    if text:\n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "# Teste\n",
    "sentence = \"Isso   √©   um   exemplo    com    espa√ßos   extra.\"\n",
    "print(\"\\033[1mAntes:\\033[0m\", sentence)\n",
    "print(\"\\033[1mDepois:\\033[0m\", remove_extra_spaces(sentence))\n",
    "\n",
    "# Remover espa√ßos | Aplicar ao dataset\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: remove_extra_spaces(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: remove_extra_spaces(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Chat Words Conversion\n",
    "\n",
    "Este √© um passo importante de pr√©-processamento de texto se se estiver a lidar com dados de conversa√ß√£o. Os utilizadores usam muitas palavras abreviadas no chat e, por isso, pode ser √∫til expandir essas palavras para fins de an√°lise. \n",
    "\n",
    "Lista de palavras de g√≠ria inglesas de ***chat*** neste [Reposit√≥rio](https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "source": [
    "#!pip install deep_translator ---- Caso n√£o us√°ssemos o pipeline('translation')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fonte: https://medium.com/analytics-vidhya/how-to-translate-text-with-python-9d203139dcf5\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "to_translate = 'I want to translate this text'\n",
    "print(\"\\033[1mAntes:\\033[0m\", to_translate)\n",
    "print(\"\\033[1mDepois:\\033[0m\", GoogleTranslator(source='english', target='portuguese').translate(to_translate))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Fun√ß√£o para traduzir um texto\n",
    "# def translate_text_pt(text):\n",
    "#     # Aplicar o pipeline de tradu√ß√£o ao texto\n",
    "#     translated_text = pipeline_translate(text, src_lang=\"en\", tgt_lang=\"pt\")[0][\"translation_text\"]\n",
    "#     return translated_text"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# print(\"\\033[1mCom o pipeline:\\033[0m\", translate_text_pt(to_translate))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "source": [
    "file = open('Datasets_Vodafone/Auxiliares/abreviaturas_inglesas.txt','r', encoding='utf-8')\n",
    "chat_words_str_en = file.read()\n",
    "file.close()\n",
    "\n",
    "chat_words_map_dict_en = {}\n",
    "chat_words_list_en = []\n",
    "for line in chat_words_str_en.split(\"\\n\"):\n",
    "    if line != \"\":\n",
    "        cw = line.split(\"=\")[0]\n",
    "        cw_expanded = line.split(\"=\")[1]\n",
    "        chat_words_list_en.append(cw)\n",
    "        chat_words_map_dict_en[cw] = GoogleTranslator(source='english', target='portuguese').translate(cw_expanded.lower())\n",
    "        # chat_words_map_dict_en[cw] = translate_text_pt(cw_expanded.lower())\n",
    "chat_words_list_en = set(chat_words_list_en)\n",
    "# chat_words_map_dict_en"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "source": [
    "def chat_words_conversion_en(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list_en:\n",
    "            new_text.append(chat_words_map_dict_en[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "text = \"One minute BRB\"\n",
    "print('\\033[1mAntes:\\033[0m', text)\n",
    "print('\\033[1mDepois:\\033[0m', chat_words_conversion_en(text.lower()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Leitura do dataset das abreviaturas em portugu√™s\n",
    "chat_words_str_pt = pd.read_excel('Datasets_Vodafone/Auxiliares/Abreviaturas_Portugues.xlsx')\n",
    "chat_words_str_pt"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Criar um dicion√°rio para mapear as abrevia√ß√µes e seus significados\n",
    "chat_words_map_dict_pt = dict(zip(chat_words_str_pt['Abreviatura'], chat_words_str_pt['Palavra']))\n",
    "\n",
    "# Criar uma lista de abrevia√ß√µes\n",
    "chat_words_list_pt = set(chat_words_str_pt['Abreviatura'])\n",
    "\n",
    "def chat_words_conversion_pt(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.lower() in chat_words_list_pt:\n",
    "            new_text.append(chat_words_map_dict_pt[w.lower()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "text = \"mn blz tm pdc gnt\"\n",
    "print('\\033[1mAntes:\\033[0m', text)\n",
    "print('\\033[1mDepois:\\033[0m', chat_words_conversion_pt(text.lower()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Aplicar aos datasets\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: chat_words_conversion_en(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: chat_words_conversion_en(text))\n",
    "\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: chat_words_conversion_pt(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: chat_words_conversion_pt(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <a class='anchor' id='99999'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Pr√©-Processamento | Principais Etapas</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "* *Tokenization*         ‚úÖ\n",
    "* *Removal of StopWords* ‚úÖ\n",
    "* *Stemming*             ‚úÖ\n",
    "* *Lemmatization*        ‚úÖ\n",
    "* *POS Tagging*          ‚úÖ\n",
    "* *Naming Entities*      ‚úÖ\n",
    "* *Desambigua√ß√£o*        ‚úÖ\n",
    "* *N-Gramas*             ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "source": [
    "## # Frase exemplo:\n",
    "text = 'As operadoras de telecomunica√ß√µes oferecem uma gama de servi√ßos de comunica√ß√£o.'\n",
    "print(\"\\033[1mAntes:\\033[0m\", text)\n",
    "text = text.lower()\n",
    "text = remove_urls(text)\n",
    "text = remove_punctuation(text)\n",
    "text = remove_extra_spaces(text)\n",
    "text = chat_words_conversion_en(text)\n",
    "text = chat_words_conversion_pt(text)\n",
    "print(\"\\033[1mDepois:\\033[0m\", text)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Removal of *StopWords*\n",
    "\n",
    "As stopwords s√£o palavras que ocorrem habitualmente numa l√≠ngua, como **\"o\"**, **\"a\"**, etc. Na maioria das vezes, podem ser removidas do texto, uma vez que n√£o fornecem informa√ß√µes valiosas para a an√°lise a jusante. Em casos como a etiquetagem da Parte do Discurso, n√£o se deve remover, pois fornecem informa√ß√µes muito valiosas sobre o `POS`.\n",
    "\n",
    "Estas listas de palavras j√° est√£o compiladas para diferentes l√≠nguas no pacote `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Obtenha a lista padr√£o de stopwords em portugu√™s\n",
    "STOPWORDS = set(stopwords.words('portuguese'))\n",
    "\n",
    "# Remova a palavra \"nos\" da lista de stopwords\n",
    "STOPWORDS.discard(\"nos\")\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Fun√ß√£o personalizada para remover as stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word.lower() not in STOPWORDS])\n",
    "\n",
    "# Teste da fun√ß√£o com um exemplo de texto\n",
    "# texto = \"O gato preto atravessou a rua nos √† noite. O cachorro latiu. Nos coment√°rios, as pessoas elogiaram a NOS.\"\n",
    "print(\"\\033[1mAntes:\\033[0m\", text)\n",
    "print(\"\\033[1mDepois:\\033[0m\", remove_stopwords(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Aplicar ao dataset\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: remove_stopwords(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: remove_stopwords(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Stemming\n",
    "\n",
    "O stemming √© o processo de redu√ß√£o de palavras flexionadas (ou por vezes derivadas) ao seu radical, base ou raiz (de [Wikipedia](https://en.wikipedia.org/wiki/Stemming))\n",
    "\n",
    "Por exemplo, se existirem duas palavras no corpus `walks` e `walking`, ent√£o o stemming reduzir√° o sufixo para as tornar `walk`. Mas, por exemplo, se houver duas palavras `console` e `consoling`, o stemmer ir√° remover o sufixo e torn√°-las `consol`, o que n√£o √© uma palavra correcta em ingl√™s.\n",
    "\n",
    "Existem v√°rios tipos de algoritmos de ***stemming*** dispon√≠veis e um dos mais famosos √© o porter stemmer que √© muito utilizado. Pode-se usar o pacote nltk para o mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "print(\"\\033[1mAntes:\\033[0m\", text)\n",
    "print(\"\\033[1mDepois:\\033[0m\", stem_words(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Aplicar ao dataset\n",
    "Facebook_Posts['post_text_clean'] = Facebook_Posts['post_text_clean'].swifter.apply(lambda text: remove_stopwords(text))\n",
    "Facebook_Comments['comment_text_clean'] = Facebook_Comments['comment_text_clean'].swifter.apply(lambda text: remove_stopwords(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text','post_text_clean']]\n",
    "Facebook_Comments[['comment_text','comment_text_clean']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lematiza√ß√£o √© o processo de agrupar inflex√µes de uma palavra numa forma base ou de recuperar a forma base da mesma palavra, agrupando diferentes formas de conjugar um verbo, por exemplo, num s√≥ formato, segue-se um exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# nltk.download('omw-1.4')\n",
    "# !pip install spacy\n",
    "# !python -m spacy download pt_core_news_sm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Carregar o modelo em portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    # Processar o texto com o modelo spaCy\n",
    "    doc = nlp(text)\n",
    "    # Lematizar cada token no texto e retornar o texto lematizado\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "# texto = \"Felizes s√£o aqueles que sonham.\"\n",
    "print(\"\\033[1mAntes:\\033[0m\", text)\n",
    "print(\"\\033[1mDepois:\\033[0m\", lemmatize_text(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### POS (Part-Of-Speech) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "O processo de tagging tem como principal obejtivo atribuir e identificar uma classe lexical a cada palavra do corpo do texto, de forma a ter muitas palavras relacionadas e formas de palavras numa √∫nica forma can√≥nica se poss√≠vel (n√£o necessariamente a forma b√°sica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidden": true
   },
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "doc = nlp(text)\n",
    "print(\"\\033[1mAntes:\\033[0m\", doc.text)\n",
    "print(\"\\033[1mDepois:\\033[0m\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Para a an√°lise pretendida **n√£o √© relevante aplicar esta t√©cnica** aos *datasets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Tokenization \n",
    "\n",
    "A tokeniza√ß√£o, no dom√≠nio do *Processamento de Linguagem Natural* (NLP), refere-se ao processo de convers√£o de uma sequ√™ncia de texto em partes mais pequenas, conhecidas como tokens. \n",
    "\n",
    "Estes **`tokens`** podem ser t√£o pequenos como caracteres ou t√£o longos como palavras. A principal raz√£o pela qual este processo √© importante √© o facto de ajudar os algoritmos a compreender a linguagem humana, dividindo-a em partes pequenas, que s√£o mais f√°ceis de analisar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "cell_id": "49ba8c8c510546c58978fe8112bf0424",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# ======================== Pr√©-processamento com vista √† representa√ß√£o de documentos ========================\n",
    "# Como ler o L√©xico de sentimentos ?\n",
    "# R: Os dataframes e as s√©ries s√£o pouco eficientes. Logo, uma melhor solu√ß√£o ser√° \n",
    "#    representar tudo com um dicion√°rio [.to_dict()], o que tornar√° o nosso c√≥digo muito mais r√°pido...\n",
    "\n",
    "# Tokeniza√ß√£o com NLTK [1¬∫ Alterantiva]\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Tokeniza√ß√£o com NLTK [2¬∫ Alterantiva] - Identifica smiles ':)' e '#s'\n",
    "mytkzr = TweetTokenizer()\n",
    "print(mytkzr.tokenize(text))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "cell_id": "49ba8c8c510546c58978fe8112bf0424",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "# Depois do pr√©-processamento inicial, calcular a frequ√™ncia de cada token\n",
    "freq=collections.Counter(tokens)\n",
    "print(freq)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Naming Entities\n",
    "\n",
    "O processo de nomear entidades envolve classificar nomes e outras entidades em categorias espec√≠ficas, como organiza√ß√µes, pessoas, locais, express√µes temporais (como tempo e data) e quantidades (valores monet√°rios, percentagens e n√∫meros de telefone). \n",
    "\n",
    "Uma das abordagens utiliza Learning Systems, os quais empregam t√©cnicas estat√≠sticas ou de machine learning para descobrir regras e crit√©rios automaticamente, geralmente exigindo grandes volumes de dados para o treinamento dos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Carregar o modelo em portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.text, ent.label_))\n",
    "    return entities\n",
    "\n",
    "texto = \"A VODAFONE, a NOS e a MEO s√£o as maiores operadoras de telecomunica√ß√£o em Portugal.\"\n",
    "print(\"\\033[1mTexto:\\033[0m\", texto, '\\n')\n",
    "print(\"\\033[1mEntidades identificadas:\\033[0m\")\n",
    "for entity, label in extract_entities(texto):\n",
    "    print(f\"\\033[1mEntidade:\\033[0m {entity}, \\033[1mTipo:\\033[0m {label}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Devido ao tipo de estrat√©gia que vamos optar no ***Modeling*** n√£o √© necess√°rio aplicar ao *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Desambigua√ß√£o\n",
    "\n",
    "Este √© o processo de remover a ambiguidade de um texto, atribuindo um significado espec√≠fico a uma palavra entre v√°rias possibilidades existentes. Geralmente, o sentido √© determinado por um dicion√°rio, mas em casos de polissemia ou homon√≠mia, a escolha deve ser baseada no contexto. \n",
    "\n",
    "Numa perspectiva de extra√ß√£o autom√°tica, selecionar a interpreta√ß√£o correta pode ser desafiador quando h√° apenas um token. √â importante notar que uma palavra pode ter m√∫ltiplos ou at√© contradit√≥rios significados, como \"t√≠tulo\" ou \"apple\". A desambigua√ß√£o √© essencial para garantir a compreens√£o correta do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "# wn.langs() # 'por' para Portugu√™s\n",
    "\n",
    "# Fun√ß√£o para desambiguar uma palavra com base no contexto\n",
    "def desambiguar_palavra(palavra, contexto):\n",
    "    synsets_palavra = wn.synsets(palavra, lang='por')\n",
    "    if not synsets_palavra:\n",
    "        return palavra  # Se n√£o houver synsets, retornar a pr√≥pria palavra\n",
    "    else:\n",
    "        melhor_synset = None\n",
    "        melhor_similaridade = -1\n",
    "        for syn_palavra in synsets_palavra:\n",
    "            for token in nltk.word_tokenize(contexto):\n",
    "                synsets_token = wn.synsets(token, lang='por')\n",
    "                for syn_token in synsets_token:\n",
    "                    similaridade = syn_palavra.path_similarity(syn_token)\n",
    "                    if similaridade and similaridade > melhor_similaridade:\n",
    "                        melhor_synset = syn_palavra\n",
    "                        melhor_similaridade = similaridade\n",
    "        if melhor_synset:\n",
    "            return melhor_synset.definition()\n",
    "        else:\n",
    "            return palavra"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Exemplo de uso\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "print(\"\\033[1mTexto:\\033[0m\", texto, '\\n')\n",
    "for token in tokens:\n",
    "    significado = desambiguar_palavra(token, texto)\n",
    "    print(f\"\\033[1m{token}:\\033[0m {significado}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üîÇ Unigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Texto de exemplo\n",
    "# texto = \"O gato preto atravessou a rua √† noite. O gato bebeu leite.\"\n",
    "print(\"\\033[1mAntes:\\033[0m\", text)\n",
    "# Tokeniza√ß√£o do texto\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Exibi√ß√£o dos unigramas\n",
    "print(\"\\033[1mUnigramas:\\033[0m\")\n",
    "for unigrama in tokens:\n",
    "    print(unigrama)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "source": [
    "#!pip install Wordcloud"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Texto de exemplo\n",
    "# texto = \"O gato preto atravessou a rua √† noite. O gato bebeu leite.\"\n",
    "print(\"\\033[1mAntes:\\033[0m\", text, '\\n')\n",
    "# Tokeniza√ß√£o do texto\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Contagem das palavras\n",
    "contagem_palavras = Counter(tokens)\n",
    "\n",
    "# Exibir as palavras mais frequentes\n",
    "print(\"\\033[1mPalavras mais frequentes:\\033[0m\")\n",
    "for palavra, frequencia in contagem_palavras.most_common(len(contagem_palavras)):  # As 5 palavras mais frequentes\n",
    "    print(f\"\\033[1m{palavra}:\\033[0m {frequencia} vezes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "source": [
    "contagem_palavras"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üí¨ Aplica√ß√£o √† base de dados j√° limpa para os *posts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tokeniza√ß√£o do texto dos posts\n",
    "tokens = []\n",
    "for post in Facebook_Posts['post_text_clean']:\n",
    "    tokens.extend(word_tokenize(str(post)))\n",
    "\n",
    "# Contagem das palavras\n",
    "contagem_palavras_post_text = Counter(tokens)\n",
    "\n",
    "# # Exibir as palavras mais frequentes\n",
    "# print(\"\\033[1mPalavras mais frequentes:\\033[0m\")\n",
    "# for palavra, frequencia in contagem_palavras_post_text.most_common(50):  # As 50 palavras mais frequentes\n",
    "#     print(f\"{palavra}: {frequencia} vezes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# import csv\n",
    "\n",
    "# # Cria√ß√£o do arquivo CSV - Unigrama Post\n",
    "# with open('Datasets_Vodafone/TextMining/Unigramas_Post.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Token\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Escrita dos tokens e suas frequ√™ncias no arquivo CSV\n",
    "#     for token, frequencia in contagem_palavras_post_text.items():\n",
    "#         escritor.writerow([token, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mN¬∫ de Tokens/Unigramas escritos na vari√°vel post_text:\\033[0m\",sum(contagem_palavras_post_text.values()), \"\\033[1mdos quais\\033[0m\", \n",
    "      len(contagem_palavras_post_text), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# !pip install wordcloud"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Define a function to plot word cloud\n",
    "def plot_cloud(wordcloud):\n",
    "    fig = plt.figure(figsize=(25, 17), dpi=80)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.box(False)\n",
    "    plt.show()\n",
    "    plt.close() "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Criar um objeto WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap='OrRd') \\\n",
    "            .generate_from_frequencies(contagem_palavras_post_text)\n",
    "\n",
    "# Plotar a nuvem de palavras\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üí¨ Aplica√ß√£o √† base de dados j√° limpa para os *comments*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Tokeniza√ß√£o do texto dos posts\n",
    "tokens = []\n",
    "for post in Facebook_Comments['comment_text_clean']:\n",
    "    tokens.extend(word_tokenize(str(post)))\n",
    "\n",
    "# Contagem das palavras\n",
    "contagem_palavras_comment_text = Counter(tokens)\n",
    "\n",
    "# # Exibir as palavras mais frequentes\n",
    "# print(\"\\033[1mPalavras mais frequentes:\\033[0m\")\n",
    "# for palavra, frequencia in contagem_palavras_comment_text.most_common(50):  # As 50 palavras mais frequentes\n",
    "#     print(f\"{palavra}: {frequencia} vezes\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# import csv\n",
    "\n",
    "# # Cria√ß√£o do arquivo CSV - Unigrama Comment\n",
    "# with open('Datasets_Vodafone/TextMining/Unigramas_Comment.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Token\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Escrita dos tokens e suas frequ√™ncias no arquivo CSV\n",
    "#     for token, frequencia in contagem_palavras_comment_text.items():\n",
    "#         escritor.writerow([token, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "source": [
    "print(\"\\033[1mN¬∫ de Tokens/Unigramas escritos na vari√°vel comment_text:\\033[0m\",sum(contagem_palavras_comment_text.values()), \"\\033[1mdos quais\\033[0m\", \n",
    "      len(contagem_palavras_comment_text), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---\n",
    "Entre as operadoras, pode-se observar que a mais mencionada nos coment√°rios √© a **NOS**, em oitavo lugar das palavras mais frequentes, seguida pela **MEO**, em d√©cimo s√©timo lugar, j√° a **Vodafone** encontra-se em quadrag√©siomo nono lugar. \n",
    "\n",
    "√â de notar que na limpeza pr√©via n√£o considerou o sentido de cada coment√°rio onde a palavra \"nos\" era utilizada, da√≠ poder n√£o estar completamente preciso em termos de ser a operadora NOS (e n√£o o \"nos\" a n√≠vel gramatical). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Criar um objeto WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap='OrRd') \\\n",
    "            .generate_from_frequencies(contagem_palavras_comment_text)\n",
    "\n",
    "# Plotar a nuvem de palavras\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üîó Bigramas e Trigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# NGram function - Fonte: https://www.kaggle.com/code/bwandowando/generate-bigram-and-trigram-wordcloud\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "def get_ngrams(text, n=2):\n",
    "    text = str(text)\n",
    "    n_grams = ngrams(text.split(), n)\n",
    "    returnVal = []\n",
    "    \n",
    "    try:\n",
    "        for grams in n_grams:\n",
    "            returnVal.append('_'.join(grams))\n",
    "    except(RuntimeError):\n",
    "        pass\n",
    "        \n",
    "    return ' '.join(returnVal).strip()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = \"A Vodafone tentou comprar a NOS e a MEO mas falhou de novo.\"\n",
    "\n",
    "# Tokeniza√ß√£o do texto\n",
    "tokens = word_tokenize(texto)\n",
    "\n",
    "# Cria√ß√£o de bi-grafos\n",
    "bi_grafos = list(bigrams(tokens))\n",
    "\n",
    "# Exibi√ß√£o dos bi-grafos\n",
    "print(\"\\033[1mBi-grafos:\\033[0m\")\n",
    "for bi_grafo in bi_grafos:\n",
    "    print(bi_grafo)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Consegue-se perceber como os bigramas funcionam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Aplica√ß√£o dos `Bigramas` e `Trigramas` √† base de dados limpa para os *posts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Bigramas 'post_text'\n",
    "bi_post_text = Facebook_Posts['post_text_clean'].swifter.apply(get_ngrams, n=2)\n",
    "\n",
    "bi_post_text_string_list = bi_post_text.tolist()\n",
    "bi_post_text_string = ' '.join(bi_post_text_string_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Cria√ß√£o do arquivo CSV - Bigramas Posts\n",
    "# with open('Datasets_Vodafone/TextMining/Bigramas_Post.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Bigrama\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Contagem dos bigramas\n",
    "#     contagem_bigramas = Counter(bi_post_text_string.split())\n",
    "\n",
    "#     # Escrita dos bigramas e suas frequ√™ncias no arquivo CSV\n",
    "#     for bigrama, frequencia in contagem_bigramas.items():\n",
    "#         escritor.writerow([bigrama, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Contar bigramas\n",
    "bigramas_post_text_str = Counter(bi_post_text_string.split())\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Bigramas escritos na vari√°vel post_text:\\033[0m\",sum(bigramas_post_text_str.values()), \n",
    "      \"\\033[1mdos quais\\033[0m\", len(bigramas_post_text_str), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "source": [
    "wordcloud = WordCloud(width = 7680, height = 4320, random_state=1, \n",
    "                      background_color='white', colormap='OrRd', # max_words = 75, \n",
    "                      collocations=False, normalize_plurals=False).generate(bi_post_text_string)\n",
    "\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Trigramas 'post_text'\n",
    "tri_post_text = Facebook_Posts['post_text_clean'].swifter.apply(get_ngrams, n=3)\n",
    "tri_post_text_string_list = tri_post_text.tolist()\n",
    "tri_post_text_string = ' '.join(tri_post_text_string_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Cria√ß√£o do arquivo CSV - Trigramas Posts\n",
    "# with open('Datasets_Vodafone/TextMining/Trigramas_Post.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Trigrama\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Contagem dos trigramas\n",
    "#     contagem_trigramas = Counter(tri_post_text_string.split())\n",
    "\n",
    "#     # Escrita dos trigramas e suas frequ√™ncias no arquivo CSV\n",
    "#     for trigrama, frequencia in contagem_trigramas.items():\n",
    "#         escritor.writerow([trigrama, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Contar trigramas\n",
    "trigramas_post_text_str = Counter(tri_post_text_string.split())\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Trigramas escritos na vari√°vel post_text:\\033[0m\",sum(trigramas_post_text_str.values()), \n",
    "      \"\\033[1mdos quais\\033[0m\", len(trigramas_post_text_str), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "source": [
    "wordcloud = WordCloud(width = 7680, height = 4320, random_state=1, \n",
    "                      background_color='white', colormap='OrRd', # max_words = 75, \n",
    "                      collocations=False, normalize_plurals=False).generate(tri_post_text_string)\n",
    "\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Aplica√ß√£o dos Bigramas e Trigramas para a base de dados dos *comments* limpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Bigramas 'comment_text'\n",
    "bi_comment_text = Facebook_Comments['comment_text_clean'].swifter.apply(get_ngrams, n=2)\n",
    "\n",
    "bi_comment_text_string_list = bi_comment_text.tolist()\n",
    "bi_comment_text_string = ' '.join(bi_comment_text_string_list)\n",
    "\n",
    "# Contar bigramas\n",
    "bigramas_comment_text_str = Counter(bi_comment_text_string.split())\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Bigramas escritos na vari√°vel comment_text:\\033[0m\",sum(bigramas_comment_text_str.values()), \n",
    "      \"\\033[1mdos quais\\033[0m\", len(bigramas_comment_text_str), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Cria√ß√£o do arquivo CSV - Bigramas Comment\n",
    "# with open('Datasets_Vodafone/TextMining/Bigramas_Comment.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Bigramas\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Escrita dos bigramas e suas frequ√™ncias no arquivo CSV\n",
    "#     for bigrama, frequencia in bigramas_comment_text_str.items():\n",
    "#         escritor.writerow([bigrama, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "source": [
    "wordcloud = WordCloud(width = 7680, height = 4320, random_state=1, \n",
    "                      background_color='white', colormap='OrRd', # max_words = 75, \n",
    "                      collocations=False, normalize_plurals=False).generate(bi_comment_text_string)\n",
    "\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Trigramas 'comment_text'\n",
    "tri_comment_text = Facebook_Comments['comment_text_clean'].swifter.apply(get_ngrams, n=3)\n",
    "tri_comment_text_string_list = tri_comment_text.tolist()\n",
    "tri_comment_text_string = ' '.join(tri_comment_text_string_list)\n",
    "\n",
    "# Contar trigramas\n",
    "trigramas_comment_text_str = Counter(tri_comment_text_string.split())\n",
    "\n",
    "print(\"\\033[1mN¬∫ de Bigramas escritos na vari√°vel comment_text:\\033[0m\",sum(trigramas_comment_text_str.values()), \n",
    "      \"\\033[1mdos quais\\033[0m\", len(trigramas_comment_text_str), \"\\033[1ms√£o √∫nicos\\033[0m\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Cria√ß√£o do arquivo CSV - Trigramas Comment\n",
    "# with open('Datasets_Vodafone/TextMining/Trigramas_Comment.csv', 'w', newline='', encoding='utf-8') as arquivo:\n",
    "#     escritor = csv.writer(arquivo)\n",
    "#     escritor.writerow([\"Trigramas\", \"Frequ√™ncia\"])\n",
    "\n",
    "#     # Escrita dos trigramas e suas frequ√™ncias no arquivo CSV\n",
    "#     for trigrama, frequencia in trigramas_comment_text_str.items():\n",
    "#         escritor.writerow([trigrama, frequencia])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "source": [
    "wordcloud = WordCloud(width = 7680, height = 4320, random_state=1, \n",
    "                      background_color='white', colormap='OrRd', # max_words = 75, \n",
    "                      collocations=False, normalize_plurals=False).generate(tri_comment_text_string)\n",
    "\n",
    "# Plot\n",
    "plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Alternativa com o `Likelihood Ratio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.collocations import BigramCollocationFinder\n",
    "# from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# # Tokeniza√ß√£o dos textos dos posts\n",
    "# tokens = []\n",
    "# for post in Facebook_Posts['post_text_clean']:\n",
    "#     tokens.extend(word_tokenize(str(post)))\n",
    "\n",
    "# # Gerar bigramas ---- Alternativa\n",
    "# finder = BigramCollocationFinder.from_words(tokens)\n",
    "# bigramas = finder.nbest(BigramAssocMeasures.likelihood_ratio, 100)  # Escolha dos 100 melhores bigramas\n",
    "\n",
    "# # Exibir os bigramas\n",
    "# # print(\"Bigramas mais frequentes:\")\n",
    "# # for bigrama in bigramas:\n",
    "# #     print(bigrama)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# bigramas_freq"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# # Converter os bigramas em uma √∫nica string\n",
    "# bigramas_str = ' '.join(['_'.join(bigram) for bigram in bigramas])\n",
    "\n",
    "# # Criar um objeto WordCloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap='OrRd').generate(bigramas_str)\n",
    "\n",
    "# # Plotar a nuvem de palavras\n",
    "# plot_cloud(wordcloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Consegue-se ter uma melhor ideia sobre o que √© que se refere a palavra \"nos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.collocations import BigramCollocationFinder\n",
    "# from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "# # Tokeniza√ß√£o dos textos dos comments\n",
    "# tokens = []\n",
    "# for post in Facebook_Comments['comment_text_clean']:\n",
    "#     tokens.extend(word_tokenize(str(post)))\n",
    "\n",
    "# # Gerar bigramas\n",
    "# finder = BigramCollocationFinder.from_words(tokens)\n",
    "# bigramas = finder.nbest(BigramAssocMeasures.likelihood_ratio, 100)  # Escolha dos 50 melhores bigramas\n",
    "\n",
    "# Exibir os bigramas\n",
    "# print(\"Bigramas mais frequentes:\")\n",
    "# for bigrama in bigramas:\n",
    "#     print(bigrama)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Converter os bigramas em uma √∫nica string\n",
    "# bigramas_str = ' '.join(['_'.join(bigram) for bigram in bigramas])\n",
    "\n",
    "# Criar um objeto WordCloud\n",
    "# wordcloud = WordCloud(width=800, height=400, background_color=\"white\", colormap='OrRd').generate(bigramas_str)\n",
    "\n",
    "# Plotar a nuvem de palavras\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9d87ff0f71f440b89fae661293b812c4",
    "deepnote_cell_type": "markdown",
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## üóÉÔ∏è Integra√ß√£o de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Compilar as bases de dados\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Posts, Facebook_Comments, how = 'right', \n",
    "                                   on = list(Facebook_Comments.columns.intersection(Facebook_Posts.columns)))\n",
    "\n",
    "# Facebook_Comments.columns.intersection(Facebook_Posts.columns) =\n",
    "#           ['page', 'post_id', 'post_text', 'post_date', 'post_reactions', 'post_comments', \n",
    "#            'post_shares', 'post_day', 'post_month', 'post_year', 'post_hour', 'post_link']\n",
    "\n",
    "Facebook_PCU = pd.merge(Facebook_Posts_Comments, Facebook_Users, how='left', on=['user_link', 'user_name'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Reorganizar as colunas\n",
    "colunas_organizadas = [\n",
    "    'page', 'post_id', 'post_link', 'post_date', 'post_day', 'post_month', 'post_year', 'post_hour', \n",
    "    'post_reactions', 'post_comments', 'post_shares', 'post_text', 'post_text_clean', 'post_language',\n",
    "    \n",
    "    'comment_id', 'comment_link', 'comment_date', 'comment_day_ago', 'comment_reactions', 'comment_num_responses',\n",
    "    'comment_operator_responded', 'comment_text', 'comment_language', 'comment_text_clean',\n",
    "    \n",
    "    'user_name', 'user_link', 'user_current_city', 'user_hometown', \n",
    "    'user_freguesia', 'user_concelho', 'user_distrito', 'user_pais', 'user_city_not_portugal', 'user_predicted_genre'\n",
    "]\n",
    "\n",
    "Facebook_PCU = Facebook_PCU[colunas_organizadas]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Guardar os dados integrados em formato .txt\n",
    "Facebook_PCU.to_csv('Datasets_Vodafone/Facebook_PCU.txt', sep='\\t', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "91ffb7cc8a654c56a154068c093f61f0",
    "deepnote_cell_type": "markdown",
    "heading_collapsed": true
   },
   "source": [
    "# <a class='anchor' id='3'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>4 | Modeling</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia [Para PCs com NVIDIA]\n",
    "# !pip install --upgrade accelerate\n",
    "# ---------\n",
    "# !pip install --upgrade transformers\n",
    "# ---------\n",
    "# !pip install simpletransformers ---------\n",
    "# !pip install xformers           ---------"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fonte: https://huggingface.co/models\n",
    "from transformers import pipeline, AutoModel, AutoTokenizer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # You may use this command to clear your cache\n",
    "torch.cuda.is_available() # You may use this command to check if you have gpu or not"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# automatically chose CPU or GPU for inference, depending on your hardware\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else -1\n",
    "# -1 == CPU ; 0 == GPU\n",
    "print(device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Hugging Face\n",
    "import os\n",
    "os.environ['HF_HOME'] = 'C:/HuggingFace'               #   -----------> Caso de Erro -> Alterar a paste de '.cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = 'C:/HuggingFace'\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'C:/HuggingFace'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Import da base de dados integrada \n",
    "# Facebook_PCU = pd.read_csv('Datasets_Vodafone/Facebook_PCU.txt', sep='\\t', encoding='utf-8')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <a class='anchor' id='4.1'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Sentiment Analysis</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [Meta-Llama-3-8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# import transformers\n",
    "\n",
    "# # Carregar o modelo local\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B\")\n",
    "\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=model, \n",
    "#     tokenizer=tokenizer,    \n",
    "#     model_kwargs={\"torch_dtype\": torch.bfloat16}, \n",
    "#     device_map=\"auto\"\n",
    "# )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# pipeline(\"Hey how are you doing today?\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Requisitos minimos n√£o correspondidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "# model_inputs = encodeds.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "# generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = tokenizer.batch_decode(generated_ids)\n",
    "# print(decoded[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Requisitos minimos n√£o correspondidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [distilbert-base-multilingual-cased-sentiments-student](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "distilled_student_sentiment_classifier = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n",
    "    return_all_scores=True\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Teste em Ingl√™s\n",
    "distilled_student_sentiment_classifier (\"I love this movie and i would watch it again and again!\")\n",
    "# >> [[{'label': 'positive', 'score': 0.9731044769287109},\n",
    "#   {'label': 'neutral', 'score': 0.016910076141357422},\n",
    "#   {'label': 'negative', 'score': 0.009985478594899178}]]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Teste em Portugu√™s\n",
    "distilled_student_sentiment_classifier(\"O servi√ßo n√£o √© de qualidade\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Baixa qualidade de classifica√ß√£o -> N√£o Adaptado para **Portugu√™s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [bert-base-multilingual-uncased-sentiment](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "source": [
    "data = [\"A Vodafone √© horr√≠vel!\", \"O Rui √© t√£o feio\", \"O senhor √© maluco!\", \"O mapa est√° feito!\", \"For√ßa FCP\"]\n",
    "classifier(data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "source": [
    "comments = Facebook_Comments['comment_text'][0:10].to_list()\n",
    "output = classifier(comments)\n",
    "df_result = pd.DataFrame(output)\n",
    "df_result['comment'] = comments\n",
    "df_result"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> N√£o se justifica esta subdivis√£o para a an√°lise de sentimentos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "9361c875b198470b9a3fb7bb48b46ec6",
    "deepnote_cell_type": "code",
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [twitter-xlm-roberta-base-sentiment-finetunned](https://huggingface.co/citizenlab/twitter-xlm-roberta-base-sentiment-finetunned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "source": [
    "model_path = \"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\"\n",
    "TXRBSF_sentiment_classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "\n",
    "# Defina a fun√ß√£o de classifica√ß√£o\n",
    "def TXRBSF_classify_sentiment(text):\n",
    "    if text.strip() == '' or text is None:\n",
    "        return np.NaN, np.NaN\n",
    "    outputs = TXRBSF_sentiment_classifier(text, batch_size=8)\n",
    "    return outputs[0]['label'], outputs[0]['score']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Teste do Modelo\n",
    "\n",
    "# Exemplo\n",
    "print(\"\\033[1mElementos da Pipeline:\\033[0m \\n\",\n",
    "      TXRBSF_sentiment_classifier(\"A Vodafone tem um servi√ßo de baixa qualidade!\"))\n",
    "\n",
    "print(\"\\n\\033[1mOutput da Fun√ß√£o TXRBSF_classify_sentiment:\\033[0m \\n\",\n",
    "      TXRBSF_classify_sentiment(\"A Vodafone tem um servi√ßo de baixa qualidade!\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 30.04.2024 ------------------------------\n",
    "# Aplicar o twitter-xlm-roberta-base-sentiment-finetunned ao dataset dos posts\n",
    "TXRBSF_classify_posts_sentiments = Facebook_Posts['post_text'].progress_apply(lambda x: TXRBSF_classify_sentiment(x))\n",
    "labels, scores = zip(*[(label, score) for label, score in TXRBSF_classify_posts_sentiments])\n",
    "\n",
    "Facebook_Posts['TXRBSF_post_sentiment_label'] = labels\n",
    "Facebook_Posts['TXRBSF_post_sentiment_score'] = scores\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Posts[['post_id','post_text', 'TXRBSF_post_sentiment_label', 'TXRBSF_post_sentiment_score']].to_csv('Datasets_Vodafone/TextMining/TXRBSF_post_sentiment.csv', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "TXRBSF_post_sentiment = pd.read_csv('Datasets_Vodafone/TextMining/TXRBSF_post_sentiment.csv') \\\n",
    "                          .drop(['post_id','post_text'], axis=1)\n",
    "Facebook_Posts = Facebook_Posts.assign(**TXRBSF_post_sentiment)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text', 'TXRBSF_post_sentiment_label', 'TXRBSF_post_sentiment_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tabela de Frequ√™ncias de sentimentos\n",
    "print(\"\\033[1mN¬∫ de NAs:\\033[0m\", Facebook_Posts['TXRBSF_post_sentiment_label'].isna().sum())\n",
    "pd.DataFrame({\n",
    "    'n':Facebook_Posts['TXRBSF_post_sentiment_label'].value_counts(),\n",
    "    '%':round(Facebook_Posts['TXRBSF_post_sentiment_label'].value_counts(normalize=True) * 100,1)})"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> O n¬∫ de **`NAs`** bate certo com os *posts* sem texto!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 01.05.2024 ------------------------------\n",
    "# Aplicar o twitter-xlm-roberta-base-sentiment-finetunned ao dataset dos coment√°rios\n",
    "TXRBSF_classify_comments_sentiments = Facebook_Comments['comment_text'].progress_apply(lambda x: TXRBSF_classify_sentiment(x))\n",
    "labels, scores = zip(*[(label, score) for label, score in TXRBSF_classify_comments_sentiments])\n",
    "\n",
    "Facebook_Comments['TXRBSF_comment_sentiment_label'] = labels\n",
    "Facebook_Comments['TXRBSF_comment_sentiment_score'] = scores\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Comments[['comment_id', 'comment_text', 'TXRBSF_comment_sentiment_label', 'TXRBSF_comment_sentiment_score']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/TXRBSF_comment_sentiment.csv', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "TXRBSF_comment_sentiment = pd.read_csv('Datasets_Vodafone/TextMining/TXRBSF_comment_sentiment.csv') \\\n",
    "                             .drop(['comment_id','comment_text'], axis=1)\n",
    "Facebook_Comments = Facebook_Comments.assign(**TXRBSF_comment_sentiment)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Verificar as 4 observa√ß√µes sem 'TXRBSF_comment_sentiment_label' e 'TXRBSF_comment_sentiment_score' -> N√£o t√™m texto\n",
    "# Facebook_Comments[Facebook_Comments[['TXRBSF_comment_sentiment_label', 'TXRBSF_comment_sentiment_score']].isna().any(axis=1)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Comments[['comment_text', 'TXRBSF_comment_sentiment_label', 'TXRBSF_comment_sentiment_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "hidden": true
   },
   "source": [
    "TXRBSF_comment_sentiment_label_df = pd.DataFrame({\n",
    "    'n':Facebook_Comments['TXRBSF_comment_sentiment_label'].value_counts(),\n",
    "    '%':round(Facebook_Comments['TXRBSF_comment_sentiment_label'].value_counts(normalize=True) * 100,1)})\n",
    "\n",
    "TXRBSF_comment_sentiment_label_df.index.name = 'TXRBSF Model'\n",
    "TXRBSF_comment_sentiment_label_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [mDeBERTa-v3-base-xnli-multilingual-nli-2mil7](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Defina o pipeline de classifica√ß√£o\n",
    "model_path = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "moritz_classifier = pipeline(\"zero-shot-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "\n",
    "# Nota: para utlizar este modelo temos de dar moritz_classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "\n",
    "# Defina as classes a classificar pelo modelo\n",
    "candidate_labels = ['Positive', 'Neutral', 'Negative']\n",
    "\n",
    "# Defina a fun√ß√£o de classifica√ß√£o\n",
    "def mDeBERTa_classify_sentiment(texts):\n",
    "    outputs = moritz_classifier([texts], candidate_labels, multi_label=False, batch_size=8)\n",
    "    labels = [output['labels'][0] for output in outputs]\n",
    "    scores = [output['scores'][0] for output in outputs]\n",
    "    return labels, scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Exemplo\n",
    "print(\"\\033[1mElementos da Pipeline:\\033[0m \\n\",\n",
    "      moritz_classifier(\"A Vodafone tem um servi√ßo de baixa qualidade!\", candidate_labels, multi_label=False))\n",
    "\n",
    "print(\"\\n\\033[1mOutput da Fun√ß√£o mDeBERTa_classify_sentiment:\\033[0m \\n\",\n",
    "      mDeBERTa_classify_sentiment(\"A Vodafone tem um servi√ßo de baixa qualidade!\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 30.04.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos posts\n",
    "mDeBERTa_posts_sentiments = Facebook_Posts['post_text'].progress_apply(lambda x: mDeBERTa_classify_sentiment([x]))\n",
    "labels, scores = zip(*[(label, score) for label, score in mDeBERTa_posts_sentiments])\n",
    "\n",
    "Facebook_Posts['mDeBERTa_post_sentiment_label'] = labels\n",
    "Facebook_Posts['mDeBERTa_post_sentiment_score'] = scores\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Posts[['post_id', 'post_text', 'mDeBERTa_post_sentiment_label', 'mDeBERTa_post_sentiment_score']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_sentiment.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_post_sentiment = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_sentiment.csv', encoding='utf-8')  \\\n",
    "                          .drop(['post_id','post_text'], axis=1)\n",
    "Facebook_Posts = Facebook_Posts.assign(**mDeBERTa_post_sentiment)\n",
    "\n",
    "# Limpar a vari√°vel 'mDeBERTa_post_sentiment_label'\n",
    "Facebook_Posts['mDeBERTa_post_sentiment_label'] = Facebook_Posts['mDeBERTa_post_sentiment_label'].str.strip(\"['']\")\n",
    "\n",
    "# Converter a vari√°vel 'mDeBERTa_post_sentiment_score' para float\n",
    "Facebook_Posts['mDeBERTa_post_sentiment_score'] = Facebook_Posts['mDeBERTa_post_sentiment_score'].str.strip(\"['']\").astype(float)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "hidden": true
   },
   "source": [
    "mDeBERTa_post_sentiment_label_df = pd.DataFrame({\n",
    "    'n':Facebook_Posts['mDeBERTa_post_sentiment_label'].value_counts(),\n",
    "    '%':round(Facebook_Posts['mDeBERTa_post_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "})\n",
    "\n",
    "mDeBERTa_post_sentiment_label_df.index.name = 'mDeBERTa Model'\n",
    "mDeBERTa_post_sentiment_label_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text', 'mDeBERTa_post_sentiment_label', 'mDeBERTa_post_sentiment_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 01.05.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos coment√°rios\n",
    "mDeBERTa_comments_sentiments = Facebook_Comments['comment_text'].progress_apply(lambda x: mDeBERTa_classify_sentiment([x]))\n",
    "labels, scores = zip(*[(label, score) for label, score in mDeBERTa_comments_sentiments])\n",
    "\n",
    "Facebook_Comments['mDeBERTa_comment_sentiment_label'] = labels\n",
    "Facebook_Comments['mDeBERTa_comment_sentiment_score'] = scores\n",
    "\n",
    "# Salvar a nova coluna como um arquivo CSV\n",
    "Facebook_Comments[['comment_id', 'comment_text', 'mDeBERTa_comment_sentiment_label', 'mDeBERTa_comment_sentiment_score']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_sentiment.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_comment_sentiment = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_sentiment.csv', encoding='utf-8') \\\n",
    "                             .drop(['comment_id','comment_text'], axis=1)\n",
    "Facebook_Comments = Facebook_Comments.assign(**mDeBERTa_comment_sentiment)\n",
    "\n",
    "# Limpar a vari√°vel 'mDeBERTa_comment_sentiment_label'\n",
    "Facebook_Comments['mDeBERTa_comment_sentiment_label'] = Facebook_Comments['mDeBERTa_comment_sentiment_label'].str.strip(\"['']\")\n",
    "\n",
    "# Converter a vari√°vel 'mDeBERTa_comment_sentiment_score' para float\n",
    "Facebook_Comments['mDeBERTa_comment_sentiment_score'] = Facebook_Comments['mDeBERTa_comment_sentiment_score'].str.strip(\"['']\").astype(float)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "hidden": true
   },
   "source": [
    "mDeBERTa_comment_sentiment_label_df = pd.DataFrame({\n",
    "    'n':Facebook_Comments['mDeBERTa_comment_sentiment_label'].value_counts(),\n",
    "    '%':round(Facebook_Comments['mDeBERTa_comment_sentiment_label'].value_counts(normalize=True) * 100,1)})\n",
    "\n",
    "mDeBERTa_comment_sentiment_label_df.index.name = 'mDeBERTa Model'\n",
    "mDeBERTa_comment_sentiment_label_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Comments[['comment_text', 'mDeBERTa_comment_sentiment_label', 'mDeBERTa_comment_sentiment_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## ‚ûï Juntar Modelos dos Sentimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Posts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Remova as linhas com valores NaN de ambas as colunas\n",
    "Facebook_Posts_clean = Facebook_Posts.dropna(subset=['TXRBSF_post_sentiment_label', 'mDeBERTa_post_sentiment_label'])\n",
    "\n",
    "# Crie a matriz de classifica√ß√£o\n",
    "conf_matrix = confusion_matrix(Facebook_Posts_clean['TXRBSF_post_sentiment_label'], \n",
    "                               Facebook_Posts_clean['mDeBERTa_post_sentiment_label'], \n",
    "                               labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "\n",
    "# Crie um DataFrame a partir da matriz de classifica√ß√£o\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
    "                              index=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                              columns=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "conf_matrix_df.index.name = \"Modelo TXRBSF\"\n",
    "conf_matrix_df.columns.name = 'Modelo mDeBERTa'\n",
    "\n",
    "display_side_by_side(conf_matrix_df,\n",
    "                     super_title=\"Matriz de Classifica√ß√£o dos Resultados de Ambos os Modelos<br></br>\",\n",
    "                     titles=[\"\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Vari√°vel dos Sentimentos com os 2 Modelos\n",
    "# Defina as condi√ß√µes para cada caso\n",
    "conditions = [\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Positive') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Positive') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Positive') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Negative'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Neutral') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Neutral') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Neutral') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Negative'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Negative') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Negative') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Posts['TXRBSF_post_sentiment_label'] == 'Negative') & (Facebook_Posts['mDeBERTa_post_sentiment_label'] == 'Negative')\n",
    "]\n",
    "\n",
    "# Defina os valores para cada caso\n",
    "values = ['Positivo', 'Tend√™ncia Positiva', 'Neutro', 'Tend√™ncia Positiva', 'Neutro', 'Tend√™ncia Negativa', 'Neutro', 'Tend√™ncia Negativa', 'Negativo']\n",
    "\n",
    "# Aplique as condi√ß√µes e os valores √† nova coluna\n",
    "Facebook_Posts['post_sentiment_label'] = np.select(conditions, values)\n",
    "\n",
    "# Atribua o valor NaN √† vari√°vel 'post_sentiment_label' nas linhas em que uma das colunas √© NaN\n",
    "Facebook_Posts['post_sentiment_label'] = np.where((Facebook_Posts['TXRBSF_post_sentiment_label'].isna()) | \n",
    "                                                           (Facebook_Posts['mDeBERTa_post_sentiment_label'].isna()), \n",
    "                                                           np.nan, \n",
    "                                                           Facebook_Posts['post_sentiment_label'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "hidden": true
   },
   "source": [
    "pd.DataFrame(Facebook_Posts['post_sentiment_label'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### `Comments` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "hidden": true
   },
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Remova as linhas com valores NaN de ambas as colunas\n",
    "Facebook_Comments_clean = Facebook_Comments.dropna(subset=['TXRBSF_comment_sentiment_label', 'mDeBERTa_comment_sentiment_label'])\n",
    "\n",
    "# Crie a matriz de classifica√ß√£o\n",
    "conf_matrix = confusion_matrix(Facebook_Comments_clean['TXRBSF_comment_sentiment_label'],\n",
    "                               Facebook_Comments_clean['mDeBERTa_comment_sentiment_label'],\n",
    "                               labels=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "\n",
    "# Crie um DataFrame a partir da matriz de classifica√ß√£o\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,\n",
    "                              index=[\"Positive\", \"Neutral\", \"Negative\"],\n",
    "                              columns=[\"Positive\", \"Neutral\", \"Negative\"])\n",
    "conf_matrix_df.index.name = \"Modelo TXRBSF\"\n",
    "conf_matrix_df.columns.name = 'Modelo mDeBERTa'\n",
    "\n",
    "display_side_by_side(conf_matrix_df,\n",
    "                     super_title=\"Matriz de Classifica√ß√£o dos Resultados de Ambos os Modelos<br></br>\",\n",
    "                     titles=[\"\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Vari√°vel dos Sentimentos com os 2 Modelos\n",
    "# Defina as condi√ß√µes para cada caso\n",
    "conditions = [\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Positive') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Positive') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Positive') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Negative'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Neutral') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Neutral') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Neutral') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Negative'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Negative') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Positive'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Negative') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Neutral'),\n",
    "    (Facebook_Comments['TXRBSF_comment_sentiment_label'] == 'Negative') & (Facebook_Comments['mDeBERTa_comment_sentiment_label'] == 'Negative')\n",
    "]\n",
    "\n",
    "# Defina os valores para cada caso\n",
    "values = ['Positivo', 'Tend√™ncia Positiva', 'Neutro', 'Tend√™ncia Positiva', 'Neutro', 'Tend√™ncia Negativa', 'Neutro', 'Tend√™ncia Negativa', 'Negativo']\n",
    "\n",
    "# Aplique as condi√ß√µes e os valores √† nova coluna\n",
    "Facebook_Comments['comment_sentiment_label'] = np.select(conditions, values)\n",
    "\n",
    "# Atribua o valor NaN √† vari√°vel 'comment_sentiment_label' nas linhas em que uma das colunas √© NaN\n",
    "Facebook_Comments['comment_sentiment_label'] = np.where((Facebook_Comments['TXRBSF_comment_sentiment_label'].isna()) |\n",
    "                                                                (Facebook_Comments['mDeBERTa_comment_sentiment_label'].isna()),\n",
    "                                                                np.nan,\n",
    "                                                                Facebook_Comments['comment_sentiment_label'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "hidden": true
   },
   "source": [
    "comment_sentiment_label_df = pd.DataFrame({\n",
    "    'n':Facebook_Comments['comment_sentiment_label'].value_counts(),\n",
    "    '%':round(Facebook_Comments['comment_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "})\n",
    "\n",
    "comment_sentiment_label_df.index.name = None\n",
    "comment_sentiment_label_df.reindex(['Positivo', 'Tend√™ncia Positiva', 'Neutro', 'Tend√™ncia Negativa', 'Negativo'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üî¥ Extra | Vari√°vel `post_text_Vodafone/MEO/NOS` e `comment_text_Vodafone/MEO/NOS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "hidden": true
   },
   "source": [
    "import re\n",
    "\n",
    "def create_binary_variable(text, names):\n",
    "    \"\"\"\n",
    "    Cria uma vari√°vel bin√°ria que √© 1 se algum dos nomes na lista aparecer no texto,\n",
    "    e 0 caso contr√°rio.\n",
    "    \"\"\"\n",
    "    # Verifica se os names √© do tipo lista ou string\n",
    "    # Caso seja string, converte numa lista \n",
    "    if isinstance(names, str):\n",
    "        names = [names]\n",
    "    \n",
    "    # Para todos os nomes poss√≠veis, verifica a sua exist√™ncia\n",
    "    for name in names:\n",
    "        # Use a fun√ß√£o `re.IGNORECASE` para tornar a pesquisa insens√≠vel a mai√∫sculas e min√∫sculas\n",
    "        if re.search(r'\\b' + re.escape(name) + r'\\b', text, re.IGNORECASE):\n",
    "            return 1\n",
    "    return 0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Teste da Fun√ß√£o\n",
    "textos = [\"A VoDafone tem bons servi√ßos!\", \"Vou mudar para a meo e para a nos\", \"N√≥s vamos √† praia e comando √© meu, vodafoneparedesdecoura\"]\n",
    "\n",
    "# Lista de nomes das operadoras\n",
    "operators = ['Vodafone', 'MEO', 'NOS', 'DIGI']\n",
    "\n",
    "for texto in textos:\n",
    "    print(\"\\033[1mTexto:\\033[0m\", texto)\n",
    "    for operator in operators:\n",
    "        print(f\"\\033[1m    {operator}:\\033[0m\", create_binary_variable(texto, operator))\n",
    "    print(\"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Lista de Possibilidades de formas como as operadoras podem ser referidas\n",
    "vodafone_expressoes = ['Vodafone', 'vodafoneparedesdecoura', 'vodafonehboportugal', 'vodafonept', \n",
    "                       'superta√ßavodafone', 'myvodafonept', 'vodafoneptparticipe', '30anosvodafone', \n",
    "                       'vodafonehappy', 'vodafonebusinessconference', 'vodafonerallydeportugal', 'vodafoneem', \n",
    "                       'vodafonetv', 'vodafonetvplay', 'minimaratonavodafone', 'vodafonecan√ß√£odoano']\n",
    "\n",
    "meo_expressoes = ['MEO', 'emmeopttvficaemcasa', 'modomeosdoeste', 'meocomo', 'nomeoptligameosurf2022', 'emmeoptmbeachcam', \n",
    "                  'meoficaemcasa', 'meosou', 'meosudoeste', 'meomaresvivas', 'emmeoptinstagram', 'meops', \n",
    "                  'emmeoptvemcomtudoomeovemcomtudo', 'meos√≥', 'meouma', 'emmeoptdesporto', 'meofui', 'meobox', \n",
    "                  'emmeoptdicaenergia', 'emmeoptprogramameos', 'meoa', 'ligameosurfjorge', 'emmeopt2021bymeo', \n",
    "                  'meono', 'meoou', 'meo', 'meoequipamentos', 'meogon√£o', 'meovisslaproericeira', 'emmeoptmmv24', \n",
    "                  'meovergonha', 'meowifi', 'meosmartphones', 'meopor', 'meoentretenimento', 'meohteam', 'meo4o', \n",
    "                  'meoestamos', 'meogo', 'meokalorama', 'emmeoptbandeiraverde', 'meose', 'ligameosurf', 'meoos', 'meoj√°',\n",
    "                  'festivalmeo', 'emmeoptsonytv4k', 'meonem', 'portugalmeo', 'meoenergia', 'embaixadorameo', 'meoobrigado',\n",
    "                  'meonao', 'meopodiam', 'ameo', 'meoque', 'meosabe', 'meoproportugal', 'meoser√°', 'emmeoptappstv', \n",
    "                  'emmeopttv', 'emmeoptsporttvfb', 'emmeoptsporttv', 'modomeosudoeste', 'meodesde', 'meopela', \n",
    "                  'emmeoptpacotesgaming', 'emmeopteuro', 'omeovemcomtudo', 'meoh√°', 'meoo', 'meopara', 'mymeo', \n",
    "                  'suportelojaonlinemeopt', 'meo5sentidos', 'tdtmelhorquemeo', 'nomeoptmaisnet', 'meoassim', 'alticemeo', \n",
    "                  'meoaltice', 'meo‚Ä¶', 'meocontactei', 'meon√£o', 'meo√©', 'meofiz', 'emmeoptregressoaulas21', 'meohifi', \n",
    "                  'emmeopteleven', 'meoe', 'emmeoptblnight', 'emmeoptbluefriday', 'meo5g', 'meocloud', 'emmeoptmeoskyshowtime', \n",
    "                  'meomas', 'embaixadormeo', 'emmeopttvcine', 'meos', 'wearemeokalorama', 'emmeoptmvod', 'meopois', 'meopt', \n",
    "                  'tdt√©melhorqueameo', 'meofeliz', 'campe√µesmeo', 'meoss√£o', 'meoparece', 'meoem', 'emmeoptmeovideochat', \n",
    "                  'meosem', 'meoripcurlpro', 'emmeoptpassatempos', 'emmeoptbtv', 'meodepois', 'meoinstagram', 'emmeoptmuda21']\n",
    "\n",
    "nos_expressoes = ['NOS', 'nosv√™', 'nos5g', 'palconos', 'nosplay', 'noscompra', 'nospt', 'nosadere', 'nosptnosstudios', \n",
    "                  'nosportugal', 'nosptkids', 'nosptadesaosporttvliganos', 'nossabe', 'nosm√∫sica', 'nos4a2', \n",
    "                  'bancadanos', 'nosno', 'liganos', 'nosalive', 'nosptnoskidsapp', 'n√£oteprendaspornos', '5gnos', \n",
    "                  'instagramcomliganos', 'nosprimaverasound', 'nosptappnoskidsfb', 'nosptnosplay', 'nosnosalive']\n",
    "\n",
    "digi_expressoes =['DIGI']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Aplicar a fun√ß√£o create_binary_variable aos conjuntos de dados Facebook_Posts e Facebook_Comments\n",
    "Facebook_Posts['post_text_Vodafone'] = Facebook_Posts['post_text'].progress_apply(lambda x: create_binary_variable(x, vodafone_expressoes))\n",
    "Facebook_Posts['post_text_MEO'] = Facebook_Posts['post_text'].progress_apply(lambda x: create_binary_variable(x, meo_expressoes))\n",
    "Facebook_Posts['post_text_NOS'] = Facebook_Posts['post_text'].progress_apply(lambda x: create_binary_variable(x, nos_expressoes))\n",
    "Facebook_Posts['post_text_DIGI'] = Facebook_Posts['post_text'].progress_apply(lambda x: create_binary_variable(x, digi_expressoes))\n",
    "\n",
    "Facebook_Comments['comment_text_Vodafone'] = Facebook_Comments['comment_text'].progress_apply(lambda x: create_binary_variable(x, vodafone_expressoes))\n",
    "Facebook_Comments['comment_text_MEO'] = Facebook_Comments['comment_text'].progress_apply(lambda x: create_binary_variable(x, meo_expressoes))\n",
    "Facebook_Comments['comment_text_NOS'] = Facebook_Comments['comment_text'].progress_apply(lambda x: create_binary_variable(x, nos_expressoes))\n",
    "Facebook_Comments['comment_text_DIGI'] = Facebook_Comments['comment_text'].progress_apply(lambda x: create_binary_variable(x, digi_expressoes))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Fun√ß√£o para calcular a %\n",
    "def calculate_percentage(value, total):\n",
    "    return round((value / total) * 100,1) if total != 0 else 0\n",
    "\n",
    "# Tabela de Frequ√™ncias das v√°riaveis relativas √† refer√™ncia do nome das operadoras\n",
    "df_post_text_operatores = pd.DataFrame({\n",
    "    '0 | N√£o (n)': Facebook_Posts[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']].apply(lambda x: x.value_counts()[0]),\n",
    "    '0 | N√£o (%)': Facebook_Posts[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']].apply(lambda x: calculate_percentage(x.value_counts()[0], len(x))),\n",
    "    '1 | Sim (n)': Facebook_Posts[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']].apply(lambda x: x.value_counts()[1]),\n",
    "    '1 | Sim (%)': Facebook_Posts[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']].apply(lambda x: calculate_percentage(x.value_counts()[1], len(x)))\n",
    "})\n",
    "\n",
    "df_post_text_operatores.index = [operators]\n",
    "df_post_text_operatores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Tabela de Frequ√™ncias das vari√°veis relativas √† refer√™ncia do nome das operadoras nos coment√°rios do Facebook\n",
    "df_comment_text_operators = pd.DataFrame({\n",
    "    '0 | N√£o (n)': Facebook_Comments[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']].apply(lambda x: x.value_counts()[0]),\n",
    "    '0 | N√£o (%)': Facebook_Comments[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']].apply(lambda x: calculate_percentage(x.value_counts()[0], len(x))),\n",
    "    '1 | Sim (n)': Facebook_Comments[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']].apply(lambda x: x.value_counts()[1]),\n",
    "    '1 | Sim (%)': Facebook_Comments[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']].apply(lambda x: calculate_percentage(x.value_counts()[1], len(x)))\n",
    "})\n",
    "\n",
    "df_comment_text_operators.index = [operators]\n",
    "df_comment_text_operators"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Matriz de % Cruzadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Simplificar a p√°gina da 'DIGI News'\n",
    "Facebook_Posts.loc[Facebook_Posts['page'].str.contains('DIGI'), 'page'] = 'DIGI News'\n",
    "Facebook_Comments.loc[Facebook_Comments['page'].str.contains('DIGI'), 'page'] = 'DIGI News'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Agrupar os dados pela vari√°vel 'page' e obter a contagem de posts que referem cada operadora em cada p√°gina\n",
    "page_posts_by_operator = Facebook_Posts.groupby('page')[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']].sum().reset_index()\n",
    "page_posts_by_operator = page_posts_by_operator.loc[page_posts_by_operator['page'].isin(['Vodafone', 'MEO', 'NOS', 'DIGI News'])]\n",
    "\n",
    "# Calcular a soma total de posts para cada p√°gina\n",
    "total_posts = Facebook_Posts[Facebook_Posts['page'].isin(['Vodafone', 'MEO', 'NOS', 'DIGI News'])].groupby('page').size()\n",
    "\n",
    "# Preencher a vari√°vel 'total_posts' com os valores correspondentes\n",
    "page_posts_by_operator['total_posts'] = page_posts_by_operator['page'].map(total_posts)\n",
    "\n",
    "# Calcular as porcentagens de refer√™ncias a cada operadora em rela√ß√£o ao total de posts em cada p√°gina\n",
    "for operator in ['Vodafone', 'MEO', 'NOS', 'DIGI']:\n",
    "    page_posts_by_operator['percentage_' + operator] = round((page_posts_by_operator['post_text_' + operator] / page_posts_by_operator['total_posts']) * 100, 1)\n",
    "\n",
    "# Page Posts Matrix (n)\n",
    "page_posts_matrix_n = page_posts_by_operator.set_index('page')[['post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI']]\n",
    "\n",
    "# Adicionar linha final com o total do n√∫mero de posts em cada coluna\n",
    "page_posts_matrix_n.loc['Total'] = page_posts_matrix_n.sum()\n",
    "\n",
    "page_posts_matrix_n.columns = ['Vodafone', 'MEO', 'NOS', 'DIGI News']\n",
    "page_posts_matrix_n = page_posts_matrix_n.reindex(['Vodafone', 'MEO', 'NOS', 'DIGI News', 'Total'])\n",
    "page_posts_matrix_n.columns.name = 'Texto c/'\n",
    "page_posts_matrix_n.index.name = 'P√°gina'\n",
    "\n",
    "# Selecionar apenas as colunas relevantes | Page Posts Matrix (%)\n",
    "page_posts_matrix = page_posts_by_operator.set_index('page')[['percentage_Vodafone', 'percentage_MEO', 'percentage_NOS', 'percentage_DIGI']]\n",
    "\n",
    "page_posts_matrix.columns = ['Vodafone', 'MEO', 'NOS', 'DIGI News']\n",
    "page_posts_matrix.columns.name = 'Texto c/'\n",
    "page_posts_matrix.index.name = 'P√°gina'\n",
    "page_posts_matrix = page_posts_matrix.reindex(['Vodafone', 'MEO', 'NOS', 'DIGI News'])\n",
    "\n",
    "display_side_by_side(page_posts_matrix_n, page_posts_matrix,\n",
    "                     super_title=\"Matriz de % Horizontais de Refer√™ncias a Operadoras em Posts <br></br>\",\n",
    "                     titles=[\"n\", \"%\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Agrupar os dados pela vari√°vel 'page' e obter a contagem de coment√°rios que referem cada operadora em cada p√°gina\n",
    "page_comments_by_operator = Facebook_Comments.groupby('page')[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']].sum().reset_index()\n",
    "page_comments_by_operator = page_comments_by_operator.loc[page_comments_by_operator['page'].isin(['Vodafone', 'MEO', 'NOS', 'DIGI News'])]\n",
    "\n",
    "# Calcular a soma total de coment√°rios para cada p√°gina\n",
    "total_comments = Facebook_Comments[Facebook_Comments['page'].isin(['Vodafone', 'MEO', 'NOS', 'DIGI News'])].groupby('page').size()\n",
    "\n",
    "# Preencher a vari√°vel 'total_comments' com os valores correspondentes\n",
    "page_comments_by_operator['total_comments'] = page_comments_by_operator['page'].map(total_comments)\n",
    "\n",
    "# Calcular as porcentagens de refer√™ncias a cada operadora em rela√ß√£o ao total de coment√°rios em cada p√°gina\n",
    "for operator in ['Vodafone', 'MEO', 'NOS', 'DIGI']:\n",
    "    page_comments_by_operator['percentage_' + operator] = round((page_comments_by_operator['comment_text_' + operator] / page_comments_by_operator['total_comments']) * 100,1)\n",
    "\n",
    "# Page Comments Matrix (n)\n",
    "page_comments_matrix_n = page_comments_by_operator.set_index('page')[['comment_text_Vodafone', 'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI']]\n",
    "\n",
    "# Adicionar linha final com o total do n√∫mero de coment√°rios em cada coluna\n",
    "page_comments_matrix_n.loc['Total'] = page_comments_matrix_n.sum()\n",
    "\n",
    "page_comments_matrix_n.columns = ['Vodafone', 'MEO', 'NOS', 'DIGI News']\n",
    "page_comments_matrix_n.columns.name = 'Texto c/'\n",
    "page_comments_matrix_n = page_comments_matrix_n.reindex(['Vodafone', 'MEO', 'NOS', 'DIGI News', 'Total'])\n",
    "page_comments_matrix_n.index.name = 'P√°gina'\n",
    "\n",
    "# Selecionar apenas as colunas relevantes | Page Comments Matrix (%)\n",
    "page_comments_matrix = page_comments_by_operator.set_index('page')[['percentage_Vodafone', 'percentage_MEO', 'percentage_NOS', 'percentage_DIGI']]\n",
    "page_comments_matrix.columns = ['Vodafone', 'MEO', 'NOS', 'DIGI News']\n",
    "page_comments_matrix.columns.name = 'Texto c/'\n",
    "page_comments_matrix = page_comments_matrix.reindex(['Vodafone', 'MEO', 'NOS', 'DIGI News'])\n",
    "page_comments_matrix.index.name = 'P√°gina'\n",
    "\n",
    "# Exibir a matriz de percentagens cruzadas\n",
    "display_side_by_side(page_comments_matrix_n, page_comments_matrix,\n",
    "                     super_title = \"Matriz de % Horizontais de Refer√™ncias a Operadoras em Coment√°rios <br></br>\",\n",
    "                     titles=[\"n\", \"%\"])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <a class='anchor' id='4.2'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: transparent; \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\n",
    "            border: 2px solid #A30000;\">\n",
    "    <center><h2 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: #A30000;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>Topic Analysis</b></h2></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## üü° HuggingFace [mDeBERTa-v3-base-xnli-multilingual-nli-2mil7](https://huggingface.co/MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Defina o pipeline de classifica√ß√£o\n",
    "model_path = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "moritz_classifier = pipeline(\"zero-shot-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "\n",
    "# Nota: para utlizar este modelo temos de dar moritz_classifier(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "\n",
    "# Defina as classes a classificar pelo modelo como t√≥picos\n",
    "candidate_labels = [\n",
    "    \n",
    "    # T√≥picos associados √†s Operadoras\n",
    "    \"Pacotes de Servi√ßos\", \"Cobertura\", \"Velocidade\", \"Pre√ßos\", \"Qualidade\", \"Atendimento ao Cliente\", \"Concorr√™ncia\", \"5G\",\n",
    "    \"Satisfa√ß√£o\", \"Rede\",  \"Fideliza√ß√£o\", \"Promo√ß√µes\", \"Seguran√ßa\", \"Plataformas Streaming\", \"Festivais\", \"Comunica√ß√£o\",\n",
    "    \n",
    "    # T√≥picos de Eventos Anuais\n",
    "    \"P√°scoa\", \"Natal\", \"Ano Novo\", \"Santos Populares\", \"Passatempo\", \"Black Friday\", \"Regresso √†s Aulas\",\n",
    "    \n",
    "    # T√≥picos Desportivos\n",
    "    \"Futebol\", \"Cristiano Ronaldo\", \"Surf\", \"Outros desportos\",    \n",
    "    \n",
    "    # T√≥picos Gerais\n",
    "    \"Problemas da Sociedade\", \"Cinema\", \"Filme/S√©rie\", \"Economia\", \"Sa√∫de\", \"Videojogo\", \"Pol√≠tica\", \"Emprego\", \n",
    "    \"Tecnologia\", \"Intelig√™ncia Artificial\", \"Ambiente\", \"Educa√ß√£o\", \"Cultura\", \"Ci√™ncia\", \"Arte\", \"Religi√£o\", \"Neg√≥cios\", \n",
    "    \"Sustentabilidade\", \"Moda\", \"Alimenta√ß√£o\", \"Viagens\", \"Fam√≠lia\", \"Guerra\", \"Pandemia\", \"Redes Sociais\", \"Sociedade\"\n",
    "    \n",
    "    # \"Internet\", \"Fibra\", \"TV\", \"Entretenimento\" -> Tirei por retirarem import√¢ncia a outros\n",
    "]\n",
    "\n",
    "# Defina a fun√ß√£o de classifica√ß√£o\n",
    "def mDeBERTa_classify_topics(texts):\n",
    "    outputs = moritz_classifier([texts], candidate_labels, multi_label=True, batch_size=8)\n",
    "    labels = [output['labels'] for output in outputs]\n",
    "    scores = [output['scores'] for output in outputs]\n",
    "    \n",
    "    # Guardar os 2 t√≥picos 'labels' e 'scores' que d√£o de resultado\n",
    "    return labels[0][:3], scores[0][:3]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "# Exemplo\n",
    "exemplo_topicos = \"A companhia ideal para ver todas as s√©ries vencedoras de Globos de Ouro \\\n",
    "e os filmes que queremos ver nomeados nos √ìscares.  #VodafoneTVPLAY\"\n",
    "\n",
    "# exemplo_topicos = \"Cara Vodafone, voc√™s sabiam que no mundo ocidental a taxa de suic√≠dio nos homens \\\n",
    "# √© de 3 a 4 vezes superior que nas mulheres? Quando ir√° aparecer uma campanha a defender e apoiar todos \\\n",
    "# aqueles homens que vos montam as antenas ou que trabalham horas a fi\"\n",
    "\n",
    "\n",
    "print(\"\\033[1mElementos da Pipeline:\\033[0m\")\n",
    "for key, value in moritz_classifier(exemplo_topicos, candidate_labels, multi_label=True).items():\n",
    "    print(f\"\\033[1m{key}:\\033[0m {value} \\n\")\n",
    "    \n",
    "print(\"\\033[1m-----------------------------------------------------\\033[0m\")\n",
    "print(\"\\n\\033[1mOutput da Fun√ß√£o mDeBERTa_classify_sentiment:\\033[0m \\n\", mDeBERTa_classify_topics(exemplo_topicos))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 03.05.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos posts [T√≥picos]\n",
    "mDeBERTa_posts_topics = Facebook_Posts['post_text'].progress_apply(lambda x: mDeBERTa_classify_topics([x]))\n",
    "top_labels, top_scores = zip(*[(label, score) for label, score in mDeBERTa_posts_topics])\n",
    "\n",
    "# Guardar os resultados em colunas separadas\n",
    "Facebook_Posts['mDeBERTa_post_topic_label_1'] = [label[0] for label in top_labels]\n",
    "Facebook_Posts['mDeBERTa_post_topic_score_1'] = [score[0] for score in top_scores]\n",
    "Facebook_Posts['mDeBERTa_post_topic_label_2'] = [label[1] for label in top_labels]\n",
    "Facebook_Posts['mDeBERTa_post_topic_score_2'] = [score[1] for score in top_scores]\n",
    "Facebook_Posts['mDeBERTa_post_topic_label_3'] = [label[2] for label in top_labels]\n",
    "Facebook_Posts['mDeBERTa_post_topic_score_3'] = [score[2] for score in top_scores]\n",
    "\n",
    "# Salvar as novas colunas como um arquivo CSV\n",
    "Facebook_Posts[['post_id', 'post_text',\n",
    "                'mDeBERTa_post_topic_label_1', 'mDeBERTa_post_topic_score_1',\n",
    "                'mDeBERTa_post_topic_label_2', 'mDeBERTa_post_topic_score_2',\n",
    "                'mDeBERTa_post_topic_label_3', 'mDeBERTa_post_topic_score_3']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_topics.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_post_topics = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_topics.csv', encoding='utf-8') \\\n",
    "                          .drop(['post_id','post_text'], axis=1)\n",
    "Facebook_Posts = Facebook_Posts.assign(**mDeBERTa_post_topics)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text', 'mDeBERTa_post_topic_label_1', 'mDeBERTa_post_topic_label_2', 'mDeBERTa_post_topic_label_3']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 04.05.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos comments [T√≥picos]\n",
    "mDeBERTa_comments_topics = Facebook_Comments['comment_text'].progress_apply(lambda x: mDeBERTa_classify_topics([x]))\n",
    "top_labels, top_scores = zip(*[(label, score) for label, score in mDeBERTa_comments_topics])\n",
    "\n",
    "# Guardar os resultados em colunas separadas\n",
    "Facebook_Comments['mDeBERTa_comment_topic_label_1'] = [label[0] for label in top_labels]\n",
    "Facebook_Comments['mDeBERTa_comment_topic_score_1'] = [score[0] for score in top_scores]\n",
    "Facebook_Comments['mDeBERTa_comment_topic_label_2'] = [label[1] for label in top_labels]\n",
    "Facebook_Comments['mDeBERTa_comment_topic_score_2'] = [score[1] for score in top_scores]\n",
    "Facebook_Comments['mDeBERTa_comment_topic_label_3'] = [label[2] for label in top_labels]\n",
    "Facebook_Comments['mDeBERTa_comment_topic_score_3'] = [score[2] for score in top_scores]\n",
    "\n",
    "# Salvar as novas colunas como um arquivo CSV\n",
    "Facebook_Comments[['comment_id', 'comment_text',\n",
    "                'mDeBERTa_comment_topic_label_1', 'mDeBERTa_comment_topic_score_1',\n",
    "                'mDeBERTa_comment_topic_label_2', 'mDeBERTa_comment_topic_score_2',\n",
    "                'mDeBERTa_comment_topic_label_3', 'mDeBERTa_comment_topic_score_3']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_topics.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_comment_topics = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_topics.csv', encoding='utf-8') \\\n",
    "                            .drop(['comment_id','comment_text'], axis=1)\n",
    "Facebook_Comments = Facebook_Comments.assign(**mDeBERTa_comment_topics)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Comments[['comment_text', 'mDeBERTa_comment_topic_label_1', 'mDeBERTa_comment_topic_label_2', \n",
    "                   'mDeBERTa_comment_topic_label_3']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### üëÅÔ∏è‚Äçüó®Ô∏èTeste do Algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# pd.set_option('display.max_rows', None) # Ver as linhas todas"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "# # ------------------------------ TESTE ------------------------------\n",
    "# # Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos posts [T√≥picos]\n",
    "# mDeBERTa_posts_topics = Facebook_Posts['post_text'].progress_apply(lambda x: mDeBERTa_classify_topics([x]))\n",
    "# top_labels, top_scores = zip(*[(label, score) for label, score in mDeBERTa_posts_topics])\n",
    "\n",
    "# pd.DataFrame({\n",
    "#     'post_text': Facebook_Posts['post_text'],\n",
    "#     'mDeBERTa_post_topic_label_1': [label[0] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_1' : [score[0] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_2': [label[1] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_2' : [score[1] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_3': [label[2] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_3' : [score[2] for score in top_scores],\n",
    "# })\n",
    "\n",
    "# # # Guardar os resultados em colunas separadas\n",
    "# # Facebook_Posts['mDeBERTa_post_topic_label_1'] = [label[0] for label in top_labels]\n",
    "# # Facebook_Posts['mDeBERTa_post_topic_score_1'] = [score[0] for score in top_scores]\n",
    "# # Facebook_Posts['mDeBERTa_post_topic_label_2'] = [label[1] for label in top_labels]\n",
    "# # Facebook_Posts['mDeBERTa_post_topic_score_2'] = [score[1] for score in top_scores]\n",
    "\n",
    "# # # Salvar as novas colunas como um arquivo CSV\n",
    "# # Facebook_Posts[['post_id', 'post_text',\n",
    "# #                 'mDeBERTa_post_topic_label_1', 'mDeBERTa_post_topic_score_1',\n",
    "# #                 'mDeBERTa_post_topic_label_2', 'mDeBERTa_post_topic_score_2']] \\\n",
    "# #     .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_topics.csv', encoding='utf-8', index=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "source": [
    "# # Define a semente (seed) para garantir reprodutibilidade\n",
    "# seed = 42\n",
    "\n",
    "# # Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos posts [T√≥picos]\n",
    "# mDeBERTa_posts_topics =  Facebook_Posts.sample(n=100, random_state=seed)['post_text'].progress_apply(lambda x: mDeBERTa_classify_topics([x]))\n",
    "# top_labels, top_scores = zip(*[(label, score) for label, score in mDeBERTa_posts_topics])\n",
    "\n",
    "# # Criar o DataFrame com os resultados\n",
    "# pd.DataFrame({\n",
    "#     'post_text': Facebook_Posts.sample(n=100, random_state=seed)['post_text'],\n",
    "#     'mDeBERTa_post_topic_label_1': [label[0] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_1' : [score[0] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_2': [label[1] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_2' : [score[1] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_3': [label[2] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_3' : [score[2] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_4': [label[3] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_4' : [score[3] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_5': [label[4] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_5' : [score[4] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_6': [label[5] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_6' : [score[5] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_7': [label[6] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_7' : [score[6] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_8': [label[7] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_8' : [score[7] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_9': [label[8] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_9' : [score[8] for score in top_scores],\n",
    "#     'mDeBERTa_post_topic_label_10': [label[9] for label in top_labels],\n",
    "#     'mDeBERTa_post_topic_score_10' : [score[9] for score in top_scores]\n",
    "# })"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## üñáÔ∏è Concatenar T√≥picos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üîµ Extra | Carece de Resposta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "source": [
    "############################################ [Carecia de Resposta]\n",
    "\n",
    "# Defina o pipeline de classifica√ß√£o\n",
    "model_path = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "moritz_classifier = pipeline(\"zero-shot-classification\", model=model_path, tokenizer=model_path, device=device)\n",
    "candidate_labels = [\"Carece de Resposta\", \"N√£o Carece de Resposta\"]\n",
    "\n",
    "# Defina a fun√ß√£o de classifica√ß√£o\n",
    "def mDeBERTa_classify_CR_NCR(texts):\n",
    "    outputs = moritz_classifier([texts], candidate_labels, multi_label=False, batch_size=8)\n",
    "    labels = [output['labels'] for output in outputs]\n",
    "    scores = [output['scores'] for output in outputs]\n",
    "    \n",
    "    # Guardar os 2 t√≥picos 'labels' e 'scores' que d√£o de resultado\n",
    "    return labels[0][0], scores[0][0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Exemplo\n",
    "exemplo_CR_NCR = 'For√ßa FCP'\n",
    "# exemplo_CR_NCR = 'Servi√ßo de Baixa Qualidade!'\n",
    "\n",
    "print(\"\\033[1mTexto Exemplo:\\033[0m\", exemplo_CR_NCR)\n",
    "print(\"\\n\\033[1mOutput da Fun√ß√£o mDeBERTa_classify_CR_NCR:\\033[0m \\n\", mDeBERTa_classify_CR_NCR(exemplo_CR_NCR))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 05.05.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos posts [Carece de Resposta]\n",
    "mDeBERTa_posts_CR = Facebook_Posts['post_text'].progress_apply(lambda x: mDeBERTa_classify_CR_NCR([x]))\n",
    "CR_labels, CR_scores = zip(*[(label, score) for label, score in mDeBERTa_posts_CR])\n",
    "\n",
    "# Guardar os resultados em colunas separadas\n",
    "Facebook_Posts['mDeBERTa_post_CR_label'] = CR_labels\n",
    "Facebook_Posts['mDeBERTa_post_CR_score'] = CR_scores\n",
    "\n",
    "# Salvar as novas colunas como um arquivo CSV\n",
    "Facebook_Posts[['post_id', 'post_text',\n",
    "                'mDeBERTa_post_CR_label', 'mDeBERTa_post_CR_score']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_CR.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_post_CR = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_post_CR.csv', encoding='utf-8') \\\n",
    "                          .drop(['post_id','post_text'], axis=1)\n",
    "Facebook_Posts = Facebook_Posts.assign(**mDeBERTa_post_CR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Posts[['post_text', 'mDeBERTa_post_CR_label', 'mDeBERTa_post_CR_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Dataframe com a contagem (n) e percentagem (%) dos casos que Carecem ou N√£o Resposta\n",
    "CR_posts_df = pd.DataFrame({\n",
    "    'n':Facebook_Posts['mDeBERTa_post_CR_label'].value_counts(),\n",
    "    '%':round(Facebook_Posts['mDeBERTa_post_CR_label'].value_counts(normalize=True) * 100,1)\n",
    "})\n",
    "CR_posts_df.index.name = None\n",
    "\n",
    "# Filtra as linhas em que o mDeBERTa_post_CR_label √© \"Carece de Resposta\" e o mDeBERTa_post_CR_score √© superior a 0.9\n",
    "filtered_posts = Facebook_Posts[(Facebook_Posts['mDeBERTa_post_CR_label'] == 'Carece de Resposta') &\n",
    "                                (Facebook_Posts['mDeBERTa_post_CR_score'] > 0.8)]\n",
    "\n",
    "# Cria um DataFrame com a contagem e a porcentagem de posts que carecem de resposta\n",
    "CR_posts_threshold_df = pd.DataFrame({\n",
    "    'n': filtered_posts['mDeBERTa_post_CR_label'].value_counts(),\n",
    "    '%': round((filtered_posts['mDeBERTa_post_CR_label'].value_counts().sum() / len(Facebook_Posts) * 100), 1)\n",
    "})\n",
    "\n",
    "# Cria um DataFrame com a contagem e a porcentagem de posts que n√£o carecem de resposta\n",
    "not_CR_posts_threshold_df = pd.DataFrame({\n",
    "    'n': len(Facebook_Posts) - filtered_posts['mDeBERTa_post_CR_label'].value_counts().sum(),\n",
    "    '%': round((len(Facebook_Posts) - filtered_posts['mDeBERTa_post_CR_label'].value_counts().sum()) / len(Facebook_Posts) * 100, 1)\n",
    "}, index=['N√£o Carece de Resposta'])\n",
    "\n",
    "# Concatena os dois DataFrames\n",
    "CR_posts_threshold_df = pd.concat([CR_posts_threshold_df, not_CR_posts_threshold_df])\n",
    "\n",
    "display_side_by_side(CR_posts_df.sort_index() ,CR_posts_threshold_df,\n",
    "                     titles=['Original', 'Threshold = 0.9'],\n",
    "                     super_title= 'Carece de Resposta? <br><br><br>')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# ------------------------------ J√° corri - 05.05.2024 ------------------------------\n",
    "# Aplicar o mDeBERTa-v3-base-mnli-xnli ao dataset dos comments [Carece de Resposta]\n",
    "mDeBERTa_comments_CR = Facebook_Comments['comment_text'].progress_apply(lambda x: mDeBERTa_classify_CR_NCR([x]))\n",
    "CR_labels, CR_scores = zip(*[(label, score) for label, score in mDeBERTa_comments_CR])\n",
    "\n",
    "# Guardar os resultados em colunas separadas\n",
    "Facebook_Comments['mDeBERTa_comment_CR_label'] = CR_labels\n",
    "Facebook_Comments['mDeBERTa_comment_CR_score'] = CR_scores\n",
    "\n",
    "# Salvar as novas colunas como um arquivo CSV\n",
    "Facebook_Comments[['comment_id', 'comment_text',\n",
    "                   'mDeBERTa_comment_CR_label', 'mDeBERTa_comment_CR_score']] \\\n",
    "    .to_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_CR.csv', encoding='utf-8', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Ler o arquivo CSV e adicionar a coluna ao DataFrame original\n",
    "mDeBERTa_comment_CR = pd.read_csv('Datasets_Vodafone/TextMining/mDeBERTa_comment_CR.csv', encoding='utf-8') \\\n",
    "                        .drop(['comment_id','comment_text'], axis=1)\n",
    "Facebook_Comments = Facebook_Comments.assign(**mDeBERTa_comment_CR)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "hidden": true
   },
   "source": [
    "Facebook_Comments[['post_text', 'mDeBERTa_comment_CR_label', 'mDeBERTa_comment_CR_score']].sample(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Dataframe com a contagem (n) e percentagem (%) dos casos que Carecem ou N√£o Resposta\n",
    "CR_comments_df = pd.DataFrame({\n",
    "    'n':Facebook_Comments['mDeBERTa_comment_CR_label'].value_counts(),\n",
    "    '%':round(Facebook_Comments['mDeBERTa_comment_CR_label'].value_counts(normalize=True) * 100,1)\n",
    "})\n",
    "CR_comments_df.index.name = None\n",
    "\n",
    "# Filtra as linhas em que o mDeBERTa_comment_CR_label √© \"Carece de Resposta\" e o mDeBERTa_comment_CR_score √© superior a 0.9\n",
    "filtered_comments = Facebook_Comments[(Facebook_Comments['mDeBERTa_comment_CR_label'] == 'Carece de Resposta') &\n",
    "                                      (Facebook_Comments['mDeBERTa_comment_CR_score'] > 0.8)]\n",
    "\n",
    "# Cria um DataFrame com a contagem e a porcentagem de coment√°rios que carecem de resposta\n",
    "CR_comments_threshold_df = pd.DataFrame({\n",
    "    'n': filtered_comments['mDeBERTa_comment_CR_label'].value_counts(),\n",
    "    '%': round((filtered_comments['mDeBERTa_comment_CR_label'].value_counts().sum() / len(Facebook_Comments) * 100), 1)\n",
    "})\n",
    "\n",
    "# Cria um DataFrame com a contagem e a porcentagem de coment√°rios que n√£o carecem de resposta\n",
    "not_CR_comments_threshold_df = pd.DataFrame({\n",
    "    'n': len(Facebook_Comments) - filtered_comments['mDeBERTa_comment_CR_label'].value_counts().sum(),\n",
    "    '%': round((len(Facebook_Comments) - filtered_comments['mDeBERTa_comment_CR_label'].value_counts().sum()) / len(Facebook_Comments) * 100, 1)\n",
    "}, index=['N√£o Carece de Resposta'])\n",
    "\n",
    "# Concatena os dois DataFrames\n",
    "CR_comments_threshold_df = pd.concat([CR_comments_threshold_df, not_CR_comments_threshold_df])\n",
    "\n",
    "display_side_by_side(CR_comments_df.sort_index() ,CR_comments_threshold_df,\n",
    "                     titles=['Original', 'Threshold = 0.8'],\n",
    "                     super_title= 'Carece de Resposta? <br><br><br>')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### üíæ Guardar Bases de Dados com An√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Colunas repetidas entre datasets\n",
    "print(list(Facebook_Posts.columns.intersection(Facebook_Comments.columns)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Compilar as bases de dados\n",
    "Facebook_Posts_Comments = pd.merge(Facebook_Posts, \n",
    "                                   Facebook_Comments.drop(['post_text', 'post_date', 'post_reactions','post_comments', \n",
    "                                                           'post_shares', 'post_day', 'post_month', 'post_year', \n",
    "                                                           'post_hour', 'post_link', 'page'], axis=1), \n",
    "                                   how = 'right', on = ['post_id'])\n",
    "\n",
    "Facebook_PCU_Analysis = pd.merge(Facebook_Posts_Comments, \n",
    "                                 Facebook_Users.drop(['user_name'], axis=1), how='left', on=['user_link'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Reorganizar as colunas\n",
    "colunas_organizadas = [\n",
    "    # Vari√°veis dos Posts\n",
    "    'page', 'post_id', 'post_link', 'post_date', 'post_day', 'post_month', 'post_year', 'post_hour',\n",
    "    'post_reactions', 'post_comments', 'post_shares', 'post_text', 'post_text_clean', 'post_language', \n",
    "    'post_sentiment_label','TXRBSF_post_sentiment_label', 'TXRBSF_post_sentiment_score', \n",
    "    'mDeBERTa_post_sentiment_label', 'mDeBERTa_post_sentiment_score', \n",
    "    'post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI',\n",
    "    'mDeBERTa_post_topic_label_1', 'mDeBERTa_post_topic_score_1', 'mDeBERTa_post_topic_label_2',\n",
    "    'mDeBERTa_post_topic_score_2', 'mDeBERTa_post_topic_label_3', 'mDeBERTa_post_topic_score_3',\n",
    "    'mDeBERTa_post_CR_label', 'mDeBERTa_post_CR_score',\n",
    "    \n",
    "    # Vari√°veis dos Comments\n",
    "    'comment_id', 'comment_link', 'comment_date', 'comment_day_ago', 'comment_reactions',\n",
    "    'comment_num_responses', 'comment_operator_responded', 'comment_text', 'comment_text_clean', 'comment_language',\n",
    "    'comment_sentiment_label', 'TXRBSF_comment_sentiment_label', 'TXRBSF_comment_sentiment_score',\n",
    "    'mDeBERTa_comment_sentiment_label', 'mDeBERTa_comment_sentiment_score', 'comment_text_Vodafone',\n",
    "    'comment_text_MEO', 'comment_text_NOS', 'comment_text_DIGI',\n",
    "    'mDeBERTa_comment_topic_label_1', 'mDeBERTa_comment_topic_score_1',\n",
    "    'mDeBERTa_comment_topic_label_2', 'mDeBERTa_comment_topic_score_2',\n",
    "    'mDeBERTa_comment_topic_label_3', 'mDeBERTa_comment_topic_score_3',\n",
    "    'mDeBERTa_comment_CR_label', 'mDeBERTa_comment_CR_score',\n",
    "\n",
    "    # Vari√°veis dos Users\n",
    "    'user_name', 'user_link', 'user_current_city', 'user_hometown', 'user_freguesia', 'user_concelho',\n",
    "    'user_distrito', 'user_pais', 'user_city_not_portugal', 'user_predicted_genre'\n",
    "]\n",
    "\n",
    "Facebook_PCU_Analysis = Facebook_PCU_Analysis[colunas_organizadas]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```Python\n",
    "# Guardar o dataset final com toda a an√°lise em formato .txt\n",
    "Facebook_PCU_Analysis.to_csv('Datasets_Vodafone/Facebook_PCU_Analysis.txt', sep='\\t', index=False, encoding='utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6350a110a50140f2ae2c2f74080fe204",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# <a class='anchor' id='5'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>5 | Evaluation</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Import da Base de Dados com An√°lise\n",
    "Facebook_PCU_Analysis = pd.read_csv('Datasets_Vodafone/Facebook_PCU_Analysis.txt', sep='\\t', encoding='utf-8')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Criar o Facebook_Posts_Analysis\n",
    "Facebook_Posts_Analysis = Facebook_PCU_Analysis.groupby('post_id').first().reset_index()\n",
    "\n",
    "# Selecionar apenas as colunas relevantes para o Facebook_Posts_Analysis\n",
    "Facebook_Posts_Analysis = Facebook_Posts_Analysis[['post_id', 'page', 'post_link', 'post_date', 'post_day', 'post_month', \n",
    "                                                   'post_year', 'post_hour', 'post_reactions', 'post_comments', 'post_shares',\n",
    "                                                   'post_text', 'post_text_clean', 'post_language', 'post_sentiment_label',\n",
    "                                                   'TXRBSF_post_sentiment_label', 'TXRBSF_post_sentiment_score', \n",
    "                                                   'mDeBERTa_post_sentiment_label', 'mDeBERTa_post_sentiment_score', \n",
    "                                                   'post_text_Vodafone', 'post_text_MEO', 'post_text_NOS', 'post_text_DIGI', \n",
    "                                                   'mDeBERTa_post_topic_label_1', 'mDeBERTa_post_topic_score_1', \n",
    "                                                   'mDeBERTa_post_topic_label_2', 'mDeBERTa_post_topic_score_2', \n",
    "                                                   'mDeBERTa_post_topic_label_3', 'mDeBERTa_post_topic_score_3', \n",
    "                                                   'mDeBERTa_post_CR_label', 'mDeBERTa_post_CR_score']]\n",
    "\n",
    "# Criar o Facebook_Comments_Analysis\n",
    "Facebook_Comments_Analysis = Facebook_PCU_Analysis.groupby('comment_id').first().reset_index()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Divis√£o em Conjunto Treino/Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "# Dividir a amostra em duas partes: 60% para treinamento e 40% para teste\n",
    "dtf_train, dtf_test = model_selection.train_test_split(Facebook_Comments_Analysis, test_size=0.4, random_state=123)\n",
    "\n",
    "# Tamanho dos dataset de Treino e Teste\n",
    "print(\"\\033[1mConjunto de Treino:\\033[0m\", dtf_train.shape)\n",
    "print(\"\\033[1mConjunto de Teste:\\033[0m\", dtf_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è An√°lise dos Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥üü°üü¢ Sentimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Criar um DataFrame com as contagens e percentagens das labels do treino e teste\n",
    "labels_df = pd.DataFrame()\n",
    "\n",
    "# Contagens e percentagens das labels do treino para post_sentiment_label\n",
    "# labels_df['Treino post_sentiment_label (n)'] = dtf_train['post_sentiment_label'].value_counts()\n",
    "labels_df['Treino post_sentiment_label (%)'] = round(dtf_train['post_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "# Contagens e percentagens das labels do teste para post_sentiment_label\n",
    "# labels_df['Teste post_sentiment_label (n)'] = dtf_test['post_sentiment_label'].value_counts()\n",
    "labels_df['Teste post_sentiment_label (%)'] = round(dtf_test['post_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "labels_df[' '] = ['', '', '', '', '']\n",
    "\n",
    "# Contagens e percentagens das labels do treino para comment_sentiment_label\n",
    "# labels_df['Treino comment_sentiment_label (n)'] = dtf_train['comment_sentiment_label'].value_counts()\n",
    "labels_df['Treino comment_sentiment_label (%)'] = round(dtf_train['comment_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "# Contagens e percentagens das labels do teste para comment_sentiment_label\n",
    "# labels_df['Teste comment_sentiment_label (n)'] = dtf_test['comment_sentiment_label'].value_counts()\n",
    "labels_df['Teste comment_sentiment_label (%)'] = round(dtf_test['comment_sentiment_label'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "labels_df.T"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "# Criar um gr√°fico de barras com as contagens das labels do treino e teste lado a lado\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "\n",
    "# Criar uma lista de 4 cores diferentes para treino e teste\n",
    "colors = ['#5B1C89', '#CE9BF3', '#094A9E', '#5D9CEE']\n",
    "\n",
    "# Ordenar as barras por valor\n",
    "labels_df.plot(kind='bar', \n",
    "               color=colors, \n",
    "               ax=ax)\n",
    "\n",
    "# Adicionar r√≥tulos dos eixos e t√≠tulo\n",
    "plt.xlabel('')\n",
    "plt.ylabel('n\\n')\n",
    "plt.title('Contagem de Labels - Treino vs Teste \\n', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Adicionar legenda\n",
    "legend_properties = {'weight':'bold', 'size':'14'}\n",
    "ax.legend(['Treino Posts', 'Teste Post', 'Treino Comments', 'Teste Comments'], \n",
    "          title='Conjuntos', title_fontproperties=legend_properties, fontsize='12', loc='upper right',frameon=False)\n",
    "\n",
    "# Ajustar os r√≥tulos dos nomes das labels no eixo x\n",
    "plt.xticks(rotation=0, ha='center', rotation_mode='anchor')\n",
    "\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/8_Barras Resposta da Operadora.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Teste Estat√≠stico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $S/T$ os $Sentimentos$ e $T√≥picos$ em estudo e $D_1$ o *dataset* de treino e $D_2$ de teste:\n",
    "\n",
    "- $X = D_1^{\\;S/T}$: Sentimentos/T√≥picos no dataset de **Treino**\n",
    "\n",
    "- $Y = D_2^{\\;S/T}$  Sentimentos/T√≥picos no dataset de **Teste**\n",
    "\n",
    "#### Hip√≥teses em teste\n",
    "\n",
    "- $H_0:$ O resultados do treino **s√£o independentes** dos de teste [$D_1^{S/T}\\; \\neq \\;D_2^{S/T}$]\n",
    "\n",
    "- $H_1:$ Existe relacionamento entre os resultados de treino e teste [$D_1^{S/T}\\; = \\;D_2^{S/T}$]\n",
    "\n",
    "ou, teoricamente,\n",
    "\n",
    "- $H_0:\\forall(i, j) \\in\\{1: r\\} \\times\\{1: c\\}: p_{i j}=p_{i .} \\times p_{. j}$\n",
    "\n",
    "- $H_1: \\exists(i, j) \\in\\{1: r\\} \\times\\{1: c\\}: p_{i j} \\neq p_{i .} \\times p_{\\cdot j}$\n",
    "\n",
    "---\n",
    "\n",
    "#### Teste Qui-Quadrado\n",
    "\n",
    "**Estat√≠stica de teste**\n",
    "\n",
    "$$E T=\\sum_{i=1}^r \\sum_{j=1}^c \\frac{\\left(o_{i j}-e_{i j}\\right)^2}{e_{i j}} \\; \\dot\\sim \\; \\chi_{(r-1)(c-1)}^2$$\n",
    "\n",
    "Sendo ambas vari√°veis fatores, o estudo do relacionamento entre elas ser√° feito atrav√©s da an√°lise do respetivo\n",
    "cruzamento (tabela de conting√™ncia), com a subsequente aplica√ß√£o do **Teste Qui-quadrado**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Crie um DataFrame com as contagens das labels do treino e teste\n",
    "chi2_df = pd.DataFrame()\n",
    "\n",
    "# Contagens das labels do treino para comment_sentiment_label\n",
    "chi2_df['Treino comment_sentiment_label'] = dtf_train['comment_sentiment_label'].dropna().value_counts()\n",
    "\n",
    "# Contagens das labels do teste para comment_sentiment_label\n",
    "chi2_df['Teste comment_sentiment_label'] = dtf_test['comment_sentiment_label'].dropna().value_counts()\n",
    "\n",
    "# Crie uma tabela de conting√™ncia com as contagens das labels do treino e teste\n",
    "contingency_table = chi2_df.values\n",
    "\n",
    "# Realize o teste qui-quadrado\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table, correction=False)\n",
    "\n",
    "print('\\033[1mEstat√≠stica qui-quadrado:\\033[0m {:.4f}'.format(chi2))\n",
    "print('\\033[1mValor-p:\\033[0m {:.4f}'.format(p))\n",
    "print('\\033[1mGraus de liberdade:\\033[0m {}'.format(dof))\n",
    "print('\\033[1mTabela de conting√™ncia esperada:\\033[0m\\n')\n",
    "pd.DataFrame(expected, index=chi2_df.index , columns=['Treino', 'Teste'])\n",
    "\n",
    "\n",
    "# No exemplo acima, a estat√≠stica qui-quadrado √© igual a 6,83, o valor-p √© igual a 0,033 e os graus de liberdade = 2. \n",
    "# Como o valor-p √© menor que o n√≠vel de signific√¢ncia usual de 0,05, voc√™ pode rejeitar a hip√≥tese nula de que as \n",
    "# distribui√ß√µes das labels do treino e teste s√£o iguais. Isso significa que h√° evid√™ncias estat√≠sticas de que as \n",
    "# distribui√ß√µes das labels do treino e teste s√£o diferentes."
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Como $p-value = 0.0079 < \\alpha$ de refer√™ncia ($\\alpha=0.05$), ent√£o n√£o se rejeita $H_0$.\n",
    "\n",
    "Ou seja, **existe** evid√™ncia estat√≠stica que revele associa√ß√£o o treino e o teste.\n",
    "\n",
    "- Isto significa que existem diverg√™ncias significativas entre as frequ√™ncias observadas e as frequ√™ncias esperadas (ou seja, o que estamos a observar numa situa√ß√£o de independ√™ncia). \n",
    "\n",
    "--- \n",
    "\n",
    "Assim, o modelo mostra-se capaz de classificar os dados sem enviesar os resultados.\n",
    "\n",
    "[**Nota:** $\\alpha = 0.05$ √© com uma confian√ßa de $95\\%$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Calcule as ECDFs\n",
    "ecdf_train = np.arange(1, len(dtf_train['comment_sentiment_label'].dropna().value_counts()) + 1) / len(dtf_train['comment_sentiment_label'].dropna().value_counts())\n",
    "ecdf_test = np.arange(1, len(dtf_test['comment_sentiment_label'].dropna().value_counts()) + 1) / len(dtf_test['comment_sentiment_label'].dropna().value_counts())\n",
    "\n",
    "# Realize o teste de Kolmogorov-Smirnov\n",
    "statistic, p_value = ks_2samp(dtf_train['comment_sentiment_label'].dropna().value_counts(), dtf_test['comment_sentiment_label'].dropna().value_counts())\n",
    "\n",
    "# Exiba os resultados\n",
    "print(\"\\033[1mEstat√≠stica D:\\033[0m\", statistic)\n",
    "print(\"\\033[1mValor-p:\\033[0m\", p_value)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Se o $p-value = 1$, isso indica que n√£o h√° evid√™ncias suficientes para rejeitar a hip√≥tese nula de que as distribui√ß√µes dos sentimentos nos conjuntos de treino e teste s√£o as mesmas. Nesse caso, voc√™ n√£o encontrou diferen√ßas significativas entre as distribui√ß√µes.\n",
    "\n",
    "> Portanto, com base no resultado do **Teste de Kolmogorov-Smirnov**, voc√™ pode concluir que as distribui√ß√µes de sentimentos nos conjuntos de treino e teste s√£o semelhantes. Isso sugere que o modelo treinado pode estar generalizando bem para os dados de teste, em termos de distribui√ß√£o de sentimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¥üîµ‚ö´ T√≥picos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Criar um DataFrame com as contagens e percentagens das labels do treino e teste\n",
    "labels_df = pd.DataFrame()\n",
    "\n",
    "# Contagens e percentagens das labels do treino para mDeBERTa_post_topic_label_1\n",
    "# labels_df['Treino mDeBERTa_post_topic_label_1 (n)'] = dtf_train['mDeBERTa_post_topic_label_1'].value_counts()\n",
    "labels_df['Treino | post_topic_label_1 (%)'] = round(dtf_train['mDeBERTa_post_topic_label_1'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "# Contagens e percentagens das labels do teste para mDeBERTa_post_topic_label_1\n",
    "# labels_df['Teste mDeBERTa_post_topic_label_1 (n)'] = dtf_test['mDeBERTa_post_topic_label_1'].value_counts()\n",
    "labels_df['Teste | post_topic_label_1 (%)'] = round(dtf_test['mDeBERTa_post_topic_label_1'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "# Contagens e percentagens das labels do treino para mDeBERTa_comment_topic_label_1\n",
    "# labels_df['Treino mDeBERTa_comment_topic_label_1 (n)'] = dtf_train['mDeBERTa_comment_topic_label_1'].value_counts()\n",
    "labels_df['Treino | comment_topic_label_1 (%)'] = round(dtf_train['mDeBERTa_comment_topic_label_1'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "# Contagens e percentagens das labels do teste para mDeBERTa_comment_topic_label_1\n",
    "# labels_df['Teste mDeBERTa_comment_topic_label_1 (n)'] = dtf_test['mDeBERTa_comment_topic_label_1'].value_counts()\n",
    "labels_df['Teste | comment_topic_label_1 (%)'] = round(dtf_test['mDeBERTa_comment_topic_label_1'].value_counts(normalize=True) * 100,1)\n",
    "\n",
    "labels_df.index.name = None\n",
    "labels_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import matplotlib.colors as mcolors\n",
    "# Criar um gr√°fico de barras com as contagens das labels do treino e teste lado a lado\n",
    "fig, ax = plt.subplots(figsize=(15,6))\n",
    "\n",
    "# Criar uma lista de 4 cores diferentes para treino e teste\n",
    "colors = ['#5B1C89', '#CE9BF3', '#094A9E', '#5D9CEE']\n",
    "\n",
    "# Ordenar as barras por valor\n",
    "# labels_df.sort_values(by=['Treino | post_topic_label_1 (%)', 'Teste | post_topic_label_1 (%)'], ascending=False).plot(kind='bar', color=colors, ax=ax)\n",
    "labels_df.sort_values(by=['Treino | post_topic_label_1 (%)', 'Teste | post_topic_label_1 (%)'], ascending=False).head(10).plot(kind='bar', color=colors, ax=ax)\n",
    "\n",
    "# Adicionar r√≥tulos dos eixos e t√≠tulo\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Contagem\\n', fontweight='bold')\n",
    "plt.ylim(0, 20)\n",
    "# plt.title('Contagem de Labels - Treino vs Teste', fontsize=18, fontweight='bold')\n",
    "plt.title('Top 10 Labels - Treino vs Teste', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Adicionar legenda\n",
    "legend_properties = {'weight':'bold', 'size':'14'}\n",
    "ax.legend(['Treino Posts', 'Teste Post', 'Treino Comments', 'Teste Comments'], \n",
    "          title='Conjuntos', title_fontproperties=legend_properties, fontsize='12', loc='upper right',frameon=False)\n",
    "\n",
    "# Ajustar os r√≥tulos dos nomes das labels no eixo x\n",
    "plt.xticks(rotation=0, ha='center', rotation_mode='anchor')\n",
    "# plt.xticks(rotation=25, ha='right', rotation_mode='anchor')\n",
    "\n",
    "sns.despine(top=True, right=True)\n",
    "plt.tight_layout()\n",
    "# fig.savefig(\"Relat√≥rio/Gr√°ficos/8_Barras Resposta da Operadora.svg\",format='svg',dpi=1200)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî¢ Teste Estat√≠stico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste Qui$^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Crie um DataFrame com as contagens das labels do treino e teste\n",
    "chi2_df = pd.DataFrame()\n",
    "\n",
    "# Contagens das labels do treino para mDeBERTa_comment_topic_label_1\n",
    "chi2_df['Treino mDeBERTa_comment_topic_label_1'] = dtf_train['mDeBERTa_comment_topic_label_1'].dropna().value_counts()\n",
    "\n",
    "# Contagens das labels do teste para mDeBERTa_comment_topic_label_1\n",
    "chi2_df['Teste mDeBERTa_comment_topic_label_1'] = dtf_test['mDeBERTa_comment_topic_label_1'].dropna().value_counts()\n",
    "\n",
    "# Crie uma tabela de conting√™ncia com as contagens das labels do treino e teste\n",
    "contingency_table = chi2_df.values\n",
    "\n",
    "# Realize o teste qui-quadrado\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency_table, correction=False)\n",
    "\n",
    "print('\\033[1mEstat√≠stica qui-quadrado:\\033[0m {:.4f}'.format(chi2))\n",
    "print('\\033[1mValor-p:\\033[0m {:.4f}'.format(p))\n",
    "print('\\033[1mGraus de liberdade:\\033[0m {}'.format(dof))\n",
    "print('\\033[1mTabela de conting√™ncia esperada:\\033[0m\\n')\n",
    "pd.DataFrame(expected, index=chi2_df.index , columns=['Treino', 'Teste'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $=$ ao do **Sentimento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste de Kolmogorov-Smirnov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Calcule as ECDFs\n",
    "ecdf_train = np.arange(1, len(dtf_train['mDeBERTa_comment_topic_label_1'].dropna().value_counts()) + 1) / len(dtf_train['mDeBERTa_comment_topic_label_1'].dropna().value_counts())\n",
    "ecdf_test = np.arange(1, len(dtf_test['mDeBERTa_comment_topic_label_1'].dropna().value_counts()) + 1) / len(dtf_test['mDeBERTa_comment_topic_label_1'].dropna().value_counts())\n",
    "\n",
    "# Realize o teste de Kolmogorov-Smirnov\n",
    "statistic, p_value = ks_2samp(dtf_train['mDeBERTa_comment_topic_label_1'].dropna().value_counts(), dtf_test['mDeBERTa_comment_topic_label_1'].dropna().value_counts())\n",
    "\n",
    "# Exiba os resultados\n",
    "print(\"\\033[1mEstat√≠stica D:\\033[0m\", statistic)\n",
    "print(\"\\033[1mValor-p:\\033[0m\", p_value)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> $=$ ao do **Sentimento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a3e6945e31a4477a8f6e992089ea0a4c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='6'></a>\n",
    "<br>\n",
    "<style>\n",
    "@import url('https://fonts.cdnfonts.com/css/avenir-next-lt-pro?styles=29974');\n",
    "</style>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right,#A30000, #F91701); \n",
    "            padding: 10px; color: white; border-radius: 300px; text-align: center;\">\n",
    "    <center><h1 style=\"margin-left: 120px;margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 34px; font-family: 'Avenir Next LT Pro', sans-serif;\"><b>6 | Deployment</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * üìä Dashboard [pcfad_chatmeter_streamlit.py](./Dashboard/pcfad_chatmeter_streamlit.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "source": [
    "print(Facebook_Comments_Analysis.info())\n",
    "Facebook_PCU_Analysis.columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "Facebook_PCU_Analysis['user_pais'].value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Colunas a eliminar\n",
    "colunas_del = [\n",
    "    'post_day', 'post_month', 'post_year', 'post_hour', 'post_language',\n",
    "    'comment_date', 'comment_day_ago', 'comment_language', 'user_current_city', 'user_hometown', 'user_city_not_portugal'\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Eliminar colunas desnecess√°rias\n",
    "# Facebook_PCU_Analysis = Facebook_PCU_Analysis.drop(columns=colunas_del)\n",
    "\n",
    "# Guardar o dataset resultante como um arquivo CSV\n",
    "# Facebook_PCU_Analysis.to_csv('Datasets_Vodafone/Facebook_PCU_Analysis.csv', sep='|', encoding='utf-8', index=False)\n",
    "\n",
    "# Guardar o dataset resultante como um arquivo Pickle\n",
    "# Facebook_PCU_Analysis.to_pickle('Datasets_Vodafone/Facebook_PCU_Analysis.pkl')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "# Dividir o DataFrame em partes menores\n",
    "num_parts = 3  # Defina o n√∫mero de partes\n",
    "chunk_size = len(Facebook_PCU_Analysis) // num_parts\n",
    "chunks = [Facebook_PCU_Analysis[i:i + chunk_size] for i in range(0, len(Facebook_PCU_Analysis), chunk_size)]\n",
    "\n",
    "# Salvar as partes como arquivos separados\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk.to_pickle(f'Datasets_Vodafone/Facebook_PCU_Analysis_part_{i+1}.pkl')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "4002a6fe0a5c4fbea9df644bb1b2cd15",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
